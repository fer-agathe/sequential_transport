[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sequential Conditional (Marginally Optimal) Transport on Probabilistic Graphs for Interpretable Counterfactual Fairness",
    "section": "",
    "text": "Introduction\nThis ebook provides the replication codes to the project titled ‘Sequential Conditional (Marginally Optimal) Transport on Probabilistic Graphs for Interpretable Counterfactual Fairness.’\nxxx",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Sequential Conditional (Marginally Optimal) Transport on Probabilistic Graphs for Interpretable Counterfactual Fairness",
    "section": "Abstract",
    "text": "Abstract\nxxx\nKeywords: xxx",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Sequential Conditional (Marginally Optimal) Transport on Probabilistic Graphs for Interpretable Counterfactual Fairness",
    "section": "Outline",
    "text": "Outline\n3  Data 4  Classifier 5  Fairadapt 6  Multivariate Optimal Transport 7  Sequential Transport 8  Counterfactuals: comparison",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "optimal-transport.html",
    "href": "optimal-transport.html",
    "title": "1  Optimal Transport",
    "section": "",
    "text": "1.1 Univariate Optimal Transport",
    "crumbs": [
      "Optimal Transport",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Optimal Transport</span>"
    ]
  },
  {
    "objectID": "optimal-transport.html#univariate-optimal-transport",
    "href": "optimal-transport.html#univariate-optimal-transport",
    "title": "1  Optimal Transport",
    "section": "",
    "text": "1.1.1 Gaussian distribution\nThe Gaussian case is the most simple one since mapping \\({T}^\\star\\), corresponding to OT, can be expressed analytically (it will be a linear mapping). Furthermore, conditional distributions of a multivariate Gaussian distribution are Gaussian distributions, and that can be used to consider an iteration of simple conditional (univariate) transports, as a substitute to joint transport \\({T}^\\star\\). Here \\(\\Phi\\) denotes the univariate cumulative distribution function of the standard Gaussian distribution \\(\\mathcal{N}(0,1)\\).\nOne can easily prove that the optimal mapping, from a \\(\\mathcal{N}(\\mu_0,\\sigma_0^2)\\) to a \\(\\mathcal{N}(\\mu_1,\\sigma_1^2)\\) distribution is: \\[\nx_{1}={T}^\\star(x_{0})= \\mu_{1}+\\frac{\\sigma_{1}}{\\sigma_{0}}(x_{0}-\\mu_{0}),\n\\] which is a nondecreasing linear transformation.\nLet us illustrate this. We start by simulating two Univariate Gaussian distributions with their densities: one for the subset \\(S=0\\) and one for the subset \\(S=1\\).\n\n# Univariate Gaussian distribution S=0\nx1_grid &lt;- seq(-5, 5, length = 251)\nm0 &lt;- -1\ns0 &lt;- 1.2\nd0x1 &lt;- dnorm(x1_grid, m0, s0)\nd_0 &lt;- data.frame(x = x1_grid, y = d0x1)\n\n# Univariate Gaussian distribution S=1\nm1 &lt;- 1.5\ns1 &lt;- .9\nd1x1 &lt;- dnorm(x1_grid, m1, s1)\nd_1 &lt;- data.frame(x = x1_grid, y = d1x1)\n\nIn the following graphs, we plot the optimal transport mapping for one example individual from subset \\(S=0\\):\n\nu &lt;- 0.1586553\n# u-quantile of X1 for subset S=0\nx1 &lt;- qnorm(u, m0, s0)\n# u-quantile of X1 for subset S=1\nx1_star &lt;- qnorm(u, m1, s1)\n\nWe also calculate the indices of \\(X_1\\) grid that are below this individual (\\(x_1\\)) and its counterfactual (\\(x_1^*\\)) in order to plot the cdf’s (for this individual) of \\(X_1\\) in both subsets \\(S=0\\) and \\(S=1\\) in the following graphs:\n\nidx1 &lt;- which(x1_grid &lt;= x1)\nidx1_star &lt;- which(x1_grid &lt;= x1_star)\n\nWe then plot the Optimal Transport line between \\(X_1|S=0\\) and \\(X_1|S=1\\):\n\n\nDisplay the codes used to create the Figure\n# Graph parameters\nlimA &lt;- c(-5, 5)\nlimB &lt;- c(-5, 5) \nlimY &lt;- c(0, .5)\nlab &lt;- c(\"A\", \"B\")\nsub &lt;- 6\n\n{\n  mat &lt;- matrix(c(1, 2, 0, 3), 2)\n  par(mfrow = c(2, 2))\n  layout(mat, c(3.5, 1), c(1, 3))\n  par(mar = c(0.5, 4.5, 0.5, 0.5))\n}\n\n# Density of X1 in subset S=0\nplot(\n  d_0$x, d_0$y, type = \"l\", col = colors[lab[1]], lwd = 2,\n  axes = FALSE, xlab = \"\", ylab = \"\", xlim = limA, ylim = limY\n)\npolygon(\n  c(0, d_0$x, 1), c(0, d_0$y, 0), \n  col = scales::alpha(colors[lab[1]], 0.1), \n  border = NA\n)\n# cdf of X1 in subset S=0\npolygon(\n  c(min(d_0$x), d_0$x[idx1], max(d_0$x[idx1])),\n  c(0, d_0$y[idx1], 0),\n  col = scales::alpha(colors[\"A\"],.2),\n  border = NA\n)\n# Add x-axis\naxis(\n  1, at = seq(limA[1], limA[2], length = sub), \n  label = c(NA, seq(limA[1], limA[2], length = sub)[-1])\n)\n\n# Optimal transport from subset S=0 to S=1 (defined with quantile functions)\npar(mar = c(4.5, 4.5, 0.5, 0.5))\nu_grid &lt;- seq(0, 1, length=261)\nq_0 &lt;- qnorm(u_grid, m0, s0)\nq_1 &lt;- qnorm(u_grid, m1, s1)\nplot(\n  q_0, q_1, col = colors[\"1\"], lwd = 2, type = \"l\", \n  xlab = \"\", ylab = \"\", xlim = limA, ylim = limB, axes = FALSE\n)\nabline(a = 0, b = 1, col = colors[\"0\"], lty = 2)\n# Add x-axis and y-axis\naxis(1)\naxis(2)\n# Legend\nmtext(\"distribution (group 0)\", side = 1, line = 3, col = \"black\")\nmtext(\"distribution (group 1)\", side = 2, line = 3, col = \"black\")\n# Example individual\npoints(x1, x1_star, pch = 19, col = colors[\"1\"])\nsegments(x1, x1_star, x1, 10, lwd = .4, col = colors[\"1\"])\nsegments(x1, x1_star, 10, x1_star, lwd = .4, col = colors[\"1\"])\n\n# Density of X1 in subset S=1\npar(mar = c(4.5, 0.5, 0.5, 0.5))\nplot(\n  d_1$y, d_1$x, type = \"l\", col = colors[lab[2]], lwd = 2,\n  ylim = limB, xlim = limY, xlab = \"\", ylab = \"\", axes = FALSE\n)\npolygon(\n  c(0, d_1$y, 0), c(0, d_1$x, 1), \n  col = scales::alpha(colors[lab[2]], 0.1), border = NA\n)\n# cdf of X1 in subset S=1\npolygon(\n  c(0, d_1$y[idx1_star], 0),\n  c(min(d_1$x), d_1$x[idx1_star], max(d_1$x[idx1_star])),\n  col = scales::alpha(colors[\"B\"],.2),\n  border = NA\n)\n# Add y-axis\naxis(\n  2, at = seq(limB[1], limB[2], length = sub), \n  label = c(NA, seq(limB[1], limB[2], length = sub)[-c(1, sub)], NA)\n)\n\n\n\n\nUnivariate optimal transport, with Gaussian distributions\n\n\n\n\n\n\n\n1.1.2 General distribution\nWe simulate two general univariate distributions based on Gaussian distributions with their densities, cdf’s and quantile functions: one for the subset \\(S=0\\) and one for the subset \\(S=1\\).\n\n# General distribution for subset S=0\nx0 &lt;- rnorm(13, m0, s0)\nf0 &lt;- density(x0, from = -5, to = 5, n = length(x1_grid))\nd0 &lt;- f0$y\nd_0 &lt;- data.frame(x = x1_grid, y = d0)\nx0s &lt;- sample(x0, size = 1e3, replace = TRUE) + rnorm(1e3, 0, f0$bw)\nF0 &lt;- Vectorize(function(x) mean(x0s &lt;= x))\nQ0 &lt;- Vectorize(function(x) as.numeric(quantile(x0s, x)))\n\n# General distribution for subset S=1\nx1 &lt;- rnorm(7, m1, 1)\nf1 &lt;- density(x1, from = -5, to = 5, n = length(x1_grid))\nd1 &lt;- f1$y\nd_1 &lt;- data.frame(x = x1_grid, y = d1)\nx1s &lt;- sample(x1, size = 1e3, replace = TRUE) + rnorm(1e3, 0, f1$bw)\nF1 &lt;- Vectorize(function(x) mean(x1s &lt;= x))\nQ1 &lt;- Vectorize(function(x) as.numeric(quantile(x1s, x)))\n\nIn the following graphs, we plot the optimal transport mapping for one example individual from subset \\(S=0\\):\n\nu &lt;- 0.1586553\nx1 &lt;- Q0(u)\nx1_star &lt;- Q1(u)\n\nWe also calculate the indices of \\(X_1\\) grid that are below this individual (\\(x_1\\)) and its counterfactual (\\(x_1^*\\)) in order to plot the cdf’s (for this individual) of \\(X_1\\) in both subsets \\(S=0\\) and \\(S=1\\) in the following graphs:\n\nidx1 &lt;- which(x1_grid &lt;= x1)\nidx1_star &lt;- which(x1_grid &lt;= x1_star)\n\nWe then plot the Optimal Transport line between \\(X_1|S=0\\) and \\(X_1|S=1\\):\n\n\nDisplay the codes used to create the Figure\n{\n  mat &lt;- matrix(c(1, 2, 0, 3), 2)\n  par(mfrow = c(2, 2))\n  layout(mat, c(3.5, 1), c(1, 3))\n  par(mar = c(0.5, 4.5, 0.5, 0.5))\n}\n\n# Density of X1 in subset S=0\nplot(d_0$x, d_0$y, type = \"l\", col = colors[lab[1]], lwd = 2,\n     axes = FALSE, xlab = \"\", ylab = \"\", xlim = limA, ylim = limY)\npolygon(c(0, d_0$x, 1), c(0, d_0$y, 0), \n        col = scales::alpha(colors[lab[1]], 0.1), \n        border = NA)\n# cdf of X1 in subset S=0\npolygon(\n  c(min(d_0$x), d_0$x[idx1], max(d_0$x[idx1])),\n  c(0, d_0$y[idx1], 0),\n  col = scales::alpha(colors[\"A\"],.2),\n  border = NA\n)\n# Add x-axis\naxis(\n  1, at = seq(limA[1], limA[2], length = sub), \n  label = c(NA, seq(limA[1], limA[2], length = sub)[-1])\n)\n\n# Optimal transport from subset S=0 to S=1 (defined with quantile functions)\npar(mar = c(4.5, 4.5, 0.5, 0.5))\nu_grid &lt;- seq(0, 1, length=261)\nq_0 &lt;- Q0(u_grid)\nq_1 &lt;- Q1(u_grid)\nplot(\n  q_0, q_1, col = colors[\"1\"], lwd = 2, type = \"l\", \n  xlab = \"\", ylab = \"\", xlim = limA, ylim = limB, axes = FALSE\n)\nabline(a = 0, b = 1, col = colors[\"0\"], lty = 2)\n# Add x-axis and y-axis\naxis(1)\naxis(2)\n# Legend\nmtext(\"distribution (group 0)\", side = 1, line = 3, col = \"black\")\nmtext(\"distribution (group 1)\", side = 2, line = 3, col = \"black\")\n# Example individual\npoints(x1, x1_star, pch = 19, col = colors[\"1\"])\nsegments(x1, x1_star, x1, 10, lwd = .4, col = colors[\"1\"])\nsegments(x1, x1_star, 10, x1_star, lwd = .4, col = colors[\"1\"])\n\n# Density of X1 in subset S=1\npar(mar = c(4.5, 0.5, 0.5, 0.5))\nplot(\n  d_1$y, d_1$x, type = \"l\", col = colors[lab[2]], lwd = 2,\n  ylim = limB, xlim = limY, xlab = \"\", ylab = \"\", axes = FALSE\n)\npolygon(\n  c(0, d_1$y, 0), c(0, d_1$x, 1), \n  col = scales::alpha(colors[lab[2]], 0.1), border = NA\n)\n# cdf of X1 in subset S=1\npolygon(\n  c(0, d_1$y[idx1_star], 0),\n  c(min(d_1$x), d_1$x[idx1_star], max(d_1$x[idx1_star])),\n  col = scales::alpha(colors[\"B\"],.2),\n  border = NA\n)\n# Add y-axis\naxis(\n  2, at = seq(limB[1], limB[2], length = sub), \n  label = c(NA, seq(limB[1], limB[2], length = sub)[-c(1, sub)], NA)\n)\n\n\n\n\nGeneral marginal distribution",
    "crumbs": [
      "Optimal Transport",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Optimal Transport</span>"
    ]
  },
  {
    "objectID": "optimal-transport.html#multivariate-optimal-gaussian-transport",
    "href": "optimal-transport.html#multivariate-optimal-gaussian-transport",
    "title": "1  Optimal Transport",
    "section": "1.2 Multivariate Optimal Gaussian Transport",
    "text": "1.2 Multivariate Optimal Gaussian Transport\nRecall that \\(\\boldsymbol{X}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\) if its density, with respect to Lebesgue measure is \\[\n\\begin{equation}\n{\\displaystyle f(\\boldsymbol{x})\\propto{{\\exp \\left(-{\\frac {1}{2}}\\left({\\boldsymbol{x} }-{\\boldsymbol {\\mu }}\\right)^{\\top}{\\boldsymbol {\\Sigma }}^{-1}\\left({\\boldsymbol {x} }-{\\boldsymbol {\\mu }}\\right)\\right)}.%{\\sqrt {(2\\pi )^{k}|{\\boldsymbol {\\Sigma }}|}}}}\n}\n}\n\\end{equation}\n\\tag{1.1}\\]\nIf \\(\\boldsymbol{X}_0\\sim\\mathcal{N}(\\boldsymbol{\\mu}_0,\\boldsymbol{\\Sigma}_0)\\) and \\(\\boldsymbol{X}_1\\sim\\mathcal{N}(\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1)\\), the optimal mapping is also linear, \\[\n\\boldsymbol{x}_{1} = T^\\star(\\boldsymbol{x}_{0})=\\boldsymbol{\\mu}_{1} + \\boldsymbol{A}(\\boldsymbol{x}_{0}-\\boldsymbol{\\mu}_{0}),\n\\] where \\(\\boldsymbol{A}\\) is a symmetric positive matrix that satisfies \\(\\boldsymbol{A}\\boldsymbol{\\Sigma}_{0}\\boldsymbol{A}=\\boldsymbol{\\Sigma}_{1}\\), which has a unique solution given by \\(\\boldsymbol{A}=\\boldsymbol{\\Sigma}_{0}^{-1/2}\\big(\\boldsymbol{\\Sigma}_{0}^{1/2}\\boldsymbol{\\Sigma}_{1}\\boldsymbol{\\Sigma}_{0}^{1/2}\\big)^{1/2}\\boldsymbol{\\Sigma}_{0}^{-1/2}\\), where \\(\\boldsymbol{M}^{1/2}\\) is the square root of the square (symmetric) positive matrix \\(\\boldsymbol{M}\\) based on the Schur decomposition (\\(\\boldsymbol{M}^{1/2}\\) is a positive symmetric matrix), as described in Higham (2008). If \\(\\boldsymbol{\\Sigma}=\\displaystyle\\begin{pmatrix}1&r\\\\r&1\\end{pmatrix}\\), and if \\(a=\\sqrt{(1-\\sqrt{1-r^2})/2}\\), then: \\[\n\\boldsymbol{\\Sigma}^{1/2}=\\displaystyle\\begin{pmatrix}\\sqrt{1-a^2}&a\\\\a&\\sqrt{1-a^2}\\end{pmatrix}.\n\\]\nObserve further this mapping is the gradient of the convex function \\[\n\\psi(\\boldsymbol{x})=\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_0)^\\top\\boldsymbol{A}(\\boldsymbol{x}-\\boldsymbol{\\mu}_0)+\\boldsymbol{x}-\\boldsymbol{\\mu}_1^\\top\\boldsymbol{x}\n\\] and \\(\\nabla T^\\star = \\boldsymbol{A}\\) (see Takatsu (2011) for more properties of Gaussian transport). And if \\(\\boldsymbol{\\mu}_0=\\boldsymbol{\\mu}_1=\\boldsymbol{0}\\), and if \\(\\boldsymbol{\\Sigma}_{0}=\\mathbb{I}\\) and \\(\\boldsymbol{\\Sigma}_{1}=\\boldsymbol{\\Sigma}\\), \\(\\boldsymbol{x}_{1} = T^\\star(\\boldsymbol{x}_{0})=\\boldsymbol{\\Sigma}^{1/2}\\boldsymbol{x}_{0}\\). Hence, \\(\\boldsymbol{\\Sigma}^{1/2}\\) is a linear operator that maps from \\(\\boldsymbol{X}_0\\sim\\mathcal{N}(\\boldsymbol{0},\\mathbb{I})\\) (the reference density) to \\(\\boldsymbol{X}_1\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma})\\) (the target density).",
    "crumbs": [
      "Optimal Transport",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Optimal Transport</span>"
    ]
  },
  {
    "objectID": "optimal-transport.html#sec-cond-gauss",
    "href": "optimal-transport.html#sec-cond-gauss",
    "title": "1  Optimal Transport",
    "section": "1.3 Conditional Gaussian Transport",
    "text": "1.3 Conditional Gaussian Transport\nAlternatively, since \\(\\boldsymbol{\\Sigma}\\) is a positive definite matrix, from the Cholesky decomposition, it can be written as the product of a lower (or upper) triangular matrix and its conjugate transpose, \\[\n\\boldsymbol{\\Sigma}=\\boldsymbol{L}\\boldsymbol{L}^\\top=\\boldsymbol{U}^\\top\\boldsymbol{U}.\n\\]\n\n1.3.1 Remark\nIf \\(\\boldsymbol{\\Sigma}=\\displaystyle\\begin{pmatrix}1&r\\\\r&1\\end{pmatrix}\\), then \\(\\boldsymbol{L}=\\boldsymbol{\\Sigma}_{2|1}^{1/2}=\\displaystyle\\begin{pmatrix}1&0\\\\r&\\sqrt{1-r^2}\\end{pmatrix}\\) while \\(\\boldsymbol{U}=\\boldsymbol{\\Sigma}_{1|2}^{1/2}=\\boldsymbol{\\Sigma}_{2|1}^{1/2\\top}=\\boldsymbol{L}^\\top\\). Then \\(\\boldsymbol{L}^\\top\\boldsymbol{L}=\\boldsymbol{\\Sigma}=\\boldsymbol{U}^\\top\\boldsymbol{U}\\).\n\nBoth \\(\\boldsymbol{L}\\) and \\(\\boldsymbol{U}\\) are linear operators that map from \\(\\boldsymbol{X}_0\\sim\\mathcal{N}(\\boldsymbol{0},\\mathbb{I})\\) (the reference density) to \\(\\boldsymbol{X}_1\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma})\\) (the target density). \\(\\boldsymbol{x}_0\\mapsto\\boldsymbol{L}\\boldsymbol{x}_0\\) and \\(\\boldsymbol{x}_0\\mapsto\\boldsymbol{U}\\boldsymbol{x}_0\\) are respectively linear lower and upper triangular transport maps.\nMore generally, in dimension 2, consider the following (lower triangular) mapping \\(T(x_0,y_0) = (T_x(x_0),T_{y|x}(y_0|x_0))\\), \\[\\begin{eqnarray*}\n&&\n\\mathcal{N}\\left(\n\\begin{pmatrix}\n    \\mu_{0x}\\\\\n    \\mu_{0y}\\\\\n\\end{pmatrix},\n\\begin{pmatrix}\n    \\sigma_{0x}^2 & r_0\\sigma_{0x}\\sigma_{0y}\\\\\n    r_0\\sigma_{0x}\\sigma_{0y} & \\sigma_{0y}^2\\\\\n\\end{pmatrix}\n\\right)\n%\\\\\n\\overset{T}{\\longrightarrow}\n\\mathcal{N}\\left(\n\\begin{pmatrix}\n    \\mu_{1x}\\\\\n    \\mu_{1y}\\\\\n\\end{pmatrix},\n\\begin{pmatrix}\n    \\sigma_{1x}^2 & r_1\\sigma_{1x}\\sigma_{1y}\\\\\n    r_1\\sigma_{1x}\\sigma_{1y} & \\sigma_{1y}^2\\\\\n\\end{pmatrix}\n\\right),\n\\end{eqnarray*}\\]\nwhere \\(T_x(x_0)\\) and \\(T_{y|x}(y_0)\\) are respectively \\[\n\\begin{cases}\n   \\mu_{1x} +\\displaystyle\\frac{\\sigma_{1x}}{\\sigma_{0x}}(x_0-\\mu_{0x})\\phantom{\\displaystyle\\int}\\\\\n     \\mu_{1y}+\\displaystyle\\frac{r_1\\sigma_{1y}}{\\sigma_{1x}}(T_x(x_0)-\\mu_{1x})+\\sqrt{\\displaystyle\\frac{\\sigma_{0x}^2(\\sigma_{1y}^2{\\sigma_{1x}^2}-{r_1^2\\sigma_{1y}^2})}{(\\sigma_{0y}^2{\\sigma_{0x}^2}-{r_0^2\\sigma_{0y}^2})\\sigma_{1x}^2}}(y_0\\!-\\!\\mu_{0y}\\!-\\!\\displaystyle\\frac{r_0\\sigma_{0y}}{\\sigma_{0x}}(x_0\\!-\\!\\mu_{0x}))\n\\end{cases}\n\\]\nthat are both linear mappings. Let us visualize this.\n\n\nCode\npar(mar = c(2.5, 2.5, 0, 0))\npar(mfrow = c(1, 1))\n\n# Assumed cdf for the individual of interest\np1 &lt;- 0.1586553\n# y coordinate for that individual\nb1 &lt;- 3\n\nvx &lt;- seq(-5, 5, length = 6001)\nvy1 &lt;- dnorm(vx,-1, 1.2)*4\nvy2 &lt;- dnorm(vx, 1.5, .9)*4\n\n# Marginal density of x0 in source group (at the bottom of graph)\nplot(\n  vx, vy1, \n  col = colors[\"A\"], xlab = \"\", ylab = \"\", axes = FALSE, type = \"l\", \n  ylim = c(0, 10)\n)\npolygon(\n  c(min(vx), vx, max(vx)),\n  c(0, vy1, 0),\n  col = scales::alpha(colors[\"A\"], .2),\n  border = NA\n)\n# Marginal density of x1 in target group (at the bottom of graph)\nlines(vx, vy2, col = colors[\"B\"])\npolygon(\n  c(min(vx), vx, max(vx)),\n  c(0, vy2, 0),\n  col = scales::alpha(colors[\"B\"], .2),\n  border = NA\n)\n\n# Corresponding quantile in the marginal distribution\na1 &lt;- qnorm(p1, -1, 1.2) # in source group\n# Transported value along x\na2 &lt;- qnorm(p1, 1.5, .9) # in target group\n# Identify observation in x below this quantile a1\nidx1 &lt;- which(vx &lt;= a1)\n# Identify observation in x below this quantile a2\nidx2 &lt;- which(vx &lt;= a2)\n# Showing P(X_1 &lt; a1 | S = 0) on the marginal density plots\npolygon(\n  c(min(vx), vx[idx1], max(vx[idx1])),\n  c(0, vy1[idx1],0),\n  col = scales::alpha(colors[\"A\"],.2),\n  border = NA\n)\n# vertical line to show quantile a1\nsegments(a1, 0, a1, 100, col = colors[\"A\"])\n# Showing P(X_1 &lt; a2 | S = 1) on the marginal density plots\npolygon(\n  c(min(vx), vx[idx2], max(vx[idx2])),\n  c(0, vy2[idx2], 0),\n  col=scales::alpha(colors[\"B\"], .2),\n  border = NA\n)\n# vertical line to show quantile a2\nsegments(a2, 0, a2, 100, col = colors[\"B\"])\n\n# Mean and variance matrix in both groups\nM1 &lt;- c(-1,-1+5)\nM2 &lt;- c(1.5,1.5+5)\nS1 &lt;- matrix(c(1,.5,.5,1)*1.2^2,2,2)\nS2 &lt;- matrix(c(1,-.4,-.4,1)*.9^2,2,2)\n\nA &lt;- sqrtm(S1) %*% S2 %*% (sqrtm(S1))\nA &lt;- solve(sqrtm(S1)) %*% sqrtm(A) %*% solve((sqrtm(S1)))\nT &lt;- function(x) as.vector(M2 + A %*% (x - M1))\n\n# Bivariate Gaussian density\nlibrary(mvtnorm)\nvx0 &lt;- seq(-5, 5, length = 251)\ndata.grid &lt;- expand.grid(x = vx0, y = vx0 + 5)\ndgauss1 &lt;- matrix(\n  mvtnorm::dmvnorm(data.grid, mean = M1, sigma = S1), length(vx0), length(vx0)\n)\ndgauss2 &lt;- matrix(\n  mvtnorm::dmvnorm(data.grid, mean = M2, sigma = S2), length(vx0), length(vx0)\n)\n\n# Contour of the bivariate Gaussian density in source group\ncontour(vx0, vx0 + 5, dgauss1, col = colors[\"A\"], add = TRUE)\n# Contour of the bivariate Gaussian density in target group\ncontour(vx0, vx0 + 5, dgauss2, col = colors[\"B\"], add = TRUE)\n\naxis(1, at = seq(-2, 2) * 2, labels = NA)\naxis(\n  1, at = a1,\n  labels = expression(x[0]),\n  col.ticks = NA,\n  col.axis = colors[\"A\"], line = .5\n)\naxis(\n  1, at = a2,\n  labels = bquote(\n    x[1]~\"=\"~mu[1][x] + frac(sigma[1][x],sigma[0][x])~(x[0]-mu[0][x])\n  ),\n  col.ticks = NA,\n  col.axis = colors[\"B\"], line = .5\n)\naxis(\n  1, at = a1,\n  labels = NA,\n  col.ticks = colors[\"A\"], line = -.5\n)\naxis(\n  1, at = a2,\n  labels = NA,\n  col.ticks = colors[\"B\"], line = -.5\n)\n\n###\n# Second axis\n###\ny &lt;- b1\nvx &lt;- vx + 5\nmu1 &lt;- M1[2] + S1[1, 2] / S1[1, 1] * (a1 - M1[1])\nsig1 &lt;- sqrt(S1[2, 2] - S1[2, 1]^2 / S1[2, 2])\nmu2 &lt;- M2[2] + S2[1, 2] / S2[1, 1] * (a2 - M2[1])\nsig2 &lt;- sqrt(S2[2, 2]- S2[2, 1]^2 / S2[2, 2])\nvz1 &lt;- dnorm(vx, mu1, sig1) * 3\nvz2 &lt;- dnorm(vx, mu2, sig2) * 3\n\n# Marginal density on y, source group\nlines(vz1 - 5, vx, col = colors[\"A\"])\npolygon(\n  c(0, vz1, 0) - 5,\n  c(min(vx), vx, max(vx)),\n  col = scales::alpha(colors[\"A\"], .2),\n  border = NA\n)\n# target group\nlines(vz2 - 5, vx, col = colors[\"B\"])\npolygon(\n  c(0, vz2, 0) - 5,\n  c(min(vx), vx, max(vx)),\n  col = scales::alpha(colors[\"B\"], .2),\n  border = NA\n)\n\n# Identify the cdf at b1 in the marginal distribution\np1 &lt;- pnorm(b1, mu1, sig1)\n# Transported value in the target marginal distribution\nb2 &lt;- qnorm(p1, mu2, sig2)\n# Identify observation in y below this quantile b1\nidx1 &lt;- which(vx &lt;= b1)\n# Identify observation in y below this quantile b2\nidx2 &lt;- which(vx &lt;= b2)\n# Showing P(X_2 &lt; b1 | S = 0) on the marginal density plots\npolygon(\n  c(0, vz1[idx1], 0) - 5,\n  c(min(vx), vx[idx1], max(vx[idx1])),\n  col = scales::alpha(colors[\"A\"], .2),\n  border = NA\n)\n# Showing P(X_2 &lt; b2 | S = 1) on the marginal density plots\npolygon(\n  c(0, vz2[idx2], 0) - 5,\n  c(min(vx), vx[idx2], max(vx[idx2])),\n  col = scales::alpha(colors[\"B\"], .2),\n  border = NA\n)\n# horizontal line to show quantile b1\nsegments(-5, b1, 100, b1, col = colors[\"A\"])\n# horizontal line to show quantile b2\nsegments(-5, b2, 100, b2, col = colors[\"B\"])\n\n# Draw the individual of interest \npoints(a1, b1, pch = 19)\n\n\naxis(2, at = c(0,1,3,4,5) * 2, labels = NA)\naxis(\n  2, at = b1,\n  labels = expression(y[0]),\n  col.ticks = NA,\n  col.axis = colors[\"A\"], line = .5\n)\naxis(\n  2, at = b2,\n  labels = bquote(y[1]),\n  col.ticks = NA,\n  col.axis = colors[\"B\"], line = .5\n)\naxis(\n  2,at = b1,\n  labels = NA,\n  col.ticks = colors[\"A\"], line = -.5\n)\naxis(\n  2,at = b2,\n  labels = NA,\n  col.ticks = colors[\"B\"], line = -.5\n)\n\n# Drawing transported point\npoints(a2, b2, pch = 19)\n\n# Decomposition of the sequential transport\n# First transport along the x axis\nsegments(a1, b1, a2, b1, lwd = 2)\n# Then transport along the y axis\narrows(a2, b1, a2, b2 - .1, length = .1, lwd = 2)\n\n# Storing in a matrix the coordinates of the point before and after transport\nX_then_Y &lt;- matrix(c(a1, b1, a2, b2), 2, 2)\ncolnames(X_then_Y) = c(\"start\",\"x_then_y\")\n\n# Showing the transported point if we assume another sequential transport:\n# first along the y axis, and then along the x axis\nM1 &lt;- c(-1, -1+5)\nM2 &lt;- c(1.5, 1.5)\nS1 &lt;- matrix(c(1, .5, .5, 1) * 1.2^2, 2, 2)\nS2 &lt;- matrix(c(1, -.4, -.4, 1) * .9^2, 2, 2)\n\n# Transport\nAA &lt;- sqrtm(S1) %*% S2 %*% (sqrtm(S1))\nAA &lt;- solve(sqrtm(S1)) %*% sqrtm(AA) %*% solve((sqrtm(S1)))\nT &lt;- function(x) as.vector(M2 + AA %*% (x - M1))\noptrasnp &lt;- T(c(a1, b1))\nXYopt &lt;- matrix(c(a1, b1, optrasnp[1], optrasnp[2] + 5), 2, 2)\ncolnames(XYopt) = c(\"start\",\"OT\")\n\n# Drawing the point transported with the different order in the sequence\npoints(optrasnp[1], optrasnp[2] + 5, pch = 15, col = \"#C93312\")\n\n\n\n\nGaussian conditional optimal transport. The process begins with a univariate transport along the \\(x\\) axis (using \\(T_x^*\\)), followed by a transport along the \\(y\\) axis on the conditional distribution (using \\(T_{y|x}^*\\)), corresponding to the “lower triangular affine mapping.” The red square is the multivariate optimal transport of the point considering an “upper triangular affine mapping” instead.\n\n\n\n\n\nNow, let us transport first along the \\(y\\) axis, and then along the \\(x\\) axis.\n\n\n\n\nGaussian conditional optimal transport. The process begins with a univariate transport along the \\(y\\) axis (using \\(T_y^*\\)), followed by a transport along the \\(x\\) axis on the conditional distribution (using \\(T_{x|y}^*\\)), corresponding to the “upper triangular affine mapping.” The red square is the multivariate optimal transport of the point considering a “lower triangular affine mapping” instead.\n\n\n\n\n\nOf course, this is highly dependent on the axis parametrization. Instead of considering projections on the axis, one could consider transport in the direction \\(\\overrightarrow{u}\\), followed by transport in the direction \\(\\overrightarrow{u}^\\perp\\) (on conditional distributions). This will be visualized in Figure 1.1 and Figure 1.2",
    "crumbs": [
      "Optimal Transport",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Optimal Transport</span>"
    ]
  },
  {
    "objectID": "optimal-transport.html#gaussian-probabilistis-graphical-models",
    "href": "optimal-transport.html#gaussian-probabilistis-graphical-models",
    "title": "1  Optimal Transport",
    "section": "1.4 Gaussian Probabilistis Graphical Models",
    "text": "1.4 Gaussian Probabilistis Graphical Models\nAn interesting feature of the Gaussian multivariate distribution is that any marginal and any conditional distribution (given other components) is still Gaussian. More precisely, if \\[\n{\\displaystyle \\boldsymbol {x} ={\\begin{pmatrix}\\boldsymbol {x} _{1}\\\\\\boldsymbol {x} _{2}\\end{pmatrix}}},~{\\displaystyle {\\boldsymbol {\\mu }}={\\begin{pmatrix}{\\boldsymbol {\\mu }}_{1}\\\\{\\boldsymbol {\\mu }}_{2}\\end{pmatrix}}}\\text{ and } {\\displaystyle {\\boldsymbol {\\Sigma }}={\\begin{pmatrix}{\\boldsymbol {\\Sigma }}_{11}&{\\boldsymbol {\\Sigma }}_{12}\\\\{\\boldsymbol {\\Sigma }}_{21}&{\\boldsymbol {\\Sigma }}_{22}\\end{pmatrix}}},\n\\] then \\(\\boldsymbol{X}_1\\sim\\mathcal{N}(\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_{11})\\), while, with notations of Equation 1.1, we can also write \\({\\displaystyle { {\\boldsymbol {B }_{1}}}={\\boldsymbol {B }}_{11}-{\\boldsymbol {B }}_{12}{\\boldsymbol {B }}_{22}^{-1}{\\boldsymbol {B }}_{21}}\\) (based on properties of inverses of block matrices, also called the Schur complement of a block matrix). Furthermore, conditional distributions are also Gaussian, \\(\\boldsymbol{X}_1|\\boldsymbol{X}_2=\\boldsymbol{x}_2\\sim\\mathcal{N}({\\boldsymbol {\\mu }_{1|2}},{\\boldsymbol {\\Sigma }_{1|2}}),\\) \\[\n\\begin{cases}\n   {\\displaystyle { {\\boldsymbol {\\mu }_{1|2}}}={\\boldsymbol {\\mu }}_{1}+{\\boldsymbol {\\Sigma }}_{12}{\\boldsymbol {\\Sigma }}_{22}^{-1}\\left(\\boldsymbol{x}_2 -{\\boldsymbol {\\mu }}_{2}\\right)}\n   \\\\\n   {\\displaystyle { {\\boldsymbol {\\Sigma }_{1|2}}}={\\boldsymbol {\\Sigma }}_{11}-{\\boldsymbol {\\Sigma }}_{12}{\\boldsymbol {\\Sigma }}_{22}^{-1}{\\boldsymbol {\\Sigma }}_{21},}\n\\end{cases}\n\\] and the inverse of the conditional variance is simply \\(\\boldsymbol {B }_{11}\\).\nIt is well known that if \\(\\boldsymbol{X}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\), \\(X_i\\indep X_j\\) if and only if \\(\\Sigma_{i,j}=0\\). More interestingly, we also have the following result, initiated by :\n\n\n\n\n\n\nProposition\n\n\n\nIf \\(\\boldsymbol{X}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\), with notations of Equation 1.1, \\(\\boldsymbol{B}=\\boldsymbol{\\Sigma}^{-1}\\), \\(\\boldsymbol{X}\\) is Markov with respect to \\(\\mathcal{G}=(E,V)\\) if and only if \\(B_{i,j}=0\\) whenever \\((i,j),(j,i)\\notin E\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\nThis is a direct consequence of the following property : if \\(\\boldsymbol{X}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\), \\(X_i\\indep X_j |\\boldsymbol{X}_{-i,j}\\) if and only if \\(B_{i,j}=0\\) (since the log-density has separate terms in \\(x_i\\) and \\(x_j\\)).",
    "crumbs": [
      "Optimal Transport",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Optimal Transport</span>"
    ]
  },
  {
    "objectID": "optimal-transport.html#sequential-transport",
    "href": "optimal-transport.html#sequential-transport",
    "title": "1  Optimal Transport",
    "section": "1.5 Sequential transport",
    "text": "1.5 Sequential transport\nIn the Gaussian case we obviously recover the results of Section 1.3, if we plug Gaussian distributions in the expressions shown in the “Sequential Transport” of the paper: \\[\n\\begin{cases}\n   X_{0:1}\\!\\sim\\!\\mathcal{N}(\\mu_{0:1},\\sigma_{0:1}^2),\\text{ hence }F_{0:1}(x)\\!=\\!\\Phi\\big(\\sigma_{0:1}^{-1}(x\\!-\\!\\mu_{0:1})\\big)\\\\\n   X_{1:1}\\!\\sim\\!\\mathcal{N}(\\mu_{1:1},\\sigma_{1:1}^2),\\text{ hence }F_{1:1}^{-1}(u)\\!=\\!\\mu_{1:1}\\!+\\!\\sigma_{1:1}\\!\\Phi^{-1}(u)\n\\end{cases}\n\\] thus \\[\nT_1^\\star(x) = F_{1:1}^{-1}\\big(F_{0:1}(x)\\big)=\\mu_{1:1} +\\displaystyle\\frac{\\sigma_{1:1}}{\\sigma_{0:1}}(x-\\mu_{0:1}),\n\\] while \\[\n\\begin{cases}\n   X_{0:2}|x_{0:1}\\sim\\mathcal{N}(\\mu_{0:2|1},\\sigma_{0:2|1}^2),\\\\\n   X_{1:2}|x_{0:1}\\sim\\mathcal{N}(\\mu_{1:2|1},\\sigma_{1:2|1}^2),\\\\\n\\end{cases}\n\\] i.e., \\[\n\\begin{cases}\n  F_{0:2|1}(x)=\\Phi\\big(\\sigma_{0:2|1}^{-1}(x-\\mu_{0:2|1})\\big),\\\\\n  F_{1:2|1}^{-1}(u)=\\mu_{1:2|1}+\\sigma_{1:2|1}\\Phi^{-1}(u),\n\\end{cases}\n\\] where we consider \\(X_{0:2}\\) conditional to \\(X_{0:1}=x_{0:1}\\) in the first place, \\[\n\\begin{cases}\n   \\mu_{0:2|1} = \\mu_{0:2} +\\displaystyle\\frac{\\sigma_{0:2}}{\\sigma_{0:1}}(x_{0:1}-\\mu_{0:1}),\\phantom{\\displaystyle\\int}\\\\\n   \\sigma_{0:2|1}^2 = \\sigma_{0:2}^2 -\\displaystyle\\frac{r^2_0\\sigma_{0:2}^2}{\\sigma_{0:1}^2},\\phantom{\\int}\n\\end{cases}\n\\] and \\(X_{1:2}\\) conditional to \\(X_{1:1}=T^\\star_1(x_{0:1})\\) in the second place, \\[\n\\begin{cases}\n   \\mu_{1:2|1} = \\mu_{1:2} +\\displaystyle\\frac{\\sigma_{1:2}}{\\sigma_{1:1}}\\big(T^\\star_1(x_{0:1})-\\mu_{1:1}\\big),\\phantom{\\int}\\\\\n   \\sigma_{1:2|1}^2= \\sigma_{1:2}^2 -\\displaystyle\\frac{r^2_1\\sigma_{1:2}^2}{\\sigma_{1:1}^2},\\phantom{\\int}\n\\end{cases}\n\\] thus \\[\nT_{2|1}(x) = F_{1:2|1}^{-1}\\big(F_{0:2|1}(x)\\big)=\\mu_{1:2|1} +\\displaystyle\\frac{\\sigma_{1:2|1}}{\\sigma_{0:2|1}}(x-\\mu_{0:2|1}),\n\\] which is \\[\n\\begin{eqnarray*}\n&&\\mu_{1:2}+\\displaystyle\\frac{r_1\\sigma_{1:2}}{\\sigma_{1:1}}\\big(\\mu_{1:1} +\\displaystyle\\frac{\\sigma_{1:1}}{\\sigma_{0:1}}(x_{0:1}-\\mu_{0:1})-\\mu_{1:1}\\big) \\\\&&+\\sqrt{\\frac{\\sigma_{0:1}^2(\\sigma_{1:2}^2{\\sigma_{1:1}^2}-{r_1^2\\sigma_{1:2}^2})}{(\\sigma_{0:2}^2{\\sigma_{0:1}^2}-{r_0^2\\sigma_{0:2}^2})\\sigma_{1:1}^2}}\\cdot\\big(x-\\mu_{0:2}-\\displaystyle\\frac{r_0\\sigma_{0:2}}{\\sigma_{0:1}}(x_{0:1}-\\mu_{0:1})\\big).    \n\\end{eqnarray*}\n\\]",
    "crumbs": [
      "Optimal Transport",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Optimal Transport</span>"
    ]
  },
  {
    "objectID": "optimal-transport.html#general-conditional-transport",
    "href": "optimal-transport.html#general-conditional-transport",
    "title": "1  Optimal Transport",
    "section": "1.6 General Conditional Transport",
    "text": "1.6 General Conditional Transport\nAn interesting property of Gaussian vectors is the stability under rotations. In dimension 2, instead of a sequential transport of \\(\\boldsymbol{x}\\) on \\(\\overrightarrow{e}_x\\) and then (conditionally) on $_y, one could consider a projection on any unit vector \\(\\overrightarrow{u}\\) (with angle \\(\\theta\\)), and then (conditionally) along the orthogonal direction \\(\\displaystyle \\overrightarrow{u}^\\perp\\). In Figure 1.1, we can visualize the set of all counterfactuals \\(\\boldsymbol{x}^\\star\\) when \\(\\theta\\in[0,2\\pi]\\). The (global) optimal transport is also considered.\n\n\nCode\nangle &lt;- function(theta,\n                  A = c(-2.2,-2)) {\n  \n  # Rotation\n  R &lt;- matrix(c(cos(theta), sin(theta), -sin(theta), cos(theta)), 2, 2)\n  # Mean and Variance\n  M1 &lt;- c(-1, -1)\n  M2 &lt;- c(1.5, 1.5)\n  S1 &lt;- matrix(c(1, .5, .5, 1) * 1.2^2, 2, 2)\n  S2 &lt;- matrix(c(1, -.4, -.4, 1) * .9^2, 2, 2)\n  \n  # After applying the rotation\n  M1 &lt;- as.vector(R %*% M1)\n  M2 &lt;- as.vector(R %*% M2)\n  S1 &lt;- t(R) %*% S1 %*% R\n  S2 &lt;- t(R) %*% S2 %*% R\n  A &lt;- as.vector(R %*% A)\n  \n  # Coordinates\n  a1 &lt;- A[1]\n  b1 &lt;- A[2]\n  a2 &lt;- qnorm(pnorm(a1, M1[1], sqrt(S1[1, 1])), M2[1], sqrt(S2[1, 1]))\n  mu1 &lt;- M1[2] + S1[1, 2] / S1[1, 1] * (a1 - M1[1])\n  sig1 &lt;- sqrt(S1[2, 2] - S1[2, 1]^2 / S1[2, 2])\n  mu2 &lt;- M2[2] + S2[1, 2] / S2[1, 1] * (a2 - M2[1])\n  sig2 &lt;- sqrt(S2[2, 2] - S2[2, 1]^2 / S2[2, 2])\n  p1 &lt;- pnorm(b1, mu1, sig1)\n  b2 &lt;- qnorm(p1, mu2, sig2)\n  B &lt;- c(a2, b2)\n  \n  c(\n    as.vector(t(R) %*% A),\n    as.vector(t(R) %*% B)\n  )\n}\n\n# Mean and variance\nM1 &lt;- c(-1, -1)\nM2 &lt;- c(1.5, 1.5)\nS1 &lt;- matrix(c(1, .5, .5, 1) * 1.2^2, 2, 2)\nS2 &lt;- matrix(c(1, -.4, -.4, 1) * .9^2, 2, 2)\n\nA &lt;- c(-2.2, -2)\n\npar(mfrow = c(1, 1), mar = c(.5, .5, 0, 0))\n\n# Bivariate Gaussian density\nlibrary(mvtnorm)\nvx0 &lt;- seq(-5, 5, length = 251)\ndata.grid &lt;- expand.grid(x = vx0, y = vx0)\ndgauss1 &lt;- matrix(\n  mvtnorm::dmvnorm(data.grid, mean = M1, sigma = S1), length(vx0), length(vx0)\n)\ndgauss2 &lt;- matrix(\n  mvtnorm::dmvnorm(data.grid, mean = M2, sigma = S2), length(vx0), length(vx0)\n)\n\n# Contour of the bivariate Gaussian density in source group\ncontour(\n  vx0, vx0, dgauss1, col = colors[\"A\"], \n  xlim = c(-5, 5), ylim = c(-5, 5), axes = FALSE\n)\n# Contour of the bivariate Gaussian density in target group\ncontour(vx0, vx0, dgauss2, col = colors[\"B\"], add = TRUE)\n\n# Horizontal and vertical line showing the coordinates of the point of interest\nsegments(A[1], -5, A[1], 100, col = colors[\"A\"])\nsegments(-5, A[2], 100, A[2], col = colors[\"A\"])\n# Drawing this point\npoints(A[1], A[2], pch = 19, colour = \"darkblue\")\n\n# Add axes\naxis(1, at = c(0, 1, 2, 3, 4, 5) * 2 - 5, labels = NA)\naxis(2, at = c(0, 1, 2, 3, 4, 5) * 2 - 5, labels = NA)\n\n# Apply rotations\nMANGLE &lt;- Vectorize(angle)(seq(0, 2 * pi, length = 100))\n# Draw the transported points for each rotation\nlines(MANGLE[3, ], MANGLE[4, ])\n\n# Display specific point when transporting first along the x axis and then the\n# orthogonal axis, i.e., y\nm &lt;- angle(0, c(-2.2, -2))\npoints(m[3],m[4],pch=19)\n\n# Same but transporting first along the y axis and then on the orthogonal axis,\n# i.e., x\nm &lt;- angle(pi/2, c(-2.2, -2))\npoints(m[3], m[4], pch = 19)\n\n\nT &lt;- function(x) as.vector(M2 + AA %*% (x - M1))\noptrasnp &lt;- T(c(A[1], A[2]))\npoints(optrasnp[1], optrasnp[2], pch = 15, col = \"#C93312\")\n\n\n\n\n\nFigure 1.1: Gaussian conditional optimal transports. The process begins with a univariate transport along the direction \\(\\overrightarrow{u}\\) (using \\(\\displaystyle T^\\star_{\\overrightarrow{u}}\\)) followed by a transport along the orthogonal direction \\(\\displaystyle \\overrightarrow{u}^\\perp\\), on conditional distributions (using \\(\\displaystyle T^\\star_{\\overrightarrow{u}^\\perp|{\\overrightarrow{u}}}\\)). The blue point is the starting point. The curves in the upper right corner represent the set of all transport maps of the same point (bottom left corner) for all possible directions \\(\\displaystyle \\overrightarrow{u}\\), the black points correspond to classical \\(x\\) (horizontal) and \\(y\\) (vertical) directions.\n\n\n\n\n\n\n\n\nWe can consider another starting point.\n\n\nCode\n# Consider the following starting point\nA = c(-2.2, .5)\n\npar(mfrow = c(1, 1), mar = c(.5, .5, 0, 0))\n\n# Bivariate Gaussian density\nvx0 &lt;- seq(-5, 5, length = 251)\ndata.grid &lt;- expand.grid(x = vx0, y = vx0)\ndgauss1 &lt;- matrix(\n  mvtnorm::dmvnorm(data.grid, mean = M1, sigma = S1), length(vx0), length(vx0)\n)\ndgauss2 &lt;- matrix(\n  mvtnorm::dmvnorm(data.grid, mean = M2, sigma = S2), length(vx0), length(vx0)\n)\n# Contour of the bivariate Gaussian density in source group\ncontour(\n  vx0, vx0, dgauss1, col = colors[\"A\"], \n  xlim = c(-5, 5), ylim = c(-5, 5), axes = FALSE\n)\n# Contour of the bivariate Gaussian density in target group\ncontour(vx0, vx0, dgauss2, col = colors[\"B\"], add = TRUE)\n\n# Vertical and horizontal lines showing the coordinates of the point\nsegments(A[1], -5, A[1], 100, col = colors[\"A\"])\nsegments(-5, A[2], 100, A[2], col = colors[\"A\"])\n# Drawing that starting point\npoints(A[1], A[2], pch = 19, colour = \"darkblue\")\n\n# Showing axes\naxis(1, at = c(0,1,2,3,4,5) * 2 - 5, labels = NA)\naxis(2, at = c(0,1,2,3,4,5) * 2 - 5, labels = NA)\n\n# Apply rotations\nMANGLE &lt;- \n  Vectorize(function(x) angle(x, c(-2.2, .5)))(seq(0, 2 * pi, length = 100))\n# Draw the transported points for each rotation\nlines(MANGLE[3, ], MANGLE[4, ])\n\n# Display specific point when transporting first along the x axis and then the\n# orthogonal axis, i.e., y\nm &lt;- angle(0, c(-2.2, .5))\npoints(m[3], m[4], pch = 19)\n\n# Same but transporting first along the y axis and then on the orthogonal axis,\n# i.e., x\nm &lt;- angle(pi/2, c(-2.2, .5))\npoints(m[3], m[4], pch = 19)\n\nAA &lt;- sqrtm(S1) %*% S2 %*% (sqrtm(S1))\nAA &lt;- solve(sqrtm(S1)) %*% sqrtm(AA) %*% solve((sqrtm(S1)))\nT &lt;- function(x) as.vector(M2 + AA %*% (x - M1))\noptrasnp &lt;- T(c(A[1], A[2]))\npoints(optrasnp[1], optrasnp[2], pch = 15, col = \"#C93312\")\n\n\n\n\n\nFigure 1.2: Gaussian conditional optimal transports. The process begins with a univariate transport along the direction \\(\\overrightarrow{u}\\) (using \\(\\displaystyle T^\\star_{\\overrightarrow{u}}\\)) followed by a transport along the orthogonal direction \\(\\displaystyle \\overrightarrow{u}^\\perp\\), on conditional distributions (using \\(\\displaystyle T^\\star_{\\overrightarrow{u}^\\perp|{\\overrightarrow{u}}}\\)). The blue point is the starting point. The curves in the upper right corner represent the set of all transport maps of the same point (bottom left corner) for all possible directions \\(\\displaystyle \\overrightarrow{u}\\), the black points correspond to classical \\(x\\) (horizontal) and \\(y\\) (vertical) directions.\n\n\n\n\n\n\n\n\n\n\n\n\nHigham, Nicholas J. 2008. Functions of Matrices: Theory and Computation. SIAM.\n\n\nTakatsu, Asuka. 2011. “Wasserstein Geometry of Gaussian Measures.” Osaka Journal of Mathematics 48 (4): 1005–26.",
    "crumbs": [
      "Optimal Transport",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Optimal Transport</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "2  Regression",
    "section": "",
    "text": "2.1 Data Generation\nConsider a causal structural model with a sensitive attribute \\(s\\) with no parents, two legitimate features \\(x_1\\) and \\(x_2\\), and an outcome \\(y\\).\nLet us draw 100 observation per group. For group 0, we draw values from a multivariate Gaussian distribution with mean -1 and the following variance-covariance matrix: \\(\\Sigma_0 = \\begin{bmatrix}1.2^2 & \\frac{1.2^2}{2}\\\\ \\frac{1.2^2}{2} & 1.2^2\\end{bmatrix}\\). For group 1, we draw values from a multivariate Gaussian distribution with mean 1.5 and with the following variance-covariance matrix: \\(\\Sigma_0 = \\begin{bmatrix}.9^2 & -\\frac{4\\times .9^2}{10}\\\\ -\\frac{4 \\times .9^2}{2} & .9^2\\end{bmatrix}\\).\n# Number of observations per group\nset.seed(123) # set the seed for reproductible results\nn &lt;- 100\n\n# First bivariate Gaussian distribution: group s=0\nM0 &lt;- c(-1, -1)\nS0 &lt;- matrix(c(1, .5, .5,1) * 1.2^2, 2, 2)\nX0 &lt;- mnormt::rmnorm(n, M0, S0)\nD_SXY_0 &lt;- data.frame(\n  S = 0,\n  X1 = X0[, 1],\n  X2 = X0[, 2]\n)\n\n# Second bivariate Gaussian distribution: group s=1\nM1 &lt;- c(1.5, 1.5)\nS1 &lt;- matrix(c(1, -.4, -.4, 1) * .9^2, 2, 2)\nX1 &lt;- mnormt::rmnorm(n, M1, S1)\nD_SXY_1 &lt;- data.frame(\n  S = 1,\n  X1 = X1[,1],\n  X2 = X1[,2]\n)\nAssume the response variable, \\(Y\\), to be a binary variable that depends on the covariates of each group. More specifically, assume that it is drawn from a Bernoulli distribution with probability of occurrence being linked through a logistic function to \\(x_1\\) and \\(x_2\\), i.e., \\(Y \\sim \\mathcal{B}(p(S))\\), where \\(p(S)\\) differs among groups: \\[\np(S) = \\begin{cases}\n\\frac{\\exp{(\\eta_0})}{1+\\exp{(\\eta_0)}}, & \\text{if } S = 0,\\\\\n\\frac{\\exp{(\\eta_1})}{1+\\exp{(\\eta_1)}}, & \\text{if } S = 1,\n\\end{cases}\n\\] where \\[\n\\begin{cases}\n\\eta_0 = \\frac{1.2 x_1 + 1.6x2}{2}\\\\\n\\eta_1 = \\frac{.8 x_1 + 2.4x2}{2}.\n\\end{cases}\n\\]\n# Drawing random binary response variable Y with logistic model for each group\neta_0 &lt;- (D_SXY_0$X1 * 1.2 + D_SXY_0$X2 / 2 * .8) / 2\neta_1 &lt;- (D_SXY_1$X1 * .8 + D_SXY_1$X2 / 2 * 1.2) / 2\np_0 &lt;- exp(eta_0) / (1 + exp(eta_0))\np_1 &lt;- exp(eta_1) / (1 + exp(eta_1))\nD_SXY_0$Y &lt;- rbinom(n, size = 1, prob = p_0)\nD_SXY_1$Y &lt;- rbinom(n, size = 1, prob = p_1)\nWe merge the two datasets in a single one\nD_SXY &lt;- rbind(D_SXY_0, D_SXY_1)\nAnd we create two datasets that contain individuals from group 0 only, and individuals from group 1 only:\n# Dataset with individuals in group 0 only\nD_SXY0 &lt;- D_SXY[D_SXY$S == 0, ]\n# Dataset with individuals in group 1 only\nD_SXY1 &lt;- D_SXY[D_SXY$S == 1,]",
    "crumbs": [
      "Simulations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#sec-regression-model",
    "href": "regression.html#sec-regression-model",
    "title": "2  Regression",
    "section": "2.2 Hypothetical Model",
    "text": "2.2 Hypothetical Model\nAssume the scores obtained from a logistic regression write: \\[\nm(x_1,x_2,s)=\\big(1+\\exp\\big[-\\big((x_1+x_2)/2 + \\boldsymbol{1}(s=1)\\big)\\big]\\big)^{-1}.\n\\]\n\n#' Logistic regression\n#' \n#' @param x1 first numerical predictor\n#' @param x2 second numerical predictor\n#' @param s sensitive attribute (0/1)\nlogistique_reg &lt;- function(x1, x2, s) {\n  eta &lt;- (x1 + x2) / 2 - s\n  exp(eta) / (1 + exp(eta))\n}",
    "crumbs": [
      "Simulations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#sec-regression-algo-example",
    "href": "regression.html#sec-regression-algo-example",
    "title": "2  Regression",
    "section": "2.3 Illustrative example: small grid",
    "text": "2.3 Illustrative example: small grid\nWe have two features \\(x_1\\) and \\(x_2\\) (i.e., \\(\\boldsymbol{v} = \\{x_1, x_2\\}\\)) that need to be transported from group \\(s=0\\) to group \\(s=1\\). We will therefore use a grid in dimension 2 to apply (alg-3?). Recall that we assume that \\(x_1\\) depends on \\(s\\) only, and that \\(x_2\\) depends on both \\(x_1\\) and \\(s\\). Hence, to transport \\(x_2\\), we need to transport \\(x_1\\) before.\n\n2.3.0.1 Transport of \\(x_1\\)\nWe transport the first dimension. In the notations of (alg-3?), \\(j = x_1\\).\nFor now, we define \\(k=10\\) cells in each dimension (we will use 500 after), to obtain \\(\\boldsymbol{g}_{x_1|s}\\) To define the coordinates of the grid, we expand the domain of observed values in each value by a value \\(h\\) that we set to .2 here.\n\nn_grid_example &lt;- 10\nh &lt;- .2\n\nThe coordinates of the cells \\(\\boldsymbol{g}_{x_1|s=0}\\)\n\nvx1_example_0 &lt;- seq(\n  min(D_SXY_0$X1) - h, \n  max(D_SXY_0$X1) + h, \n  length = n_grid_example + 1\n)\nvx1_example_0 # coordinates for the cells in dimension 1 for group 0\n\n [1] -3.6638967 -3.1150270 -2.5661574 -2.0172878 -1.4684182 -0.9195485\n [7] -0.3706789  0.1781907  0.7270603  1.2759300  1.8247996\n\n\nThe coordinates of the cells \\(\\boldsymbol{g}_{x_1|s=1}\\)\n\nvx1_example_1 &lt;- seq(\n  min(D_SXY_1$X1) - h, \n  max(D_SXY_1$X1) + h, \n  length = n_grid_example + 1\n)\nvx1_example_1 # coordinates for the cells in dimension 1 for group 1\n\n [1] -0.9193084 -0.4398679  0.0395725  0.5190129  0.9984534  1.4778938\n [7]  1.9573343  2.4367747  2.9162151  3.3956556  3.8750960\n\n\nAnd we extract the middle of the cells:\n\nvx1_example_0_mid &lt;- \n  (vx1_example_0[2:(1 + n_grid_example)] + vx1_example_0[1:(n_grid_example)]) / 2\nvx1_example_1_mid &lt;- \n  (vx1_example_1[2:(1 + n_grid_example)] + vx1_example_1[1:(n_grid_example)]) / 2\n\nvx1_example_0_mid # middle cells in dimension 1 for group 0\n\n [1] -3.3894619 -2.8405922 -2.2917226 -1.7428530 -1.1939833 -0.6451137\n [7] -0.0962441  0.4526255  1.0014952  1.5503648\n\nvx1_example_1_mid # middle cells in dimension 1 for group 1\n\n [1] -0.6795882 -0.2001477  0.2792927  0.7587332  1.2381736  1.7176140\n [7]  2.1970545  2.6764949  3.1559354  3.6353758\n\n\n\n\nCode\npar(mar = c(2,2,0,0))\nplot(\n  vx1_example_0, vx1_example_0*0, \n  xlab = \"\", ylab = \"\", axes = FALSE, col = NA, \n  xlim = c(-4, 4), ylim = c(.5, 1.7)\n)\naxis(1)\nfor(i in 1:n_grid_example) {\n  rect(vx1_example_0[i], .5, vx1_example_0[i+1], 1, border=\"grey\")\n}\npoints(D_SXY_0$X1, y = rep(.6, nrow(D_SXY_0)), col = alpha(colours[\"A\"], .3), pch = 19, cex = .2)\npoints(vx1_example_0_mid, y = rep(.75, n_grid_example), col = colours[\"A\"], pch = 4, cex = 1.5)\n\nfor(i in 1:n_grid_example) {\n  rect(vx1_example_1[i], 1.1, vx1_example_1[i+1], 1.6, border=\"grey\")\n}\n\npoints(D_SXY_1$X1, y = rep(1.2, nrow(D_SXY_1)), col = alpha(colours[\"B\"], .3), pch = 19, cex = .2)\npoints(vx1_example_1_mid, y = rep(1.35, n_grid_example), col = colours[\"B\"], pch = 4, cex = 1.5)\n\n\n\n\n\nFigure 2.1: Example with a grid with 10 cells created in each group (0 in green and 1 in yellow). The observed values are represented by the dots. The crosses represent the millde of the cells.\n\n\n\n\n\n\n\n\nImagine that we want to transport the following individual:\n\nid_indiv &lt;- 2\nindiv &lt;- D_SXY_0[id_indiv, ]\nindiv\n\n  S      X1          X2 Y\n2 0 0.87045 0.008499458 1\n\n\nInstead of being in group \\(s=0\\), we would like it to be in group \\(s=1\\). Assume here that we want to transport its \\(x_1\\) characteristic.\n\n\nCode\npar(mar = c(2,2,0,0))\nplot(\n  vx1_example_0, vx1_example_0*0, \n  xlab = \"\", ylab = \"\", axes = FALSE, col = NA, \n  xlim = c(-4, 4), ylim = c(.5, 1.7)\n)\naxis(1)\nfor(i in 1:n_grid_example) {\n  rect(vx1_example_0[i], .5, vx1_example_0[i+1], 1, border=\"grey\")\n}\npoints(D_SXY_0$X1, y = rep(.6, nrow(D_SXY_0)), col = alpha(colours[\"A\"], .3), pch = 19, cex = .2)\npoints(vx1_example_0_mid, y = rep(.75, n_grid_example), col = colours[\"A\"], pch = 4, cex = 1.5)\n\nfor(i in 1:n_grid_example) {\n  rect(vx1_example_1[i], 1.1, vx1_example_1[i+1], 1.6, border=\"grey\")\n}\n\npoints(D_SXY_1$X1, y = rep(1.2, nrow(D_SXY_1)), col = alpha(colours[\"B\"], .3), pch = 19, cex = .2)\npoints(vx1_example_1_mid, y = rep(1.35, n_grid_example), col = colours[\"B\"], pch = 4, cex = 1.5)\n\npoints(D_SXY_0[id_indiv, \"X1\"], .6, col = \"red\", pch = 19, cex = 1.5)\n\n\n\n\n\nFigure 2.2: The individual from group \\(s=0\\) for which we want to get a counterfactual\n\n\n\n\n\n\n\n\nWe identify in which cell of the grid that individual belongs, indexed \\(i_0\\):\n\nx1 &lt;- D_SXY_0[id_indiv, \"X1\"]\n(i_0 &lt;- which.min(abs(vx1_example_0_mid - x1)))\n\n[1] 9\n\n\n\n\nCode\npar(mar = c(2,2,0,0))\nplot(\n  vx1_example_0, vx1_example_0*0, \n  xlab = \"\", ylab = \"\", axes = FALSE, col = NA, \n  xlim = c(-4, 4), ylim = c(.5, 1.7)\n)\naxis(1)\nfor(i in 1:n_grid_example) {\n  rect(vx1_example_0[i], .5, vx1_example_0[i+1], 1, border=\"grey\")\n}\npoints(D_SXY_0$X1, y = rep(.6, nrow(D_SXY_0)), col = alpha(colours[\"A\"], .3), pch = 19, cex = .2)\npoints(vx1_example_0_mid, y = rep(.75, n_grid_example), col = colours[\"A\"], pch = 4, cex = 1.5)\n\nfor(i in 1:n_grid_example) {\n  rect(vx1_example_1[i], 1.1, vx1_example_1[i+1], 1.6, border=\"grey\")\n}\n\npoints(D_SXY_1$X1, y = rep(1.2, nrow(D_SXY_1)), col = alpha(colours[\"B\"], .3), pch = 19, cex = .2)\npoints(vx1_example_1_mid, y = rep(1.35, n_grid_example), col = colours[\"B\"], pch = 4, cex = 1.5)\n\n# Individual of interest\npoints(D_SXY_0[id_indiv, \"X1\"], .6, col = \"red\", pch = 19, cex = 1.5)\n\n# Cell in which the individual belongs to\nrect(vx1_example_0[i_0], .5, vx1_example_0[i_0+1], 1, border=\"red\")\n\n\n\n\n\nFigure 2.3: The cell for the \\(x_1\\) dimension in which that individual belongs to\n\n\n\n\n\n\n\n\nWe need to find the corresponding cell in the grid of the other group, \\(\\boldsymbol{g}_{x_1|s=1}\\). The correspondence is based on quantiles. First, we compute the c.d.f. at each coordinate of the grid: \\(F_{x_1|s=0}\\).\n\nFdR1_0 &lt;- Vectorize(function(x) mean(D_SXY_0$X1 &lt;= x))\nf1_0_example &lt;- FdR1_0(vx1_example_0)\nf1_0_example\n\n [1] 0.00 0.01 0.08 0.18 0.33 0.52 0.72 0.86 0.94 0.97 1.00\n\n\nThe c.d.f. in the identified cell, \\(F_{x_1|s=0}[i_0]\\), is:\n\n(p &lt;- f1_0_example[i_0])\n\n[1] 0.94\n\n\n\n\nCode\npar(mar = c(2,2,0,0))\nplot(\n  vx1_example_0, vx1_example_0*0, \n  xlab = \"\", ylab = \"\", axes = FALSE, col = NA, \n  xlim = c(-4, 4), ylim = c(.5, 1.7)\n)\naxis(1)\nfor(i in 1:n_grid_example) {\n  rect(\n    vx1_example_0[i], .5, vx1_example_0[i+1], 1, border=\"grey\",\n    # Add colours depending on c.d.f.\n    col = scales::alpha(colours[\"A\"], f1_0_example[i])\n  )\n}\npoints(D_SXY_0$X1, y = rep(.6, nrow(D_SXY_0)), col = alpha(colours[\"A\"], .3), pch = 19, cex = .2)\npoints(vx1_example_0_mid, y = rep(.75, n_grid_example), col = colours[\"A\"], pch = 4, cex = 1.5)\n\nfor(i in 1:n_grid_example) {\n  rect(vx1_example_1[i], 1.1, vx1_example_1[i+1], 1.6, border=\"grey\")\n}\n\npoints(D_SXY_1$X1, y = rep(1.2, nrow(D_SXY_1)), col = alpha(colours[\"B\"], .3), pch = 19, cex = .2)\npoints(vx1_example_1_mid, y = rep(1.35, n_grid_example), col = colours[\"B\"], pch = 4, cex = 1.5)\n\n# Point of interest\nx1 &lt;- D_SXY_0[id_indiv, \"X1\"]\npoints(D_SXY_0[id_indiv, \"X1\"], .6, col = \"red\", pch = 19, cex = 1.5)\n\n# Cell in which the individual belongs to\nrect(vx1_example_0[i_0], .5, vx1_example_0[i_0+1], 1, border=\"red\")\n\n\n\n\n\nFigure 2.4: The colour of the cells depend on the cumulative distribution for the point in the middle of the grid\n\n\n\n\n\n\n\n\nWe need to find the quantile level \\(u\\) that is the closest to the probability computed previously (p). The quantile levels are the following:\n\n(u &lt;- (1:n_grid_example) / (n_grid_example + 1))\n\n [1] 0.09090909 0.18181818 0.27272727 0.36363636 0.45454545 0.54545455\n [7] 0.63636364 0.72727273 0.81818182 0.90909091\n\n\nAnd the closest quantile level is:\n\n(i_1 &lt;- which.min(abs(u - p)))\n\n[1] 10\n\n\nWe need to find the quantile for that level in the distribution of the other group. To that end, we compute the quantiles of \\(x_1\\) in group \\(s=1\\), i.e., \\(Q_{x_1|s=1}\\):\n\nQtl1_1 &lt;- Vectorize(function(x) quantile(D_SXY_1$X1, x))\n(q1_1_example &lt;- Qtl1_1(u))\n\n9.090909% 18.18182% 27.27273% 36.36364% 45.45455% 54.54545% 63.63636% 72.72727% \n0.6067436 0.8658632 1.1029531 1.1904745 1.4091226 1.6073207 1.8403510 2.2358935 \n81.81818% 90.90909% \n2.4727195 2.9386579 \n\n\nThe transported value for \\(x_1\\), \\(x_1^* = T_1^*(x_1)\\), is thus:\n\n(x1star &lt;- q1_1_example[i_1])\n\n90.90909% \n 2.938658 \n\n\nAnd that point belongs in the following cell \\(\\boldsymbol{g}_{x_1, k_1|s=0}\\):\n\n(k1 &lt;- which.min(abs(vx1_example_1_mid - x1star)))\n\n[1] 9\n\n\n\n\nCode\npar(mar = c(2,2,0,0))\nplot(\n  vx1_example_0, vx1_example_0*0, \n  xlab = \"\", ylab = \"\", axes = FALSE, col = NA, \n  xlim = c(-4, 4), ylim = c(.5, 1.7)\n)\naxis(1)\nfor(i in 1:n_grid_example) {\n  rect(\n    vx1_example_0[i], .5, vx1_example_0[i+1], 1, border=\"grey\",\n    col = scales::alpha(colours[\"A\"], f1_0_example[i])\n  )\n}\npoints(D_SXY_0$X1, y = rep(.6, nrow(D_SXY_0)), col = alpha(colours[\"A\"], .3), pch = 19, cex = .2)\npoints(vx1_example_0_mid, y = rep(.75, n_grid_example), col = colours[\"A\"], pch = 4, cex = 1.5)\n\nfor(i in 1:n_grid_example) {\n  rect(vx1_example_1[i], 1.1, vx1_example_1[i+1], 1.6, border=\"grey\",\n       col = scales::alpha(colours[\"B\"], u[i])\n  )\n}\n\npoints(D_SXY_1$X1, y = rep(1.2, nrow(D_SXY_1)), col = alpha(colours[\"B\"], .3), pch = 19, cex = .2)\npoints(vx1_example_1_mid, y = rep(1.35, n_grid_example), col = colours[\"B\"], pch = 4, cex = 1.5)\n\n# Individual of interest\nx1 &lt;- D_SXY_0[id_indiv, \"X1\"]\npoints(D_SXY_0[id_indiv, \"X1\"], .6, col = \"red\", pch = 19, cex = 1.5)\n\n# Cell in which the individual belongs to\nrect(vx1_example_0[i_0], .5, vx1_example_0[i_0+1], 1, border=\"red\")\n\n# Corresponding cell in other group, based on quantile\nrect(vx1_example_1[k1], 1.1, vx1_example_1[k1+1], 1.6, border=\"blue\")\n\npoints(x1star, y = 1.35, col = \"black\", pch = 19, cex = 1.5)\n\n\n\n\n\nFigure 2.5: The matched cell in the other group, and the transport of \\(x_1\\) from \\(s=0\\) to \\(s=1\\)\n\n\n\n\n\n\n\n\n\n\n2.3.0.2 Transport of \\(x_2\\)\nNow that we have transported \\(x_1\\) for our individual, we need to transport \\(x_2\\), to obtain \\(x_2^* = T_2^*(x_2 | x_1)\\).\nAs before, we define coordinates for a grid in the second dimension, beginning with \\(\\boldsymbol{g}_{x_2 | s = 0}\\)\n\nvx2_example_0 &lt;- seq(\n  min(D_SXY_0$X2) - h, \n  max(D_SXY_0$X2) + h, \n  length = n_grid_example + 1\n)\nvx2_example_0 # coordinates for the cells in dimension 2 for group 0\n\n [1] -3.8943774 -3.2944261 -2.6944748 -2.0945235 -1.4945722 -0.8946210\n [7] -0.2946697  0.3052816  0.9052329  1.5051842  2.1051355\n\n\nand then \\(\\boldsymbol{g}_{x_2 | s = 1}\\):\n\nvx2_example_1 &lt;- seq(\n  min(D_SXY_1$X2) - h, \n  max(D_SXY_1$X2) + h, \n  length = n_grid_example + 1\n)\nvx2_example_1 # coordinates for the cells in dimension 2 for group 1\n\n [1] -0.9319286 -0.3678532  0.1962223  0.7602977  1.3243731  1.8884485\n [7]  2.4525239  3.0165994  3.5806748  4.1447502  4.7088256\n\n\nFor illustration, we would like to display the contour of the density in each group on the graphs. To do so, we rely on a kernel density estimation:\n\n# Computation of smoothing parameters (bandwidth) for kernel density estimation\nH0 &lt;- Hpi(D_SXY0[, c(\"X1\",\"X2\")])\nH1 &lt;- Hpi(D_SXY1[, c(\"X1\",\"X2\")])\n\n# Calculating multivariate densities in each group\nf0_2d &lt;- kde(D_SXY0[, c(\"X1\",\"X2\")], H = H0, xmin = c(-5, -5), xmax = c(5, 5))\nf1_2d &lt;- kde(D_SXY1[, c(\"X1\",\"X2\")], H = H1, xmin = c(-5, -5), xmax = c(5, 5))\n\nThe grids can be drawn, and we can display the contour:\n\n\nCode\npar(mar = c(2,2,0,0))\nplot(\n  vx1_example_0, vx2_example_0, \n  xlab = \"\", ylab = \"\", axes = FALSE, col = NA, \n  xlim = c(-4, 5), ylim = c(-4, 5)\n)\naxis(1)\naxis(2)\n# Grid for Group 0\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_0[i], ybottom = vx2_example_0[j], \n      xright = vx1_example_0[i+1], ytop = vx2_example_0[j+1],\n      border = alpha(colours[\"A\"], .4)\n    )\n}\n# Observed values\npoints(D_SXY_0$X1, D_SXY_0$X2, col = alpha(colours[\"A\"], .4), pch = 19, cex = .1)\n# Estimated density\ncontour(\n  f0_2d$eval.point[[1]],\n  f0_2d$eval.point[[2]],\n  f0_2d$estimate,col=scales::alpha(colours[\"A\"], 1),\n  add = TRUE\n)\n# Grid for Group 1\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_1[i], ybottom = vx2_example_1[j], \n      xright = vx1_example_1[i+1], ytop = vx2_example_1[j+1],\n      border = alpha(colours[\"B\"], .4)\n    )\n}\npoints(D_SXY_1$X1, D_SXY_1$X2, col = alpha(colours[\"B\"], .4), pch = 19, cex = .1)\ncontour(\n  f1_2d$eval.point[[1]],\n  f1_2d$eval.point[[2]],\n  f1_2d$estimate,col=scales::alpha(colours[\"B\"], 1),\n  add = TRUE\n)\n# Point of interest\npoints(indiv$X1, indiv$X2, col = \"red\", pch = 19)\n\n\n\n\n\nFigure 2.6: The individual of interest with its two coordinates, and the two estimated densities (one for \\(s=0\\) and one for \\(s=1\\))\n\n\n\n\n\n\n\n\nThe cell in the dimension \\(x_1\\) in which the point of interest belongs to has not changed:\n\n# identify closest cell in s=0 for x1 coordinate\n(k0 &lt;- which.min(abs(vx1_example_0_mid - x1)))\n\n[1] 9\n\n\nAnd we need to find the cell in the other dimension this point belongs to. Let us define the midpoints in the second dimension in group \\(s=1\\):\n\nvx2_example_1_mid &lt;- \n  (vx2_example_1[2:(1 + n_grid_example)] + vx2_example_1[1:(n_grid_example)]) / 2\n\n\n\nCode\npar(mar = c(2,2,0,0))\nplot(\n  vx1_example_0, vx2_example_0, \n  xlab = \"\", ylab = \"\", axes = FALSE, col = NA, \n  xlim = c(-4, 5), ylim = c(-4, 5)\n)\naxis(1)\naxis(2)\n# Grid for Group 0\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_0[i], ybottom = vx2_example_0[j], \n      xright = vx1_example_0[i+1], ytop = vx2_example_0[j+1],\n      border = alpha(colours[\"A\"], .4)\n    )\n}\n# Observed values\npoints(D_SXY_0$X1, D_SXY_0$X2, col = alpha(colours[\"A\"], .4), pch = 19, cex = .1)\n# Estimated density\ncontour(\n  f0_2d$eval.point[[1]],\n  f0_2d$eval.point[[2]],\n  f0_2d$estimate,col=scales::alpha(colours[\"A\"], 1),\n  add = TRUE\n)\n# Grid for Group 1\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_1[i], ybottom = vx2_example_1[j], \n      xright = vx1_example_1[i+1], ytop = vx2_example_1[j+1],\n      border = alpha(colours[\"B\"], .4)\n    )\n}\npoints(D_SXY_1$X1, D_SXY_1$X2, col = alpha(colours[\"B\"], .4), pch = 19, cex = .1)\ncontour(\n  f1_2d$eval.point[[1]],\n  f1_2d$eval.point[[2]],\n  f1_2d$estimate,col=scales::alpha(colours[\"B\"], 1),\n  add = TRUE\n)\n# Point of interest\npoints(indiv$X1, indiv$X2, col = \"red\", pch = 19)\n# identified cells in the dimension x1, in group 0\nrect(\n  xleft = vx1_example_0[k0], ybottom = vx2_example_0[1], \n  xright = vx1_example_0[k0+1], ytop = vx2_example_0[n_grid_example+1],\n  border=\"red\"\n)\n# corresponding cells in the same dimension, in group 1\nrect(\n  xleft = vx1_example_1[k0], ybottom = vx2_example_1[1], \n  xright = vx1_example_1[k0+1], ytop = vx2_example_1[n_grid_example+1],\n  border=\"blue\"\n)\n# midpoints of these cells\npoints(\n  rep(vx1_example_1_mid[k0],n_grid_example), vx2_example_1_mid, \n  col = colours[\"B\"], pch = 4\n)\n\n\n\n\n\nFigure 2.7: The transported point from \\(s=0\\) to \\(s=1\\) when sequentially transporting by \\(x_1\\) and then by \\(x_2 | x_1\\) will be in the blue rectangle\n\n\n\n\n\n\n\n\nThe coordinate \\(x_2\\) of the individuals is:\n\n(x2 &lt;- indiv$X2)\n\n[1] 0.008499458\n\n\nAnd the cell in which this point belongs to in the grid for that dimension (\\(\\boldsymbol{g}_{x_2|s=0}\\)) needs to be identified. To do so, we look at how close the point is from each middle point of the grid from the second coordinate in group \\(s=0\\):\n\nvx2_example_0_mid &lt;- \n  (vx2_example_0[2:(1 + n_grid_example)] + vx2_example_0[1:(n_grid_example)]) / 2\n# identify closest cell in s=0 for x2 coordinate\n(i_0 &lt;- which.min(abs(vx2_example_0_mid - x2)))\n\n[1] 7\n\n\n\n\nCode\npar(mar = c(2,2,0,0))\nplot(\n  vx1_example_0, vx2_example_0, \n  xlab = \"\", ylab = \"\", axes = FALSE, col = NA, \n  xlim = c(-4, 5), ylim = c(-4, 5)\n)\naxis(1)\naxis(2)\n# Grid for Group 0\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_0[i], ybottom = vx2_example_0[j], \n      xright = vx1_example_0[i+1], ytop = vx2_example_0[j+1],\n      border = alpha(colours[\"A\"], .4)\n    )\n}\n# Observed values\npoints(D_SXY_0$X1, D_SXY_0$X2, col = alpha(colours[\"A\"], .4), pch = 19, cex = .1)\n# Estimated density\ncontour(\n  f0_2d$eval.point[[1]],\n  f0_2d$eval.point[[2]],\n  f0_2d$estimate,col=scales::alpha(colours[\"A\"], 1),\n  add = TRUE\n)\n# Grid for Group 1\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_1[i], ybottom = vx2_example_1[j], \n      xright = vx1_example_1[i+1], ytop = vx2_example_1[j+1],\n      border = alpha(colours[\"B\"], .4)\n    )\n}\npoints(D_SXY_1$X1, D_SXY_1$X2, col = alpha(colours[\"B\"], .4), pch = 19, cex = .1)\ncontour(\n  f1_2d$eval.point[[1]],\n  f1_2d$eval.point[[2]],\n  f1_2d$estimate,col=scales::alpha(colours[\"B\"], 1),\n  add = TRUE\n)\n# Point of interest\npoints(indiv$X1, indiv$X2, col = \"red\", pch = 19)\n# identified cells in the dimension x1, in group 0\nrect(\n  xleft = vx1_example_0[k0], ybottom = vx2_example_0[1], \n  xright = vx1_example_0[k0+1], ytop = vx2_example_0[n_grid_example+1],\n  border=\"red\"\n)\n# Identified cell in the dimension x2, in group 0\nrect(\n  xleft = vx1_example_0[k0], ybottom = vx2_example_0[i_0], \n  xright = vx1_example_0[k0+1], ytop = vx2_example_0[i_0+1],\n  border = \"red\", lwd = 2\n)\n# corresponding cells in the same dimension, in group 1\nrect(\n  xleft = vx1_example_1[k0], ybottom = vx2_example_1[1], \n  xright = vx1_example_1[k0+1], ytop = vx2_example_1[n_grid_example+1],\n  border=\"blue\"\n)\n# midpoints of these cells\npoints(\n  rep(vx1_example_1_mid[k0],n_grid_example), vx2_example_1_mid, \n  col = colours[\"B\"], pch = 4\n)\n\n\n\n\n\nFigure 2.8: The cell in which the inddividual belongs to in the second coordinate is identified\n\n\n\n\n\n\n\n\nTo transport \\(x_2\\) conditionally on \\(x_1\\), we need to compute the c.d.f. of \\(x_2\\) in group 0 conditional on \\(x_1\\), i.e., \\(F_{x_2|s=0}[\\cdot, \\boldsymbol{i}]\\) (where \\(\\boldsymbol{i}=\\{1, \\ldots, 10\\}\\) here, since \\(x_2\\) only has one parent among \\(\\boldsymbol{v}\\)). To do so, we use an iterative process: we loop over the grid in the dimension of \\(x_1\\) (the parent), and then, for each cell, we identify the points in the neighborhood with respect to their \\(x_1\\) coordinates, and then, we can compute the c.d.f. at each point of the grid in the second dimension.\nDuring the iteration process, we also compute the quantiles of \\(x_2\\) conditional on \\(x_1\\), but in the group \\(s=1\\), i.e., \\(Q_{x_2|s=1}[\\cdot,\\boldsymbol{i}]\\).\n\n# setting a neighborhood\nd &lt;- .5\nF2_0 &lt;- matrix(NA, n_grid_example, n_grid_example)\nQ2_1 &lt;- matrix(NA, n_grid_example, n_grid_example)\n\nfor (i in 1:n_grid_example) {\n  # Identify points in group 0 close to the coordinate x1 of the midpoint of\n  # the current cell\n  idx1_0 &lt;- which(abs(D_SXY_0$X1 - vx1_example_0_mid[i]) &lt; d)\n  # c.d.f.\n  FdR2_0 &lt;- Vectorize(function(x) mean(D_SXY_0$X2[idx1_0] &lt;= x))\n  F2_0[,i] &lt;- FdR2_0(vx2_example_0_mid)\n  # Identify points in group 1 close to the coordinate x1 of the midpoint of\n  # the current cell\n  idx1_1 &lt;- which(abs(D_SXY_1$X1 - vx1_example_1_mid[i]) &lt; d)\n  Qtl2_1 &lt;- Vectorize(function(x) quantile(D_SXY_1$X2[idx1_1], x))\n  Q2_1[,i] = Qtl2_1(u)\n}\n\nLet us visualize the estimated c.d.f. in each cell of the complete grid in group 0:\n\n\nCode\npar(mar = c(2,2,0,0))\nplot(\n  vx1_example_0, vx2_example_0, \n  xlab = \"\", ylab = \"\", axes = FALSE, col = NA, \n  xlim = c(-4, 5), ylim = c(-4, 5)\n)\naxis(1)\naxis(2)\n# Grid for Group 0\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_0[i], ybottom = vx2_example_0[j], \n      xright = vx1_example_0[i+1], ytop = vx2_example_0[j+1],\n      border = alpha(colours[\"A\"], .4),\n      col = alpha(colours[\"A\"],  F2_0[i,j]/2)\n    )\n}\n# Observed values\npoints(D_SXY_0$X1, D_SXY_0$X2, col = alpha(colours[\"A\"], .4), pch = 19, cex = .1)\n# Estimated density\ncontour(\n  f0_2d$eval.point[[1]],\n  f0_2d$eval.point[[2]],\n  f0_2d$estimate,col=scales::alpha(colours[\"A\"], 1),\n  add = TRUE\n)\n# Grid for Group 1\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_1[i], ybottom = vx2_example_1[j], \n      xright = vx1_example_1[i+1], ytop = vx2_example_1[j+1],\n      border = alpha(colours[\"B\"], .4)\n    )\n}\npoints(D_SXY_1$X1, D_SXY_1$X2, col = alpha(colours[\"B\"], .4), pch = 19, cex = .1)\ncontour(\n  f1_2d$eval.point[[1]],\n  f1_2d$eval.point[[2]],\n  f1_2d$estimate,col=scales::alpha(colours[\"B\"], 1),\n  add = TRUE\n)\n# Point of interest\npoints(indiv$X1, indiv$X2, col = \"red\", pch = 19)\n# identified cells in the dimension x1, in group 0\nrect(\n  xleft = vx1_example_0[k0], ybottom = vx2_example_0[1], \n  xright = vx1_example_0[k0+1], ytop = vx2_example_0[n_grid_example+1],\n  border=\"red\"\n)\n# Identified cell in the dimension x2, in group 0\nrect(\n  xleft = vx1_example_0[k0], ybottom = vx2_example_0[i_0], \n  xright = vx1_example_0[k0+1], ytop = vx2_example_0[i_0+1],\n  border = \"red\", lwd = 2\n)\n# corresponding cells in the same dimension, in group 1\nrect(\n  xleft = vx1_example_1[k0], ybottom = vx2_example_1[1], \n  xright = vx1_example_1[k0+1], ytop = vx2_example_1[n_grid_example+1],\n  border=\"blue\"\n)\n# midpoints of these cells\npoints(\n  rep(vx1_example_1_mid[k0],n_grid_example), vx2_example_1_mid, \n  col = colours[\"B\"], pch = 4\n)\n\n\n\n\n\nFigure 2.9: The conditional c.d.f of \\(x_2 | x_1\\) in group \\(s=0\\).\n\n\n\n\n\n\n\n\nAnd the estimated conditional quantiles of \\(x_2 | x_1\\) in the group \\(s=1\\), \\(Q_{x_2|s=1[\\cdot, \\boldsymbol{i}]}\\):\n\n\nCode\npar(mar = c(2,2,0,0))\nplot(\n  vx1_example_0, vx2_example_0, \n  xlab = \"\", ylab = \"\", axes = FALSE, col = NA, \n  xlim = c(-4, 5), ylim = c(-4, 5)\n)\naxis(1)\naxis(2)\n# Grid for Group 0\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_0[i], ybottom = vx2_example_0[j], \n      xright = vx1_example_0[i+1], ytop = vx2_example_0[j+1],\n      border = alpha(colours[\"A\"], .4),\n      col = alpha(colours[\"A\"],  F2_0[i,j]/3)\n    )\n}\n# Observed values\npoints(D_SXY_0$X1, D_SXY_0$X2, col = alpha(colours[\"A\"], .4), pch = 19, cex = .1)\n# Estimated density\ncontour(\n  f0_2d$eval.point[[1]],\n  f0_2d$eval.point[[2]],\n  f0_2d$estimate,col=scales::alpha(colours[\"A\"], 1),\n  add = TRUE\n)\n# Grid for Group 1\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_1[i], ybottom = vx2_example_1[j], \n      xright = vx1_example_1[i+1], ytop = vx2_example_1[j+1],\n      border = alpha(colours[\"B\"], .4),\n      col = alpha(colours[\"B\"],  Q2_1[i,j]/3)\n    )\n}\npoints(D_SXY_1$X1, D_SXY_1$X2, col = alpha(colours[\"B\"], .4), pch = 19, cex = .1)\ncontour(\n  f1_2d$eval.point[[1]],\n  f1_2d$eval.point[[2]],\n  f1_2d$estimate,col=scales::alpha(colours[\"B\"], 1),\n  add = TRUE\n)\n# Point of interest\npoints(indiv$X1, indiv$X2, col = \"red\", pch = 19)\n# identified cells in the dimension x1, in group 0\nrect(\n  xleft = vx1_example_0[k0], ybottom = vx2_example_0[1], \n  xright = vx1_example_0[k0+1], ytop = vx2_example_0[n_grid_example+1],\n  border=\"red\"\n)\n# Identified cell in the dimension x2, in group 0\nrect(\n  xleft = vx1_example_0[k0], ybottom = vx2_example_0[i_0], \n  xright = vx1_example_0[k0+1], ytop = vx2_example_0[i_0+1],\n  border = \"red\", lwd = 2\n)\n# corresponding cells in the same dimension, in group 1\nrect(\n  xleft = vx1_example_1[k0], ybottom = vx2_example_1[1], \n  xright = vx1_example_1[k0+1], ytop = vx2_example_1[n_grid_example+1],\n  border=\"blue\"\n)\n\n\n\n\n\nFigure 2.10: The conditional quantiles of \\(x_2 | x_1\\) in group \\(s=1\\).\n\n\n\n\n\n\n\n\nLastly, we can identify the c.d.f. in the identified cell:\n\n# c.d.f. for the closest cell in s=0 for x2 coordinates\n(p &lt;- F2_0[i_0, k0])\n\n[1] 0.6\n\n\nThe corresponding quantile level:\n\n(i_1 &lt;- which.min(abs(u - p)))\n\n[1] 7\n\n\nAnd the quantile in group \\(s=1\\), which is therefore the transported value of \\(x_2 | x_1\\), \\(T_2^*(x_2 | x_1)\\):\n\n(x2star &lt;-  Q2_1[i_1, k1])\n\n[1] 1.025227\n\n\nThe corresponding cell in group \\(s=1\\) for dimension \\(x_2\\):\n\n(k2 &lt;- which.min(abs(vx2_example_1 - x2star)))\n\n[1] 4\n\n\n\nindiv # factual\n\n  S      X1          X2 Y\n2 0 0.87045 0.008499458 1\n\ntibble(S = 1, X1 = x1star, X2 = x2star) # counterfactual\n\n# A tibble: 1 × 3\n      S    X1    X2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  2.94  1.03\n\n\n\n\nCode\npar(mar = c(2,2,0,0))\nplot(\n  vx1_example_0, vx2_example_0, \n  xlab = \"\", ylab = \"\", axes = FALSE, col = NA, \n  xlim = c(-4, 5), ylim = c(-4, 5)\n)\naxis(1)\naxis(2)\n# Grid for Group 0\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_0[i], ybottom = vx2_example_0[j], \n      xright = vx1_example_0[i+1], ytop = vx2_example_0[j+1],\n      border = alpha(colours[\"A\"], .4),\n      col = alpha(colours[\"A\"],  F2_0[i,j]/3)\n    )\n}\n# Observed values\npoints(D_SXY_0$X1, D_SXY_0$X2, col = alpha(colours[\"A\"], .4), pch = 19, cex = .1)\n# Estimated density\ncontour(\n  f0_2d$eval.point[[1]],\n  f0_2d$eval.point[[2]],\n  f0_2d$estimate,col=scales::alpha(colours[\"A\"], 1),\n  add = TRUE\n)\n# Grid for Group 1\nfor (i in 1:n_grid_example) {\n  for (j in 1:n_grid_example)\n    rect(\n      xleft = vx1_example_1[i], ybottom = vx2_example_1[j], \n      xright = vx1_example_1[i+1], ytop = vx2_example_1[j+1],\n      border = alpha(colours[\"B\"], .4),\n      col = alpha(colours[\"B\"],  Q2_1[i,j]/3)\n    )\n}\npoints(D_SXY_1$X1, D_SXY_1$X2, col = alpha(colours[\"B\"], .4), pch = 19, cex = .1)\ncontour(\n  f1_2d$eval.point[[1]],\n  f1_2d$eval.point[[2]],\n  f1_2d$estimate,col=scales::alpha(colours[\"B\"], 1),\n  add = TRUE\n)\n# Point of interest\npoints(indiv$X1, indiv$X2, col = \"red\", pch = 19)\n# identified cells in the dimension x1, in group 0\nrect(\n  xleft = vx1_example_0[k0], ybottom = vx2_example_0[1], \n  xright = vx1_example_0[k0+1], ytop = vx2_example_0[n_grid_example+1],\n  border=\"red\"\n)\n# Identified cell in the dimension x2, in group 0\nrect(\n  xleft = vx1_example_0[k0], ybottom = vx2_example_0[i_0], \n  xright = vx1_example_0[k0+1], ytop = vx2_example_0[i_0+1],\n  border = \"red\", lwd = 2\n)\n# corresponding cells in the same dimension, in group 1\nrect(\n  xleft = vx1_example_1[k0], ybottom = vx2_example_1[1], \n  xright = vx1_example_1[k0+1], ytop = vx2_example_1[n_grid_example+1],\n  border=\"blue\"\n)\nrect(\n  xleft = vx1_example_1[k0], ybottom = vx2_example_1[k2], \n  xright = vx1_example_1[k0+1], ytop = vx2_example_1[k2+1],\n  border = \"blue\", lwd = 2\n)\npoints(x1star, x2star, col = \"black\", pch = 19)\n\n\n\n\nTransported value of (\\(s=0, x_1, x_2\\)) in group 0 to (\\(s=1, x_1^* = T_1*(x_1), x_2^* = T_2^*(x_2 | x_1)\\))",
    "crumbs": [
      "Simulations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#larger-grid",
    "href": "regression.html#larger-grid",
    "title": "2  Regression",
    "section": "2.4 Larger grid",
    "text": "2.4 Larger grid\nLet us now consider a much larger grid of \\(500\\times 500\\) instead of \\(10\\times 10\\).\n\nn_grid &lt;- 500\nh &lt;- .2\n\nWe can then compute the coordinates of each cell in each dimension, \\(g_{j|s}\\) for all variable \\(j \\in \\{x_1,x2\\}\\). As the bounds of the feature space might differ between each group (0 and 1), we make different grids for each group.\n\n# Group 0\n## First dimension (x1)\nvx1_0 &lt;- seq(\n  min(D_SXY_0$X1) - h, \n  max(D_SXY_0$X1) + h, \n  length = n_grid + 1\n)\n## Second dimension (x2)\nvx2_0 &lt;- seq(\n  min(D_SXY_0$X2) - h, \n  max(D_SXY_0$X2) + h, \n  length = n_grid + 1\n)\n# Group 1\n## First dimension (x1)\nvx1_1 &lt;- seq(\n  min(D_SXY_1$X1) - h, \n  max(D_SXY_1$X1) + h, \n  length = n_grid + 1\n)\n## Second dimension (x2)\nvx2_1 &lt;- seq(\n  min(D_SXY_1$X2) - h, \n  max(D_SXY_1$X2) + h, \n  length = n_grid + 1\n)\n\nLet us get the middle of each cell:\n\n# Group 1\nvx1_0_mid &lt;- (vx1_0[2:(1 + n_grid)] + vx1_0[1:(n_grid)]) / 2\nvx1_1_mid &lt;- (vx1_1[2:(1 + n_grid)] + vx1_1[1:(n_grid)]) / 2\n# Group 2\nvx2_0_mid &lt;- (vx2_0[2:(1 + n_grid)] + vx2_0[1:(n_grid)]) / 2\nvx2_1_mid &lt;- (vx2_1[2:(1 + n_grid)] + vx2_1[1:(n_grid)]) / 2\n\nWe compute the c.d.f. \\(F_{j|s}\\) and quantile functions \\(Q_{j|s}\\):\n\nF1_0 &lt;- F2_0 &lt;- F1_1 &lt;- F2_1 &lt;- matrix(NA, n_grid, n_grid)\nQ1_0 &lt;- Q2_0 &lt;- Q1_1 &lt;- Q2_1 &lt;- matrix(NA, n_grid, n_grid)\n\nu &lt;- (1:n_grid) / (n_grid + 1)\n\nFdR1_0 &lt;- Vectorize(function(x) mean(D_SXY_0$X1 &lt;= x))\nf1_0 &lt;- FdR1_0(vx1_0_mid)\nFdR2_0 &lt;- Vectorize(function(x) mean(D_SXY_0$X2 &lt;= x))\nf2_0 &lt;- FdR2_0(vx2_0_mid)\nFdR1_1 &lt;- Vectorize(function(x) mean(D_SXY_1$X1 &lt;= x))\nf1_1 &lt;- FdR1_1(vx1_1_mid)\nFdR2_1 &lt;- Vectorize(function(x) mean(D_SXY_1$X2&lt;=x))\nf2_1 &lt;- FdR2_1(vx2_1_mid)\n\nQtl1_0 &lt;- Vectorize(function(x) quantile(D_SXY_0$X1, x))\nq1_0 &lt;- Qtl1_0(u)\nQtl2_0 &lt;- Vectorize(function(x) quantile(D_SXY_0$X2, x))\nq2_0 &lt;- Qtl1_0(u)\nQtl1_1 &lt;- Vectorize(function(x) quantile(D_SXY_1$X1, x))\nq1_1 &lt;- Qtl1_1(u)\nQtl2_1 &lt;- Vectorize(function(x) quantile(D_SXY_1$X2, x))\nq2_1 &lt;- Qtl2_1(u)\n\nThen we loop on the cells of the grid of \\(x_1\\) to compute the conditional c.d.f. in group 0 and 1, and the quantiles in group 0 and 1.\n\nfor (i in 1:n_grid) {\n  idx1_0 &lt;- which(abs(D_SXY_0$X1 - vx1_0_mid[i]) &lt; d)\n  FdR2_0 &lt;- Vectorize(function(x) mean(D_SXY_0$X2[idx1_0] &lt;= x))\n  F2_0[, i] &lt;- FdR2_0(vx2_0_mid)\n  Qtl2_0 &lt;- Vectorize(function(x) quantile(D_SXY_0$X2[idx1_0], x))\n  Q2_0[, i] &lt;- Qtl2_0(u)\n  idx2_0 &lt;- which(abs(D_SXY_0$X2 - vx2_0_mid[i]) &lt; d)\n  FdR1_0 &lt;- Vectorize(function(x) mean(D_SXY_0$X1[idx2_0] &lt;= x))\n  F1_0[, i] &lt;- FdR1_0(vx1_0_mid)\n  Qtl1_0 &lt;- Vectorize(function(x) quantile(D_SXY_0$X1[idx2_0], x))\n  Q1_0[, i] &lt;- Qtl1_0(u)\n  idx1_1 &lt;- which(abs(D_SXY_1$X1 - vx1_1_mid[i]) &lt; d)\n  FdR2_1 &lt;- Vectorize(function(x) mean(D_SXY_1$X2[idx1_1] &lt;= x))\n  F2_1[, i] &lt;- FdR2_1(vx2_1_mid)\n  Qtl2_1 &lt;- Vectorize(function(x) quantile(D_SXY_1$X2[idx1_1], x))\n  Q2_1[, i] &lt;- Qtl2_1(u)\n  idx2_1 &lt;- which(abs(D_SXY_1$X2 - vx2_1_mid[i]) &lt; d)\n  FdR1_1 &lt;- Vectorize(function(x) mean(D_SXY_1$X1[idx2_1] &lt;= x))\n  F1_1[, i] &lt;- FdR1_1(vx1_1_mid)\n  Qtl1_1 &lt;- Vectorize(function(x) quantile(D_SXY_1$X1[idx2_1], x))\n  Q1_1[, i] &lt;- Qtl1_1(u) \n}\n\nLet us make predictions with the model presented in Section 2.2 on a grid:\n\nvx0 &lt;- seq(-5, 5, length = 251)\ndata_grid &lt;- expand.grid(x = vx0, y = vx0)\nL0 &lt;- logistique_reg(x1 = data_grid$x, x2 = data_grid$y, s = 0)\nL1 &lt;- logistique_reg(x1 = data_grid$x, x2 = data_grid$y, s = 1)\n\n# as a grid:\ndlogistique0 &lt;- matrix(L0, length(vx0), length(vx0))\ndlogistique1 &lt;- matrix(L1, length(vx0), length(vx0))\n\nAs in the illustrative example, we estimate the multivariate densities using kernel estimators. This will only be useful for visualization purposes.\n\n# Computation of smoothing parameters (bandwidth) for kernel density estimation\nH0 &lt;- Hpi(D_SXY0[, c(\"X1\",\"X2\")])\nH1 &lt;- Hpi(D_SXY1[, c(\"X1\",\"X2\")])\n\n# Calculating multivariate densities in each group\nf0_2d &lt;- kde(D_SXY0[, c(\"X1\",\"X2\")], H = H0, xmin = c(-5, -5), xmax = c(5, 5))\nf1_2d &lt;- kde(D_SXY1[, c(\"X1\",\"X2\")], H = H1, xmin = c(-5, -5), xmax = c(5, 5))\n\n\n2.4.1 Functions for Sequential Transport\nLet us define a function to perform the transport of \\(x_0\\) from \\(s=0\\) to \\(s_1\\), i.e., \\(T_1^*(x_1)\\)\n\n#' Transport of x1 from s=0 to s=1\n#' \n#' @param x2 vector of x2's values\n#' @descriptions\n#'  - vx1_0_mid: coordinates of center of cells (axis x1, s=0)\n#'  - f1_0: c.d.f. values for x1 in group s=0\n#'  - u: quantile levels\n#'  - q1_1: quantile values for x1 in group s=1\ntransport_x1 &lt;- function(x1) {\n  # identify closest cell of the grid to the coordinates of x1 in group s=0\n  i &lt;- which.min(abs(vx1_0_mid - x1))\n  # c.d.f. for that cell, in group s=0\n  p &lt;- f1_0[i]\n  # identify closest quantile level\n  i &lt;- which.min(abs(u - p))\n  # corresponding quantile in group s=1\n  x1star &lt;- q1_1[i]\n  x1star\n}\n\nAnother function to transport \\(x_2\\) from \\(s=0\\) to \\(s=1\\), i.e., \\(T_2^*(x_2)\\):\n\n#' Transport of x2 from s=0 to s=1\n#' \n#' @param x2 vector of x2's values\n#' @descriptions\n#'  - vx2_0_mid: coordinates of center of cells (axis x2, s=0)\n#'  - f2_0: c.d.f. values for x2 in group s=0\n#'  - u: quantile levels\n#'  - q2_1: quantile values of x2 in group s=1\ntransport_x2 &lt;- function(x2) {\n  # identify closest cell of the grid to the coordinates of x2 in group s=0\n  i &lt;- which.min(abs(vx2_0_mid - x2))\n  # c.d.f. for that cell, in group s=0\n  p &lt;- f2_0[i]\n  # identify closest quantile level\n  i &lt;- which.min(abs(u - p))\n  # corresponding quantile in group s=1\n  x2star &lt;- q2_1[i]\n  x2star\n}\n\nA function to transport \\(x_1\\) conditionally on \\(x_2\\), i.e., \\(T_1^*(x_1 | x_2)\\):\n\n#' Transport for x1 conditional on x2, from s=0 to s=1\n#' \n#' @param x1 numerical vector with x1's values\n#' @param x2 numerical vector with x2's values\n#' @description\n#'  - vx2_0_mid: coordinates of center of cells (axis x2, s=0)\n#'  - vx2_1_mid: coordinates of center of cells (axis x2, s=1)\n#'  - vx1_0_mid: coordinates of center of cells (axis x1, s=0)\ntransport_x1_cond_x2 &lt;- function(x1, x2) {\n  # identify closest cell in s=0 for x2 coordinate\n  k0 &lt;- which.min(abs(vx2_0_mid - x2))\n  # identify closest cell in s=1 for transported x2 coordinate\n  k1 &lt;- which.min(abs(vx2_1_mid - transport_x2(x2)))\n  # identify closest cell in s=0 for x1 coordinate\n  i_0 &lt;- which.min(abs(vx1_0_mid - x1))\n  # c.d.f. for the closest cell in s=0 for x1 coordinates\n  p &lt;- F1_0[i_0, k0]\n  # identify closest quantile level\n  i_1 &lt;- which.min(abs(u - p))\n  # corresponding quantile in group s=1\n  x1star &lt;- Q1_1[i_1, k1]\n  x1star\n}\n\nAnd a last function to transport \\(x_2\\) conditionally on \\(x_1\\), i.e., \\(T_2^*(x_2 |x_1)\\):\n\n#' Transport for x2 conditional on x1, from s=0 to s=1\n#' \n#' @param x2 numerical vector with x2's values\n#' @param x1 numerical vector with x1's values\n#' @description\n#'  - vx1_0_mid: coordinates of center of cells (axis x1, s=0)\n#'  - vx1_1_mid: coordinates of center of cells (axis x1, s=1)\n#'  - vx2_0_mid: coordinates of center of cells (axis x2, s=0)\ntransport_x2_cond_x1 &lt;-  function(x2, x1){\n  # identify closest cell in s=0 for x1 coordinate\n  k0 &lt;- which.min(abs(vx1_0_mid - x1))\n  # identify closest cell in s=1 for transported x1 coordinate\n  k1 &lt;- which.min(abs(vx1_1_mid - transport_x1(x1)))\n  # identify closest cell in s=0 for x2 coordinate\n  i_0 &lt;- which.min(abs(vx2_0_mid - x2))\n  # c.d.f. for the closest cell in s=0 for x2 coordinates\n  p &lt;- F2_0[i_0, k0]\n  # identify closest quantile level\n  i_1 &lt;- which.min(abs(u - p))\n  # corresponding quantile in group s=1\n  x2star &lt;-  Q2_1[i_1, k1]\n  x2star\n}\n\n\n\nCode\npar(mar = c(2,2,0,0))\n# Group 0\n## Estimated density: level curves for (x1, x2) -&gt; m(0, x1, x2)\ncontour(\n  f0_2d$eval.point[[1]],\n  f0_2d$eval.point[[2]],\n  f0_2d$estimate,col=scales::alpha(colours[\"A\"], 1),\n  axes = FALSE, xlab = \"\", ylab = \"\"\n)\n# Contour of estimates by the model\ncontour(\n  vx0,\n  vx0,\n  dlogistique0,\n  levels = (1:9) / 10,\n  col = scl, lwd = 1.6,\n  add = TRUE\n)\naxis(1)\naxis(2)\n\n\n\n\n\nFigure 2.11: Level curves for \\((x_1, x_2) \\rightarrow m(0, x_1, x_2)\\) In the background, estimated density of \\((s=0, x_1, x_2)\\) (level curves).\n\n\n\n\n\n\n\n\n\n\nCode\n# Group 1\n## Estimated density: level curves for (x1, x2) -&gt; m(1, x1, x2)\ncontour(\n  f1_2d$eval.point[[1]],\n  f1_2d$eval.point[[2]],\n  f1_2d$estimate,\n  col = scales::alpha(colours[\"B\"], 1),\n  axes = FALSE, xlab = \"\", ylab = \"\"\n)\n# Contour of estimates by the model\ncontour(\n  vx0,\n  vx0,\n  dlogistique1, \n  col = scl,\n  add = TRUE,\n  levels = (1:9) / 10,\n  lwd = 1.6\n)\naxis(1)\naxis(2)\n\n\n\n\n\nFigure 2.12: Level curves for \\((x_1, x_2) \\rightarrow m(1, x_1, x_2)\\) In the background, estimated density of \\((s=1, x_1, x_2)\\) (level curves).\n\n\n\n\n\n\n\n\nLet us focus on individual \\((s, -2, 1)\\).\n\nxystart &lt;- c(-2,-1)\n\nLet us make multiple predictions for that individual, given the assumed structural model.\n\n\n\n\n\nFigure 2.13: Causal network with two legitimate mitigating variables \\(x_1\\) and \\(x_2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.14: Causal network with two legitimate mitigating variables \\(x_1\\) and \\(x_2\\).\n\n\n\n\n\n\n\n\n\nx2_then_x1: \\(T_1^*(x1 | x2), T_2^*(x2)\\), the transported value when assuming the structural model shown in Figure 2.14\nx1_then_x2: \\(T_1^*(x1), T_2^*(x2|x_1)\\), the transported value when assuming the structural model shown in Figure 2.13\nx1_intermediaire: \\((T_1^*(x1), x2)\\), the transported value for \\(x_1\\) only\nx2_intermediaire: \\((x1, T_2^*(x2))\\), the transported value for \\(x_2\\) only.\n\n\nprediction &lt;- data.frame(\n  # 1. (x1, x2)\n  start = xystart,\n  # 2. (T_1(x1 | x2), T_2(x2))\n  x2_then_x1 = c(\n    transport_x1_cond_x2(x1 = xystart[1], x2 = xystart[2]),\n    transport_x2(xystart[2])\n  ),\n  # 3. (T_1(x1), T_2(x2 | x1))\n  x1_then_x2 = c(\n    transport_x1(xystart[1]), \n    transport_x2_cond_x1(x2 = xystart[1], x1 = xystart[2])\n  ),\n  # 4. (T_1(x1), x2)\n  x1_intermediaire = c(transport_x1(x1 = xystart[1]), xystart[2]),\n  # 5. (x1, T_2(x2))\n  x2_intermediaire = c(xystart[1], transport_x2(xystart[2]))\n)\n\nThen, we make predictions for all these points using the model.\n\nlibrary(plotrix)\nv &lt;- c(\n  # 1. Prediction at initial values (S=0, x1, x2)\n  logistique_reg(x1 = prediction$start[1], x2 = prediction$start[2], s = 0),\n  # 2. Prediction if (S=1, x1, x2)\n  logistique_reg(prediction$start[1], prediction$start[2], 1),\n  # 3. Prediction if (S = 1, T_1(x1), T_2(x2 | x1))\n  logistique_reg(prediction$x1_then_x2[1], prediction$x1_then_x2[2], 1),\n  # 4. Prediction if (S = 1, T_1(x1 | x2), T_2(x2))\n  logistique_reg(prediction$x2_then_x1[1], prediction$x2_then_x1[2], 1),\n  # 5. Prediction if (S = 1, T_1(x1), x2)\n  logistique_reg(prediction$x1_intermediaire[1], prediction$x1_intermediaire[2], 1),\n  # 6. Prediction if (S = 1, x1, T_2(x2))\n  logistique_reg(prediction$x2_intermediaire[1], prediction$x2_intermediaire[2], 1)\n)\nv\n\n[1] 0.18242552 0.07585818 0.40945028 0.54065719 0.25576437 0.23658466\n\n\nLet us first explore the mutatis mutandis difference \\(m(s=1, x_1^*, x_2^*) - m(s = 0, x1, x_2)\\). If we assume the causal structure as in Figure 2.13, it is equal to:\n\nv[3] - v[1]\n\n[1] 0.2270248\n\n\nThis can be decomposed as follows:\n\nthe change du to the impact of \\(s\\) only, \\(m(s=1,x_1,x_2) - m(s=0,x_1,x_2)\\) (i.e., the ceteris paribus impact):\n\n\nv[2] - v[1]\n\n[1] -0.1065673\n\n\n\nthe change due to the impact of \\(s\\) on \\(x_1\\), i.e., \\(m(s=1,x^\\star_1,x_2) - m(s=1,x_1,x_2)\\):\n\n\nv[5] - v[2]\n\n[1] 0.1799062\n\n\n\nthe change due to the impact of both \\(s\\) and \\(x_1\\) on \\(x_2\\), i.e., \\(m(s=1,x^\\star_1,x^\\star_2) - m(s=1,x^\\star_1,x_2)\\):\n\n\nv[3] - v[5]\n\n[1] 0.1536859\n\n\nIf, instead, we assume the causal structure as in Figure 2.14, the mutatis mutandis effect, \\(m(s=1, x_1^*, x_2^*) - m(s = 0, x1, x_2)\\), would be:\n\nv[4] - v[1]\n\n[1] 0.3582317\n\n\nThis can be decomposed as follows:\n\nthe change du to the impact of \\(s\\) only, \\(m(s=1,x_1,x_2) - m(s=0,x_1,x_2)\\) (i.e., the ceteris paribus impact):\n\n\nv[2] - v[1]\n\n[1] -0.1065673\n\n\n\nthe change due to the impact of \\(s\\) on \\(x_2\\), i.e., \\(m(s=1,x_1,x_2^*) - m(s=1,x_1,x_2)\\):\n\n\nv[6] - v[2]\n\n[1] 0.1607265\n\n\n\nthe change due to the impact of both \\(s\\) and \\(x_2\\) on \\(x_1\\), i.e., \\(m(s=1,x^\\star_1,x^\\star_2) - m(s=1,x_1,x_2^*)\\):\n\n\nv[4] - v[6]\n\n[1] 0.3040725\n\n\nLet us visualize this ! We begin with the predicted value by the model.\n\npar(mar = c(2, 2, 0, 0))\n# Group 0\n## Estimated density: level curves for (x1, x2) -&gt; m(0, x1, x2)\nCeX &lt;- 1\ncontour(\n  f0_2d$eval.point[[1]],\n  f0_2d$eval.point[[2]],\n  f0_2d$estimate,\n  col = scales::alpha(colours[\"A\"], .3),\n  axes = FALSE, xlab = \"\", ylab = \"\"\n)\n# Group 1\n## Estimated density: level curves for (x1, x2) -&gt; m(1, x1, x2)\ncontour(\n  f1_2d$eval.point[[1]],\n  f1_2d$eval.point[[2]],\n  f1_2d$estimate,\n  col = scales::alpha(colours[\"B\"], .3), add = TRUE\n)\ncontour(\n  vx0, vx0, dlogistique0,\n  levels = (1:9)/10,\n  col = scl,\n  add = TRUE,lwd = 2\n)\naxis(1)\naxis(2)\n\n###\n# Individual (S=0, x1=-2, x2=-1)\n###\npoints(prediction$start[1], prediction$start[2], pch = 19, cex = CeX)\n## Predicted value for the individual, based on factuals\ntext(\n  prediction$start[1], prediction$start[2],\n  paste(round(v[1] * 100, 1), \"%\", sep = \"\"), \n  pos = 1, cex = CeX, col = \"darkblue\"\n)\n\n\n\n\nFigure 2.15: The predicted value by the model, \\(m(s=0, x_1, x_2)\\)\n\n\n\n\n\n\n\n\nAnd then we can show the the predictions made on the counterfactuals depending on the causal assumption.\n\npar(mar = c(2, 2, 0, 0))\n# Group 0\n## Estimated density: level curves for (x1, x2) -&gt; m(0, x1, x2)\ncontour(\n  f0_2d$eval.point[[1]],\n  f0_2d$eval.point[[2]],\n  f0_2d$estimate,\n  col = scales::alpha(colours[\"A\"], .3),\n  axes = FALSE, xlab = \"\", ylab = \"\"\n)\n# Group 1\n## Estimated density: level curves for (x1, x2) -&gt; m(1, x1, x2)\ncontour(\n  f1_2d$eval.point[[1]],\n  f1_2d$eval.point[[2]],\n  f1_2d$estimate,\n  col = scales::alpha(colours[\"B\"], .3), add = TRUE\n)\n\n# Contour of estimates by the model for s=1\ncontour(\n  vx0, vx0, dlogistique1,\n  levels = (1:9) / 10,\n  col = scl, lwd=2,\n  add = TRUE\n)\naxis(1)\naxis(2)\n\n###\n# Individual (s=0, x1=-2, x2=-1)\n###\npoints(prediction$start[1], prediction$start[2], pch = 19, cex = CeX)\n## Predicted value for the individual, based on factuals\ntext(\n  prediction$start[1] - .3, prediction$start[2] - .42 - .3,\n  paste(round(v[1] * 100, 1), \"%\", sep = \"\"), \n  pos = 1, cex = CeX, col = \"darkblue\"\n)\n\n###\n# Transported individual when transporting x1 and then x2, i.e.,\n# (do(s=1), T_1^*(x1), T_2^*(x_2 | x_1))\n###\npoints(prediction$x1_then_x2[1],prediction$x1_then_x2[2], pch = 19, cex = CeX)\nsegments(\n  x0 = prediction$start[1], y0 = prediction$start[2], \n  x1 = prediction$x1_then_x2[1], y1 = prediction$start[2], \n  lwd = .8\n)\nsegments(\n  x0 = prediction$x1_then_x2[1], y0 = prediction$x1_then_x2[2],\n  x1 = prediction$x1_then_x2[1], y1 = prediction$start[2], \n  lwd = .8\n)\n## Intermediate point\npoints(prediction$x1_then_x2[1], prediction$start[2], pch = 19, col = \"white\", cex = CeX)\npoints(prediction$x1_then_x2[1], prediction$start[2], pch = 1, cex = CeX)\ntext(\n  prediction$x1_then_x2[1], prediction$start[2] - .42,\n  paste(round(v[5] * 100, 1), \"%\", sep = \"\"), pos = 4, cex = CeX\n)\n# New predicted value for # (do(s=1), T_1^*(x1), T_2^*(x_2 | x_1))\ntext(\n  prediction$x1_then_x2[1], prediction$x1_then_x2[2],\n  paste(round(v[3]*100,1),\"%\",sep=\"\"), pos = 3, cex = CeX\n)\n\n###\n# Transported individual when transporting x2 and then x1, i.e.,\n# (do(s=1), T_1^*(x1 | x2), T_2^*(x_2))\n###\npoints(prediction$x2_then_x1[1],prediction$x2_then_x1[2],pch=19,cex=CeX)\nsegments(\n  x0 = prediction$start[1], y0 = prediction$start[2],\n  x1 = prediction$start[1], y1 = prediction$x2_then_x1[2],\n  lwd = .8\n)\nsegments(\n  x0 = prediction$x2_then_x1[1], y0 = prediction$x2_then_x1[2],\n  x1 = prediction$start[1], y1 = prediction$x2_then_x1[2],\n  lwd = .8\n)\n## Intermediate point\npoints(prediction$start[1], prediction$x2_then_x1[2], pch = 19, col = \"white\", cex = CeX)\npoints(prediction$start[1], prediction$x2_then_x1[2], pch = 1, cex = CeX)\ntext(\n  prediction$start[1], prediction$x2_then_x1[2],\n     paste(round(v[6] * 100, 1), \"%\", sep = \"\"), pos = 2, cex = CeX\n)\n## New predicted value for (do(s=1), T_1^*(x1 | x2), T_2^*(x_2))\ntext(\n  prediction$x2_then_x1[1], prediction$x2_then_x1[2],\n  paste(round(v[4] * 100, 1), \"%\", sep = \"\"), pos = 3, cex = CeX\n)\n\n\n###\n# New predicted value for (do(s=1), x1, x2), no transport\n###\nry &lt;- .2\ndraw.circle(\n  x = prediction$start[1] - ry, y = prediction$start[2] - ry, \n  radius = ry * sqrt(2)\n)\ntext(\n  prediction$start[1], prediction$start[2] + .42,\n  paste(round(v[2] * 100, 1), \"%\", sep = \"\"), pos = 4, cex = CeX\n)\n\n\n\n\nFigure 2.16: Two counterfactuals, based on either the causal assumption from Figure 2.13 (bottom right path) or on the causal assumption from Figure 2.14 (top left path)",
    "crumbs": [
      "Simulations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "3  Data",
    "section": "",
    "text": "3.1 Data Pre-Processing\nWe load the data:\ndf &lt;- read_csv('data/law_data.csv')\nHere is some summary information on this dataset:\nsummary(df)\n\n      ...1           race                sex             LSAT      \n Min.   :    0   Length:21791       Min.   :1.000   Min.   :11.00  \n 1st Qu.: 6516   Class :character   1st Qu.:1.000   1st Qu.:33.00  \n Median :13698   Mode  :character   Median :2.000   Median :37.00  \n Mean   :13732                      Mean   :1.562   Mean   :36.77  \n 3rd Qu.:20862                      3rd Qu.:2.000   3rd Qu.:41.00  \n Max.   :27476                      Max.   :2.000   Max.   :48.00  \n      UGPA       region_first            ZFYA           sander_index   \n Min.   :0.000   Length:21791       Min.   :-3.35000   Min.   :0.3875  \n 1st Qu.:3.000   Class :character   1st Qu.:-0.55000   1st Qu.:0.7116  \n Median :3.300   Mode  :character   Median : 0.09000   Median :0.7696  \n Mean   :3.227                      Mean   : 0.09643   Mean   :0.7669  \n 3rd Qu.:3.500                      3rd Qu.: 0.75000   3rd Qu.:0.8274  \n Max.   :4.200                      Max.   : 3.48000   Max.   :1.0000  \n    first_pf     \n Min.   :0.0000  \n 1st Qu.:1.0000  \n Median :1.0000  \n Mean   :0.8884  \n 3rd Qu.:1.0000  \n Max.   :1.0000\nThen, we focus on a subset of variables of interest:\ndf &lt;- df |&gt; \n  select(\n    race,\n    sex, # we can take S = gender\n    LSAT, # or S = race (white/black)\n    UGPA,\n    ZFYA # Y\n  )\nWe create a dataset where the only protected class is the race, and we focus on Black individuals and White individuals only:\n# Table for S = race\ndf_race &lt;- df |&gt; \n  select(\n    race,\n    UGPA,\n    LSAT,\n    ZFYA\n  ) |&gt; \n  filter(\n    race %in% c(\"Black\", \"White\")\n  ) |&gt; \n  rename(\n    S = race,\n    X1 = UGPA,\n    X2 = LSAT,\n    Y = ZFYA\n  ) |&gt;  # no NA values\n  mutate(\n    S = as.factor(S)\n  )\nAnd another dataset in which the only protected class is the sex:\n# Table for S = gender\ndf_gender &lt;- df |&gt; \n  select(\n    sex,\n    UGPA,\n    LSAT,\n    ZFYA\n  ) |&gt; \n  rename(\n    S = sex,\n    X1 = UGPA,\n    X2 = LSAT,\n    Y = ZFYA\n  ) |&gt;  # no NA values\n  mutate(\n    S = as.factor(S)\n  )",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#data-pre-processing",
    "href": "data.html#data-pre-processing",
    "title": "3  Data",
    "section": "",
    "text": "S = RaceS = Gender\n\n\n\nggplot(\n  data = df_race, \n  mapping = aes(x = Y, fill = S)\n) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), \n    alpha = 0.5, position = \"identity\", binwidth = 0.5\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Race\",\n    x = \"Y\",\n    y = \"Density\"\n  ) +\n  global_theme()\n\n\n\n\nFigure 3.1: Distribution of the standardized first-year law school grades among the two groups, when \\(S\\) is the race\n\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = df_gender, \n  mapping = aes(x = Y, fill = S)) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), \n    alpha = 0.5, position = \"identity\", binwidth = 0.5\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Gender\",\n    x = \"Y\",\n    y = \"Density\"\n  ) +\n  global_theme()\n\n\n\n\nFigure 3.2: Distribution of the standardized first-year law school grades among the two groups, when \\(S\\) is the gender",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#causal-graph",
    "href": "data.html#causal-graph",
    "title": "3  Data",
    "section": "3.2 Causal graph",
    "text": "3.2 Causal graph\nThe assumed causal graph we use here is different from that of the different papers De Lara et al. (2024), Kusner et al. (2017), Black, Yeom, and Fredrikson (2020) using the same dataset.\nWe make the following assumptions:\n\nThe sensitive attribute, (S) (race), has no parents.\nThe two other explanatory variables, (X_1) (UGPA) and (X_2) (LSAT), both directly depend on the sensitive attribute.\nThe second variable, (X_2) (LSAT), also depends on the first variable, (X_1) (UGPA). This is done for illustrative purposes, assuming that the score obtained on the LSAT is influenced by the UGPA.\nThe two variables, (X_1) (UGPA) and (X_2) (LSAT), cause the target variable (Y), i.e., whether the student obtained a high standardized first-year average (ZFYA).\n\nThe corresponding Structural Equation Model writes:\n\\[\n\\begin{cases}\nS: \\text{ sensitive attribute (race)} \\\\\nX_1 = h_1(S, U_1): \\text{ UGPA, dependent on } S \\\\\nX_2 = h_2(S, X_1, U_2): \\text{ LSAT, dependent on } S \\text{ and } X_1 \\\\\nY = h_3(X_1, X_2, U_Y): \\text{ ZFYA, dependent on } X_1 \\text{ and } X_2 \\\\\n\\end{cases}\n\\]\nwhere (U_1), (U_2), and (U_Y) are independent error terms.\nIn R, we construct the upper triangular adjacency matrix to reflect our assumed causal structure:\n\nvariables &lt;- colnames(df_race)\n# Adjacency matrix: upper triangular\nadj &lt;- matrix(\n  c(0, 1, 1, 1,\n    0, 0, 1, 1,\n    0, 0, 0, 1,\n    0, 0, 0, 0),\n  ncol = length(variables), \n  dimnames = rep(list(variables), 2),\n  byrow = TRUE\n)\n\nWhich can be visualized as follows:\n\ncausal_graph &lt;- fairadapt::graphModel(adj)\nplot(causal_graph)\n\n\n\n\nFigure 3.3: Causal Graph\n\n\n\n\n\n\n\n\nThe topological order:\n\ntop_order &lt;- variables\ntop_order\n\n[1] \"S\"  \"X1\" \"X2\" \"Y\"",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-save",
    "href": "data.html#sec-data-save",
    "title": "3  Data",
    "section": "3.3 Saving objects",
    "text": "3.3 Saving objects\n\nsave(df_race, file = \"data/df_race.rda\")\nsave(df_gender, file = \"data/df_gender.rda\")\nsave(adj, file = \"data/adj.rda\")\n\n\n\n\n\nBlack, Emily, Samuel Yeom, and Matt Fredrikson. 2020. “Fliptest: Fairness Testing via Optimal Transport.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 111–21.\n\n\nDe Lara, Lucas, Alberto González-Sanz, Nicholas Asher, Laurent Risser, and Jean-Michel Loubes. 2024. “Transport-Based Counterfactual Models.” Journal of Machine Learning Research 25 (136): 1–59.\n\n\nKusner, Matt J, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. “Counterfactual Fairness.” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 4066–76. NIPS.\n\n\nLara, Lucas de, Alberto González-Sanz, Nicholas Asher, and Jean-Michel Loubes. 2021. “Transport-Based Counterfactual Models.” arXiv 2108.13025.\n\n\nSander, Richard H. 2004. “A Systemic Analysis of Affirmative Action in American Law Schools.” Stan. L. Rev. 57: 367.\n\n\nWightman, Linda F. 1998. “LSAC National Longitudinal Bar Passage Study. LSAC Research Report Series.” In. https://api.semanticscholar.org/CorpusID:151073942.",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "classifier.html",
    "href": "classifier.html",
    "title": "4  Classifier",
    "section": "",
    "text": "4.1 Load Data\nWe load the data obtained in Chapter 3.3:\nload(\"data/df_race.rda\")",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classifier</span>"
    ]
  },
  {
    "objectID": "classifier.html#pre-processing",
    "href": "classifier.html#pre-processing",
    "title": "4  Classifier",
    "section": "4.2 Pre-processing",
    "text": "4.2 Pre-processing\nFirst, we transform \\(Y\\) into a binary variable:\n\nmed &lt;- median(df_race$Y)\ndf_race_c &lt;- df_race |&gt; \n  mutate(\n    Y_c = ifelse(Y &gt; med, 1, 0)\n  ) |&gt; \n  select(S, X1, X2, Y = Y_c)\n\nWe turn the response variable to a factor:\n\ndf_race_c$Y &lt;- as.factor(df_race_c$Y)\nlevels(df_race_c$Y)\n\n[1] \"0\" \"1\"\n\n\nLet us split the dataset into train/test sets (we use the split_dataset() function defined in functions/utils.R):\n\nseed &lt;- 2025\nsets &lt;- split_dataset(df_race_c, seed)\ndata_train &lt;- sets$data_train\ndata_test &lt;- sets$data_test",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classifier</span>"
    ]
  },
  {
    "objectID": "classifier.html#training-the-model",
    "href": "classifier.html#training-the-model",
    "title": "4  Classifier",
    "section": "4.3 Training the Model",
    "text": "4.3 Training the Model\nThen, we train two models:\n\nunaware logistic regression classifier: model without including the sensitive attribute.\naware logistic regression classifier: model with the sensitive attribute included in the set of features.\n\nThe model is trained using the log_reg_train() function defined in functions/utils.R:\n\nlog_reg_train\n\nfunction (train_data, test_data, type = c(\"aware\", \"unaware\")) \n{\n    if (type == \"unaware\") {\n        train_data_ &lt;- train_data %&gt;% select(-S)\n        test_data_ &lt;- test_data %&gt;% select(-S)\n    }\n    else {\n        train_data_ &lt;- train_data\n        test_data_ &lt;- test_data\n    }\n    model &lt;- glm(Y ~ ., data = train_data_, family = binomial)\n    pred_train &lt;- predict(model, newdata = train_data_, type = \"response\")\n    pred_test &lt;- predict(model, newdata = test_data_, type = \"response\")\n    list(model = model, pred_train = pred_train, pred_test = pred_test)\n}\n\n\nLet us train the two models. Then, we extract the predicted values on both the train set and the test set.\n\n# Unaware logistic regression classifier (model without S)\npred_unaware &lt;- log_reg_train(data_train, data_test, type = \"unaware\")\npred_unaware_train &lt;- pred_unaware$pred_train\npred_unaware_test &lt;- pred_unaware$pred_test\n\n# Aware logistic regression classifier (model with S)\npred_aware &lt;- log_reg_train(data_train, data_test, type = \"aware\")\npred_aware_train &lt;- pred_aware$pred_train\npred_aware_test &lt;- pred_aware$pred_test\n\nWe create a table for each model, with the sensitive attribute and the predicted value by the model (()), only for observations from the test set.\n\ndf_test_unaware &lt;- tibble(\n  S = data_test$S, \n  pred = pred_unaware_test\n)\n\ndf_test_aware &lt;- tibble(\n  S = data_test$S, \n  pred = pred_aware_test\n)\n\n\nUnawareAware\n\n\n\nggplot(\n  data = df_test_unaware, \n  mapping = aes(x = pred, fill = S)) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), \n    alpha = 0.5, position = \"identity\", binwidth = 0.05\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Unaware Model, with S being Race\",\n    x = \"Predictions for Y\",\n    y = \"Density\"\n  ) +\n  global_theme()\n\n\n\n\nFigure 4.1: Density of predictions on the test set, for the unaware model, when the sensitive attribute is the race\n\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = df_test_aware,\n  mapping = aes(x = pred, fill = S)) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), \n    alpha = 0.5, position = \"identity\", binwidth = 0.05\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Aware Model, with S being Race\",\n    x = \"Predictions for Y\",\n    y = \"Density\"\n  ) +\n  global_theme()\n\n\n\n\nFigure 4.2: Density of predictions on the test set, for the aware model, when the sensitive attribute is the race",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classifier</span>"
    ]
  },
  {
    "objectID": "classifier.html#predictions",
    "href": "classifier.html#predictions",
    "title": "4  Classifier",
    "section": "4.4 Predictions",
    "text": "4.4 Predictions\nWe predict values with the unaware model on the factuals:\n\nmodel_unaware &lt;- pred_unaware$model\npred_unaware_all &lt;- predict(\n  model_unaware, \n  newdata = df_race_c |&gt; select(S, X1, X2), \n  type = \"response\"\n)\n\nAnd with the aware model:\n\nmodel_aware &lt;- pred_aware$model\npred_aware_all &lt;- predict(\n  model_aware, \n  newdata = df_race_c |&gt; select(S, X1, X2), \n  type = \"response\"\n)",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classifier</span>"
    ]
  },
  {
    "objectID": "classifier.html#sec-classifier-save",
    "href": "classifier.html#sec-classifier-save",
    "title": "4  Classifier",
    "section": "4.5 Saving Objects",
    "text": "4.5 Saving Objects\n\nsave(df_race_c, file = \"data/df_race_c.rda\")\nsave(pred_aware, file = \"data/pred_aware.rda\")\nsave(pred_unaware, file = \"data/pred_unaware.rda\")\nsave(pred_unaware_all, file = \"data/pred_unaware_all.rda\")\nsave(pred_aware_all, file = \"data/pred_aware_all.rda\")\n\n\n\n\n\nBlack, Emily, Samuel Yeom, and Matt Fredrikson. 2020. “Fliptest: Fairness Testing via Optimal Transport.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 111–21.\n\n\nKusner, Matt J, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. “Counterfactual Fairness.” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 4066–76. NIPS.\n\n\nPlečko, Drago, Nicolas Bennett, and Nicolai Meinshausen. 2021. “Fairadapt: Causal Reasoning for Fair Data Pre-Processing.” arXiv Preprint arXiv:2110.10200.",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classifier</span>"
    ]
  },
  {
    "objectID": "cf-fairadapt.html",
    "href": "cf-fairadapt.html",
    "title": "5  Fairadapt",
    "section": "",
    "text": "5.1 Load Data and Classifier\nWe load the dataset where the sensitive attribute ((S)) is the race, obtained Chapter 3.3:\nload(\"data/df_race.rda\")\nWe also load the dataset where the sensitive attribute is also the race, but where where the target variable ((Y), ZFYA) is binary (1 if the student obtained a standardized first year average over the median, 0 otherwise). This dataset was saved in Chapter 4.5:\nload(\"data/df_race_c.rda\")\nWe also need the predictions made by the classifier (see Chapter 4):\n# Predictions on train/test sets\nload(\"data/pred_aware.rda\")\nload(\"data/pred_unaware.rda\")\n# Predictions on the factuals, on the whole dataset\nload(\"data/pred_aware_all.rda\")\nload(\"data/pred_unaware_all.rda\")\nWe load the adjacency matrix that translates the assumed causal structure, obtained in @Chapter 3.3:\nload(\"data/adj.rda\")",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fairadapt</span>"
    ]
  },
  {
    "objectID": "cf-fairadapt.html#counterfactuals-with-fairadapt",
    "href": "cf-fairadapt.html#counterfactuals-with-fairadapt",
    "title": "5  Fairadapt",
    "section": "5.2 Counterfactuals with fairadapt",
    "text": "5.2 Counterfactuals with fairadapt\nWe adapt the code from Plečko, Bennett, and Meinshausen (2021) to handle the test set. This avoids estimating cumulative distribution and quantile functions on the test set, which would otherwise necessitate recalculating quantile regression functions for each new sample.\nWe do not need to adapt Y here, so we need to remove it from the adjacency matrix:\n\nadj_wo_Y &lt;- adj[-4,-4]\nadj_wo_Y\n\n   S X1 X2\nS  0  1  1\nX1 0  0  1\nX2 0  0  0\n\n\nWe create a dataset with the sensitive attribute and the two other predictors:\n\ndf_race_fpt &lt;- df_race_c |&gt; select(S, X1, X2)\n\nLet us have a look at the levels of our sensitive variable:\n\nlevels(df_race_fpt$S)\n\n[1] \"Black\" \"White\"\n\n\nThe reference class here consists of Black individuals.\nTwo configurations will be considered in turn:\n\nThe reference class consists of Black individuals, and FairAdapt will be used to obtain the counterfactual UGPA and LSAT scores for White individuals as if they had been Black.\nThe reference class consists of White individuals, and FairAdapt will be used to obtain the counterfactual UGPA and LSAT scores for Black individuals as if they had been White.\n\n\n# White (factuals) --&gt; Black (counterfactuals)\nfpt_model_white &lt;- fairadapt(\n  X2 ~ ., \n  train.data = df_race_fpt,\n  prot.attr = \"S\", adj.mat = adj_wo_Y,\n  quant.method = linearQuants\n)\nadapt_df_white &lt;- adaptedData(fpt_model_white)\n\n# Black (factuals) --&gt; White (counterfactuals)\ndf_race_fpt$S &lt;- factor(df_race_fpt$S, levels = c(\"White\", \"Black\"))\nfpt_model_black &lt;- fairadapt(\n  X2 ~ ., \n  train.data = df_race_fpt,\n  prot.attr = \"S\", adj.mat = adj_wo_Y,\n  quant.method = linearQuants\n)\nadapt_df_black &lt;- adaptedData(fpt_model_black)\n\nLet us wrap up:\n\nwe have two predictive models for the FYA (above median = 1, or below median = 0):\n\nunaware (without S)\naware (with S)\n\nwe have the counterfactual characteristics obtained with fairadapt in two situations depending on the reference class:\n\nBlack individuals as reference\nWhite individuals as reference.\n\n\nThe predictive models will be used to compare predictions made using:\n\nRaw characteristics (initial characteristics).\nCharacteristics possibly altered through FairAdapt for individuals who were not in the reference group (i.e., using counterfactuals).\n\n\n5.2.1 Unaware Model\nThe predicted values using the initial characteristics (the factuals), for the unaware model are stored in the object pred_unaware_all. We put in a table the initial characteristics (factuals) and the prediction made by the unaware model:\n\nfactuals_unaware &lt;- tibble(\n  S = df_race$S,\n  X1 = df_race$X1,\n  X2 = df_race$X2,\n  pred = pred_unaware_all,\n  type = \"factual\"\n)\n\nLet us save this dataset in a csv file (this file will be used to perform multivariate transport in python).\n\nwrite.csv(\n  factuals_unaware, \n  file = \"data/factuals_unaware.csv\", row.names = FALSE\n)\n\nLet us build a dataset containing only counterfactual characteristics (obtained with fairadapt): values for \\(X_1\\) and \\(X_2\\) of White individuals as if they had been Black, and values for \\(X_1\\) and \\(X_2\\) of Black individuals as if they had been White.\n\nind_white &lt;- which(df_race_fpt$S == \"White\")\nind_black &lt;- which(df_race_fpt$S == \"Black\")\ndf_counterfactuals_fpt &lt;- factuals_unaware |&gt; select(-pred, -type)\ndf_counterfactuals_fpt[ind_white, ] &lt;- \n  adapt_df_white[ind_white, ] |&gt; select(S, X1, X2)\ndf_counterfactuals_fpt[ind_black, ] &lt;- \n  adapt_df_black[ind_black,] |&gt; select(S, X1, X2)\n\nLet us get the predicted values for the counterfactuals, using the unaware model:\n\nmodel_unaware &lt;- pred_unaware$model\npred_unaware_fpt &lt;- predict(\n  model_unaware, newdata = df_counterfactuals_fpt, type = \"response\"\n)\n\nWe create a table with the counterfactual characteristics and the prediction by the unaware model:\n\ncounterfactuals_unaware_fpt &lt;- tibble(\n  S = df_counterfactuals_fpt$S,\n  X1 = df_counterfactuals_fpt$X1,\n  X2 = df_counterfactuals_fpt$X2,\n  pred = pred_unaware_fpt,\n  type = \"counterfactual\"\n)\n\nWe merge the two datasets, factuals_unaware and counterfactuals_unaware_fpt in a single one.\n\n# dataset with counterfactuals, for unaware model\nunaware_fpt &lt;- bind_rows(factuals_unaware, counterfactuals_unaware_fpt)\n\nNow, we can visualize the distribution of the values predicted by the unaware model within each group defined by the sensitive attribute.\n\nunaware_fpt_white &lt;- unaware_fpt |&gt; filter(S == \"White\") \nunaware_fpt_black &lt;- unaware_fpt |&gt; filter(S == \"Black\")\n\n\nRef: BlackRef: White\n\n\n\nggplot(unaware_fpt_black, aes(x = pred, fill = type)) +\n  geom_histogram(\n    aes(y = after_stat(density)), alpha = 0.5, position = \"identity\", binwidth = 0.05) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Unaware model, Sensitive: Race, Reference: Black individuals\",\n       x = \"Predictions for Y\",\n       y = \"Density\") +\n  global_theme()\n\n\n\n\nFigure 5.1: Unaware model, Sensitive: Race, Reference: Black individuals\n\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = unaware_fpt_white,\n  mapping = aes(x = pred, fill = type)\n) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), \n    alpha = 0.5, position = \"identity\", binwidth = 0.05\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Unaware model, Sensitive: Race, Reference: White individuals\",\n       x = \"Predictions for Y\",\n       y = \"Density\"\n  ) +\n  global_theme()\n\n\n\n\nFigure 5.2: Unaware model, Sensitive: Race, Reference: White individuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Aware Model\nNow, we turn to the model that includes the sensitive attribute, i.e., the aware model.\nThe predicted values by the model, on the initial characteristics (on the factuals) are stored in the pred_aware_all object.\nWe create a tibble with the factuals and the predictions by the aware model:\n\nfactuals_aware &lt;- tibble(\n  S = df_race$S,\n  X1 = df_race$X1,\n  X2 = df_race$X2,\n  pred = pred_aware_all,\n  type = \"factual\"\n)\n\nLet us save this table in a CSV file (this file will be used to perform multivariate transport in python):\n\nwrite.csv(factuals_aware, file = \"data/factuals_aware.csv\", row.names = FALSE)\n\nRecall we created an object called df_counterfactuals_fpt which contains the counterfactual characteristics of all students, obtained with fairadapt:\n\ndf_counterfactuals_fpt\n\n# A tibble: 19,567 × 3\n   S        X1    X2\n   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Black  2.7   31.3\n 2 Black  2.6   28  \n 3 Black  2.7   21  \n 4 Black  3.1   28.1\n 5 Black  3.3   21.0\n 6 Black  3.3   26.9\n 7 Black  2.4   29.6\n 8 Black  2.3   29.8\n 9 Black  3.3   21  \n10 Black  2.85  33.5\n# ℹ 19,557 more rows\n\n\nWe make predictions with the aware model on these counterfactuals:\n\nmodel_aware &lt;- pred_aware$model\npred_aware_fpt &lt;- predict(\n  model_aware, newdata = df_counterfactuals_fpt, type = \"response\"\n)\n\nThen, we create a table with the counterfactuals and the predicted value by the aware model:\n\ncounterfactuals_aware_fpt &lt;- tibble(\n  S = df_counterfactuals_fpt$S,\n  X1 = df_counterfactuals_fpt$X1,\n  X2 = df_counterfactuals_fpt$X2,\n  pred = pred_aware_fpt,\n  type = \"counterfactual\"\n)\n\nWe bind together the table with the factuals and the counterfactuals (as well as their predicted values by the aware model):\n\naware_fpt &lt;- bind_rows(factuals_aware, counterfactuals_aware_fpt)\n\nLastly, we can visualize the distribution of predicted values by the aware model once the characteristics of the individuals who are not on the reference group have been modified using fairadapt.\n\naware_fpt_white &lt;- aware_fpt %&gt;% filter(S == \"White\") \naware_fpt_black &lt;- aware_fpt %&gt;% filter(S == \"Black\")\n\n\nRef: BlackRef: White\n\n\n\nggplot(\n  data = aware_fpt_black, \n  mapping = aes(x = pred, fill = type)) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), \n    alpha = 0.5, position = \"identity\", binwidth = 0.05\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Aware model, Sensitive: Race, Reference: Black individuals\",\n    x = \"Predictions for Y\",\n    y = \"Density\"\n  ) +\n  global_theme()\n\n\n\n\nFigure 5.3: Aware model, Sensitive: Race, Reference: Black individuals\n\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = aware_fpt_white, \n  mapping = aes(x = pred, fill = type)) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), \n    alpha = 0.5, position = \"identity\", binwidth = 0.05) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Aware model, Sensitive: Race, Reference: White individuals\",\n    x = \"Predictions for Y\",\n    y = \"Density\"\n  ) +\n  global_theme()\n\n\n\n\nFigure 5.4: Aware model, Sensitive: Race, Reference: White individuals",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fairadapt</span>"
    ]
  },
  {
    "objectID": "cf-fairadapt.html#comparison-for-two-individuals",
    "href": "cf-fairadapt.html#comparison-for-two-individuals",
    "title": "5  Fairadapt",
    "section": "5.3 Comparison for Two Individuals",
    "text": "5.3 Comparison for Two Individuals\nLet us focus on two individuals: the 24th (Black) and the 25th (White) of the dataset.\n\n(indiv_factuals_unaware &lt;- factuals_unaware[24:25, ])\n\n# A tibble: 2 × 5\n  S        X1    X2  pred type   \n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  \n1 Black   2.8    29 0.300 factual\n2 White   2.8    34 0.382 factual\n\n\nThe characteristics of these two individuals would be, according to what was estimated using fairadapt, if the reference group was the one in which they do not belong:\n\n(indiv_counterfactuals_unaware_fpt &lt;- counterfactuals_unaware_fpt[24:25, ])\n\n# A tibble: 2 × 5\n  S        X1    X2  pred type          \n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         \n1 White  3.25  37.6 0.509 counterfactual\n2 Black  2.5   26   0.225 counterfactual\n\n\nWe put the factuals and counterfactuals in a single table:\n\nindiv_unaware_fpt &lt;- bind_rows(\n  indiv_factuals_unaware |&gt; mutate(id = c(24, 25)), \n  indiv_counterfactuals_unaware_fpt |&gt; mutate(id = c(24, 25))\n)\nindiv_unaware_fpt\n\n# A tibble: 4 × 6\n  S        X1    X2  pred type              id\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 Black  2.8   29   0.300 factual           24\n2 White  2.8   34   0.382 factual           25\n3 White  3.25  37.6 0.509 counterfactual    24\n4 Black  2.5   26   0.225 counterfactual    25\n\n\nThe difference between the counterfactual and the factual for these two individuals:\n\nindiv_unaware_fpt |&gt; select(id , type, pred) |&gt; \n  pivot_wider(names_from = type, values_from = pred) |&gt; \n  mutate(diff_fpt = counterfactual - factual)\n\n# A tibble: 2 × 4\n     id factual counterfactual diff_fpt\n  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1    24   0.300          0.509    0.209\n2    25   0.382          0.225   -0.157\n\n\nWe apply the same procedure with the aware model:\n\nindiv_aware_fpt &lt;- bind_rows(\n  factuals_aware[c(24, 25),] |&gt; mutate(id = c(24, 25)),\n  counterfactuals_aware_fpt[c(24, 25),] |&gt; mutate(id = c(24, 25))\n)\nindiv_aware_fpt\n\n# A tibble: 4 × 6\n  S        X1    X2   pred type              id\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 Black  2.8   29   0.133  factual           24\n2 White  2.8   34   0.413  factual           25\n3 White  3.25  37.6 0.522  counterfactual    24\n4 Black  2.5   26   0.0991 counterfactual    25\n\n\nThe difference between the counterfactual and the factual for these two individuals, when using the aware model:\n\nindiv_aware_fpt |&gt; select(id , type, pred) |&gt; \n  pivot_wider(names_from = type, values_from = pred) |&gt; \n  mutate(diff = counterfactual - factual)\n\n# A tibble: 2 × 4\n     id factual counterfactual   diff\n  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1    24   0.133         0.522   0.389\n2    25   0.413         0.0991 -0.314",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fairadapt</span>"
    ]
  },
  {
    "objectID": "cf-fairadapt.html#counterfactual-demographic-parity",
    "href": "cf-fairadapt.html#counterfactual-demographic-parity",
    "title": "5  Fairadapt",
    "section": "5.4 Counterfactual Demographic Parity",
    "text": "5.4 Counterfactual Demographic Parity\nLet us assume here that the reference group is “White individuals” (i.e., the group with the most individuals in the dataset). We focus on the minority, i.e., Black individuals. We consider here that the model is fair towards the minority class if: \\[\nP(\\hat{Y}_{S \\leftarrow \\text{White}} = 1 | S = \\text{Black}, X_1, X_2) = P(\\hat{Y} = 1 | S = \\text{White}, X_1, X_2)\n\\] If the model is fair with respect to this criterion, the proportion of Black individuals predicted to have grades above the median should be the same as if they had been white.\nFor predictions made with the unaware model:\n\ndp_unaware_fpt &lt;- mean(\n  counterfactuals_unaware_fpt |&gt; filter(S == \"White\") |&gt; pull(\"pred\") - \n    factuals_unaware |&gt; filter(S == \"Black\") |&gt; pull(\"pred\")\n)\ndp_unaware_fpt\n\n[1] 0.19177\n\n\nWe do the same with the aware model:\n\ndp_aware_fpt &lt;- mean(\n  counterfactuals_aware_fpt |&gt; filter(S == \"White\") |&gt; pull(\"pred\") - \n    factuals_aware |&gt; filter(S == \"Black\") |&gt; pull(\"pred\")\n)\ndp_aware_fpt\n\n[1] 0.3809912",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fairadapt</span>"
    ]
  },
  {
    "objectID": "cf-fairadapt.html#sec-cf-fairadapt-save",
    "href": "cf-fairadapt.html#sec-cf-fairadapt-save",
    "title": "5  Fairadapt",
    "section": "5.5 Saving Objects",
    "text": "5.5 Saving Objects\n\nsave(factuals_unaware, file = \"data/factuals_unaware.rda\")\nsave(factuals_aware, file = \"data/factuals_aware.rda\")\nsave(counterfactuals_unaware_fpt, file = \"data/counterfactuals_unaware_fpt.rda\")\nsave(counterfactuals_aware_fpt, file = \"data/counterfactuals_aware_fpt.rda\")\n\n\n\n\n\nPlečko, Drago, Nicolas Bennett, and Nicolai Meinshausen. 2021. “Fairadapt: Causal Reasoning for Fair Data Pre-Processing.” arXiv Preprint arXiv:2110.10200.\n\n\nPlečko, Drago, and Nicolai Meinshausen. 2020. “Fair Data Adaptation with Quantile Preservation.” Journal of Machine Learning Research 21 (242): 1–44.",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fairadapt</span>"
    ]
  },
  {
    "objectID": "cf-ot.html",
    "href": "cf-ot.html",
    "title": "6  Multivariate Optimal Transport",
    "section": "",
    "text": "6.1 Load Data and Classifier\nWe load the dataset where the sensitive attribute ((S)) is the race, obtained Chapter 3.3:\nload(\"data/df_race.rda\")\nWe also load the dataset where the sensitive attribute is also the race, but where where the target variable ((Y), ZFYA) is binary (1 if the student obtained a standardized first year average over the median, 0 otherwise). This dataset was saved in Chapter 4.5:\nload(\"data/df_race_c.rda\")\nWe also need the predictions made by the classifier (see Chapter 4):\n# Predictions on train/test sets\nload(\"data/pred_aware.rda\")\nload(\"data/pred_unaware.rda\")\n# Predictions on the factuals, on the whole dataset\nload(\"data/pred_aware_all.rda\")\nload(\"data/pred_unaware_all.rda\")",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate Optimal Transport</span>"
    ]
  },
  {
    "objectID": "cf-ot.html#counterfactuals-with-multivariate-optimal-transport",
    "href": "cf-ot.html#counterfactuals-with-multivariate-optimal-transport",
    "title": "6  Multivariate Optimal Transport",
    "section": "6.2 Counterfactuals with Multivariate Optimal Transport",
    "text": "6.2 Counterfactuals with Multivariate Optimal Transport\nWe apply multivariate optimal transport (OT), following the methodology developed in De Lara et al. (2024). Note that with OT, it is not possible to handle new cases. Counterfactuals will only be calculated on the train set.\nThe codes are run in python. We use the {reticulate} R package to call python in this notebook.\n\nlibrary(reticulate)\n\nSome libraries need to be loaded (including POT called ot)\n\nimport ot\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pl\nimport ot.plot\n\nThe data with the factuals need to be loaded:\n\ndf_aware = pd.read_csv('data/factuals_aware.csv')\ndf_unaware = pd.read_csv('data/factuals_unaware.csv')\n\n\nx_S = df_aware.drop(columns=['pred', 'type'])\nx_S.head()\n\n       S   X1    X2\n0  White  3.1  39.0\n1  White  3.0  36.0\n2  White  3.1  30.0\n3  White  3.4  37.0\n4  White  3.6  30.5\n\n\n\nx_white = x_S[x_S['S'] == 'White']\nx_white = x_white.drop(columns=['S'])\nx_black = x_S[x_S['S'] == 'Black']\nx_black = x_black.drop(columns=['S'])\n\nn_white = len(x_white)\nn_black = len(x_black)\n# Uniform weights\nw_white = (1/n_white)*np.ones(n_white)\nw_black = (1/n_black)*np.ones(n_black)\n\nCost matrix between both distributions:\n\nx_white = x_white.to_numpy()\nx_black = x_black.to_numpy()\nC = ot.dist(x_white, x_black)\n\n\npl.figure(1)\npl.plot(x_white[:, 0], x_white[:, 1], '+b', label='Source samples')\npl.plot(x_black[:, 0], x_black[:, 1], 'xr', label='Target samples')\npl.legend(loc=0)\npl.title('Source and target distributions')\n\n\n\n\nFigure 6.1: Source and target distributions\n\n\n\n\n\n\n\n\n\npl.figure(2)\npl.imshow(C, interpolation='nearest')\npl.title('Cost matrix C')\n\n\n\n\nFigure 6.2: Cost matric C\n\n\n\n\n\n\n\n\nThe transport plan: white –&gt; black\n\npi_white_black = ot.emd(w_white, w_black, C, numItermax=1e8)\npi_black_white = pi_white_black.T\npi_white_black.shape\n\n(18285, 1282)\n\n\n\nsum_of_rows = np.sum(pi_white_black, axis=1)\nsum_of_rows*n_white\n\narray([1., 1., 1., ..., 1., 1., 1.])\n\n\n\npi_black_white.shape\n\n(1282, 18285)\n\nsum_of_rows = np.sum(pi_black_white, axis=1)\nsum_of_rows*n_black\n\narray([1., 1., 1., ..., 1., 1., 1.])\n\n\n\npl.figure(3)\npl.imshow(pi_white_black, interpolation='nearest')\npl.title('OT matrix pi_white_black')\n\npl.figure(4)\not.plot.plot2D_samples_mat(x_white, x_black, pi_white_black, c=[.5, .5, 1])\npl.plot(x_white[:, 0], x_white[:, 1], '+b', label='Source samples')\npl.plot(x_black[:, 0], x_black[:, 1], 'xr', label='Target samples')\npl.legend(loc=0)\npl.title('OT matrix with samples')\n\n\n\n\nFigure 6.3: OT matrix pi_white_black\n\n\n\n\n\n\n\n\n\ntransformed_x_white = n_white*pi_white_black@x_black\n\n\n\n\n\n\n\n\n\ntransformed_x_white.shape\n\n(18285, 2)\n\n\n\ntransformed_x_white\n\narray([[ 2.7, 31. ],\n       [ 2.7, 28. ],\n       [ 2.6, 21. ],\n       ...,\n       [ 3.9, 28. ],\n       [ 2.5, 22. ],\n       [ 3. , 19. ]])\n\n\n\ntransformed_x_black = n_black*pi_black_white@x_white\n\n\ntransformed_x_black.shape\n\n(1282, 2)\n\n\n\ntransformed_x_black\n\narray([[ 3.2       , 37.58851518],\n       [ 3.28565491, 28.02103363],\n       [ 2.95793273, 32.14022423],\n       ...,\n       [ 3.28597758, 33.        ],\n       [ 2.65092152, 41.43910309],\n       [ 2.75152858, 36.        ]])\n\n\n\ncounterfactual_x = x_S.drop(columns=['S'])\ncounterfactual_x[x_S['S'] == 'White'] = transformed_x_white\ncounterfactual_x[x_S['S'] == 'Black'] = transformed_x_black\n\n\ncounterfactual_x.head()\n\n    X1    X2\n0  2.7  31.0\n1  2.7  28.0\n2  2.6  21.0\n3  3.1  28.0\n4  3.2  21.0\n\n\n\ncounterfactual_x.shape\n\n(19567, 2)\n\n\nLastly, we export the results in a CSV file:\n\ncsv_file_path = 'data/counterfactuals_ot.csv'\ncounterfactual_x.to_csv(csv_file_path, index=False)\n\nLet us get back to R, and load the results.\n\ncounterfactuals_ot &lt;- read_csv('data/counterfactuals_ot.csv')\n\nWe add the sensitive attribute to the dataset (Black individuals become White, and conversely):\n\nS_star &lt;- df_race_c |&gt; \n  mutate(\n    S_star = case_when(\n      S == \"Black\" ~ \"White\",\n      S == \"White\" ~ \"Black\",\n      TRUE ~ \"Error\"\n    )\n  ) |&gt; \n  pull(\"S_star\")\n\ncounterfactuals_ot &lt;- counterfactuals_ot |&gt; \n  mutate(S = S_star)\n\n\n6.2.1 Unaware Model\nLet us make prediction with the unaware model on the counterfactuals obtained with OT:\n\nmodel_unaware &lt;- pred_unaware$model\npred_unaware_ot &lt;- predict(\n  model_unaware, newdata = counterfactuals_ot, type = \"response\"\n)\ncounterfactuals_unaware_ot &lt;- counterfactuals_ot |&gt; \n  mutate(pred = pred_unaware_ot, type = \"counterfactual\")\n\nWe put in a table the initial characteristics (factuals) and the prediction made by the unaware model:\n\nfactuals_unaware &lt;- tibble(\n  S = df_race$S,\n  X1 = df_race$X1,\n  X2 = df_race$X2,\n  pred = pred_unaware_all,\n  type = \"factual\"\n)\n\nWe bind the factuals and counterfactuals with their respective predicted values in a single dataset:\n\nunaware_ot &lt;- bind_rows(\n  # predicted values on factuals\n  factuals_unaware |&gt; mutate(type = \"factual\"), \n  # predicted values on counterfactuals obtained with OT\n  counterfactuals_unaware_ot\n)\n\nThen, we can visualize the distribution of the values predicted by the unaware model within each group defined by the sensitive attribute.\n\nunaware_ot_white &lt;- unaware_ot %&gt;% filter(S == \"White\") \nunaware_ot_black &lt;- unaware_ot %&gt;% filter(S == \"Black\")\n\n:::{.panel-tabset}\n\n6.2.1.1 Ref: Black\n\nggplot(\n  data = unaware_ot_black, \n  mapping = aes(x = pred, fill = type)\n) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), \n    alpha = 0.5, position = \"identity\", binwidth = 0.05\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Unaware model, Sensitive: Race, Reference: Black individual\",\n       x = \"Predictions for Y\",\n       y = \"Density\"\n  ) +\n  global_theme()\n\n\n\n\nFigure 6.4: Unaware model, Sensitive: Race, Reference: Black individuals\n\n\n\n\n\n\n\n\n\n\n6.2.1.2 Ref: White\n\nggplot(\n  data = unaware_ot_white, \n  mapping = aes(x = pred, fill = type)) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), \n    alpha = 0.5, position = \"identity\", binwidth = 0.05\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Unaware model, Sensitive: Race, Reference: White\",\n       x = \"Predictions for Y\",\n       y = \"Density\") +\n  global_theme()\n\n\n\n\nFigure 6.5: Unaware model, Sensitive: Race, Reference: White individuals\n\n\n\n\n\n\n\n\n\n\n\n6.2.2 Aware Model\nLet us make prediction with the aware model on the counterfactuals obtained with OT:\n\nmodel_aware &lt;- pred_aware$model\npred_aware_ot &lt;- predict(\n  model_aware, newdata = counterfactuals_ot, type = \"response\"\n)\ncounterfactuals_aware_ot &lt;- counterfactuals_ot |&gt;  \n  mutate(pred = pred_aware_ot, type = \"counterfactual\")\ncounterfactuals_aware_ot\n\n# A tibble: 19,567 × 5\n      X1    X2 S       pred type          \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;         \n 1   2.7  31   Black 0.141  counterfactual\n 2   2.7  28   Black 0.120  counterfactual\n 3   2.6  21   Black 0.0791 counterfactual\n 4   3.1  28   Black 0.143  counterfactual\n 5   3.2  21   Black 0.104  counterfactual\n 6   3.3  27.5 Black 0.152  counterfactual\n 7   2.4  29   Black 0.111  counterfactual\n 8   2.3  29   Black 0.106  counterfactual\n 9   3.3  22   Black 0.115  counterfactual\n10   2.8  34   Black 0.171  counterfactual\n# ℹ 19,557 more rows\n\n\nWe put in a table the initial characteristics (factuals) and the prediction made by the aware model:\n\nfactuals_aware &lt;- tibble(\n  S = df_race$S,\n  X1 = df_race$X1,\n  X2 = df_race$X2,\n  pred = pred_aware_all,\n  type = \"factual\"\n)\n\nWe bind the factuals and counterfactuals with their respective predicted values in a single dataset:\n\naware_ot &lt;- bind_rows(\n  factuals_aware, \n  counterfactuals_aware_ot\n)\n\nThen, we can visualize the distribution of the values predicted by the unaware model within each group defined by the sensitive attribute.\n\naware_ot_white &lt;- aware_ot |&gt; filter(S == \"White\") \naware_ot_black &lt;- aware_ot |&gt;  filter(S == \"Black\")\n\n\nRef: BlackRef: White\n\n\n\nggplot(\n  data = aware_ot_black, \n  mapping = aes(x = pred, fill = type)\n) +\n  geom_histogram(\n    mapping = aes(y = ..density..), \n    alpha = 0.5, position = \"identity\", binwidth = 0.05\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Aware model, Sensitive: Race, Reference: Black individual\",\n       x = \"Predictions for Y\",\n       y = \"Density\"\n  ) +\n  global_theme()\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nFigure 6.6: Aware model, Sensitive: Race, Reference: Black individuals\n\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = aware_ot_white, \n  mapping = aes(x = pred, fill = type)) +\n  geom_histogram(\n    mapping = aes(y = ..density..), \n    alpha = 0.5, position = \"identity\", binwidth = 0.05\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Aware model, Sensitive: Race, Reference: White\",\n       x = \"Predictions for Y\",\n       y = \"Density\") +\n  global_theme()\n\n\n\n\nFigure 6.7: Aware model, Sensitive: Race, Reference: White individuals",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate Optimal Transport</span>"
    ]
  },
  {
    "objectID": "cf-ot.html#comparison-for-two-individuals",
    "href": "cf-ot.html#comparison-for-two-individuals",
    "title": "6  Multivariate Optimal Transport",
    "section": "6.3 Comparison for Two Individuals",
    "text": "6.3 Comparison for Two Individuals\nLet us, again, focus on two individuals: 24 (Black) and 25 (White):\n\n(indiv_factuals_unaware &lt;- factuals_unaware[24:25, ])\n\n# A tibble: 2 × 5\n  S        X1    X2  pred type   \n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  \n1 Black   2.8    29 0.300 factual\n2 White   2.8    34 0.382 factual\n\n\nThe counterfactuals for those individuals, using the unaware model:\n\n(indiv_counterfactuals_unaware_ot &lt;- counterfactuals_unaware_ot[24:25, ])\n\n# A tibble: 2 × 5\n     X1    X2 S      pred type          \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;         \n1  3.20  37.6 White 0.502 counterfactual\n2  2.4   25   Black 0.203 counterfactual\n\n\nLet us put the factuals and counterfactuals in a single table:\n\nindiv_unaware_ot &lt;- bind_rows(\n  indiv_factuals_unaware |&gt; mutate(id = c(24, 25)),\n  indiv_counterfactuals_unaware_ot |&gt; mutate(id = c(24, 25))\n)\nindiv_unaware_ot\n\n# A tibble: 4 × 6\n  S        X1    X2  pred type              id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 Black  2.8   29   0.300 factual           24\n2 White  2.8   34   0.382 factual           25\n3 White  3.20  37.6 0.502 counterfactual    24\n4 Black  2.4   25   0.203 counterfactual    25\n\n\nWe compute the difference between the predicted value by the unaware model using the counterfactuals and the predicted value by the unaware model using the factuals:\n\nindiv_unaware_ot |&gt; select(id , type, pred) |&gt; \n  pivot_wider(names_from = type, values_from = pred) |&gt; \n  mutate(diff = counterfactual - factual)\n\n# A tibble: 2 × 4\n     id factual counterfactual   diff\n  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1    24   0.300          0.502  0.202\n2    25   0.382          0.203 -0.179\n\n\nWe do the same for the aware model:\n\nindiv_aware_ot &lt;- bind_rows(\n  factuals_aware[c(24, 25),] |&gt; mutate(id = c(24, 25)),\n  counterfactuals_aware_ot[c(24, 25),] |&gt; mutate(id = c(24, 25))\n)\nindiv_aware_ot\n\n# A tibble: 4 × 6\n  S        X1    X2   pred type              id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 Black  2.8   29   0.133  factual           24\n2 White  2.8   34   0.413  factual           25\n3 White  3.20  37.6 0.515  counterfactual    24\n4 Black  2.4   25   0.0898 counterfactual    25\n\n\nThe difference between the counterfactual and the factual for these two individuals, when using the aware model:\n\nindiv_aware_ot |&gt; select(id , type, pred) |&gt; \n  pivot_wider(names_from = type, values_from = pred) |&gt; \n  mutate(diff = counterfactual - factual)\n\n# A tibble: 2 × 4\n     id factual counterfactual   diff\n  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1    24   0.133         0.515   0.383\n2    25   0.413         0.0898 -0.323",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate Optimal Transport</span>"
    ]
  },
  {
    "objectID": "cf-ot.html#counterfactual-demographic-parity",
    "href": "cf-ot.html#counterfactual-demographic-parity",
    "title": "6  Multivariate Optimal Transport",
    "section": "6.4 Counterfactual Demographic Parity",
    "text": "6.4 Counterfactual Demographic Parity\nAs for the counterfactuals obtained with fairadapt, we assume here that the reference group is “White individuals” (i.e., the group with the most individuals in the dataset). We focus on the minority, i.e., Black individuals. We consider here that the model is fair towards the minority class if: \\[\nP(\\hat{Y}_{S \\leftarrow \\text{White}} = 1 | S = \\text{Black}, X_1, X_2) = P(\\hat{Y} = 1 | S = \\text{White}, X_1, X_2)\n\\] If the model is fair with respect to this criterion, the proportion of Black individuals predicted to have grades above the median should be the same as if they had been white.\nFor predictions made with the unaware model:\n\ndp_unaware_pt &lt;- mean(\n  counterfactuals_unaware_ot |&gt; filter(S == \"White\") |&gt; pull(\"pred\") - \n    factuals_unaware |&gt; filter(S == \"Black\") |&gt; pull(\"pred\")\n)\ndp_unaware_pt\n\n[1] 0.1821212\n\n\nWe do the same with the aware model:\n\ndp_aware_ot &lt;- mean(\n  counterfactuals_aware_ot |&gt; filter(S == \"White\") |&gt; pull(\"pred\") - \n    factuals_aware |&gt; filter(S == \"Black\") |&gt; pull(\"pred\")\n)\ndp_aware_ot\n\n[1] 0.3726591",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate Optimal Transport</span>"
    ]
  },
  {
    "objectID": "cf-ot.html#sec-cf-fairadapt-save",
    "href": "cf-ot.html#sec-cf-fairadapt-save",
    "title": "6  Multivariate Optimal Transport",
    "section": "6.5 Saving Objects",
    "text": "6.5 Saving Objects\n\nsave(counterfactuals_unaware_ot, file = \"data/counterfactuals_unaware_ot.rda\")\nsave(counterfactuals_aware_ot, file = \"data/counterfactuals_aware_ot.rda\")\n\n\n\n\n\nDe Lara, Lucas, Alberto González-Sanz, Nicholas Asher, Laurent Risser, and Jean-Michel Loubes. 2024. “Transport-Based Counterfactual Models.” Journal of Machine Learning Research 25 (136): 1–59.",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multivariate Optimal Transport</span>"
    ]
  },
  {
    "objectID": "cf-seq-transport.html",
    "href": "cf-seq-transport.html",
    "title": "7  Sequential Transport",
    "section": "",
    "text": "7.1 Load Data and Classifier\nWe load the dataset where the sensitive attribute ((S)) is the race, obtained Chapter 3.3:\nload(\"data/df_race.rda\")\nWe also load the dataset where the sensitive attribute is also the race, but where where the target variable ((Y), ZFYA) is binary (1 if the student obtained a standardized first year average over the median, 0 otherwise). This dataset was saved in Chapter 4.5:\nload(\"data/df_race_c.rda\")\nWe also need the predictions made by the classifier (see Chapter 4):\n# Predictions on train/test sets\nload(\"data/pred_aware.rda\")\nload(\"data/pred_unaware.rda\")\n# Predictions on the factuals, on the whole dataset\nload(\"data/pred_aware_all.rda\")\nload(\"data/pred_unaware_all.rda\")",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Transport</span>"
    ]
  },
  {
    "objectID": "cf-seq-transport.html#counterfactuals-with-sequential-transport",
    "href": "cf-seq-transport.html#counterfactuals-with-sequential-transport",
    "title": "7  Sequential Transport",
    "section": "7.2 Counterfactuals with Sequential Transport",
    "text": "7.2 Counterfactuals with Sequential Transport\nWe now turn to sequential transport (the methodology developed in our paper). We define a function, fonction_transport() (see in functions/utils.R) to perform a fast sequential transport on causal graph.\n\n#' Sequential transport\n#'\n#' @param data dataset with three columns:\n#'  - S: sensitive attribute\n#'  - X1: first predictor, assumed to be causally linked to S\n#'  - X2: second predictor, assumed to be causally linked to S and X1\n#' @param S_0 value for the sensitive attribute for the source distribution\n#' @param number of cells in each dimension (default to 15)\n#' @param h small value added to extend the area covered by the grid (default\n#'  to .2)\n#' @param d neighborhood weight when conditioning by x1 (default to .5)\nfonction_transport &lt;- function(data,\n                               S_0,\n                               n_grid = 15,\n                               h = .2,\n                               d = .5\n                               ) {\n\n  # Subset of the data: 0 for Black, 1 for White\n  D_SXY_0 &lt;- data[data$S ==S_0, ]\n  D_SXY_1 &lt;- data[data$S!= S_0, ]\n\n  # Coordinates of the cells of the grid on subset of 0 (Black)\n  vx1_0 &lt;- seq(min(D_SXY_0$X1) - h, max(D_SXY_0$X1) + h, length = n_grid + 1)\n  vx2_0 &lt;- seq(min(D_SXY_0$X2) - h, max(D_SXY_0$X2) + h, length = n_grid + 1)\n  # and middle point of the cells\n  vx1_0_mid &lt;- (vx1_0[2:(1+n_grid)]+vx1_0[1:(n_grid)]) / 2\n  vx2_0_mid &lt;- (vx2_0[2:(1+n_grid)]+vx2_0[1:(n_grid)]) / 2\n\n  # Coordinates of the cells of the grid on subset of 1 (White)\n  vx1_1 &lt;- seq(min(D_SXY_1$X1) -h, max(D_SXY_1$X1) + h, length = n_grid + 1)\n  vx1_1_mid &lt;- (vx1_1[2:(1 + n_grid)] + vx1_1[1:(n_grid)]) / 2\n  # and middle point of the cells\n  vx2_1 &lt;- seq(min(D_SXY_1$X2) - h, max(D_SXY_1$X2) + h, length = n_grid + 1)\n  vx2_1_mid &lt;- (vx2_1[2:(1 + n_grid)] + vx2_1[1:(n_grid)]) / 2\n\n  # Creation of the grids for the CDF and Quantile function\n  # init with NA values\n  # One grid for X1 and X2, on both subsets of the data (Black/White)\n  F1_0 &lt;- F2_0 &lt;- F1_1 &lt;- F2_1 &lt;- matrix(NA, n_grid, n_grid)\n  Q1_0 &lt;- Q2_0 &lt;- Q1_1 &lt;- Q2_1 &lt;- matrix(NA, n_grid, n_grid)\n\n  # Empirical CDF for X1 on subset of Black\n  FdR1_0 &lt;- Vectorize(function(x) mean(D_SXY_0$X1 &lt;= x))\n  f1_0 &lt;- FdR1_0(vx1_0_mid)\n  # Empirical CDF for X2 on subset of Black\n  FdR2_0 &lt;- Vectorize(function(x) mean(D_SXY_0$X2 &lt;= x))\n  f2_0 &lt;- FdR2_0(vx2_0_mid)\n  # Empirical CDF for X1 on subset of White\n  FdR1_1 &lt;- Vectorize(function(x) mean(D_SXY_1$X1 &lt;= x))\n  f1_1 &lt;- FdR1_1(vx1_1_mid)\n  # Empirical CDF for X2 on subset of White\n  FdR2_1 &lt;- Vectorize(function(x) mean(D_SXY_1$X2 &lt;= x))\n  f2_1 &lt;- FdR2_1(vx2_1_mid)\n\n  u &lt;- (1:n_grid) / (n_grid + 1)\n  # Empirical quantiles for X1 on subset of Black\n  Qtl1_0 &lt;- Vectorize(function(x) quantile(D_SXY_0$X1, x))\n  q1_0 &lt;- Qtl1_0(u)\n  # Empirical quantiles for X2 on subset of Black\n  Qtl2_0 &lt;- Vectorize(function(x) quantile(D_SXY_0$X2, x))\n  q2_0 &lt;- Qtl2_0(u)\n  # Empirical quantiles for X1 on subset of White\n  Qtl1_1 &lt;- Vectorize(function(x) quantile(D_SXY_1$X1, x))\n  q1_1 &lt;- Qtl1_1(u)\n  # Empirical quantiles for X2 on subset of White\n  Qtl2_1 &lt;- Vectorize(function(x) quantile(D_SXY_1$X2, x))\n  q2_1 &lt;- Qtl2_1(u)\n\n  for(i in 1:n_grid) {\n    # Subset of Black\n    idx1_0 &lt;- which(abs(D_SXY_0$X1 - vx1_0_mid[i]) &lt; d)\n    FdR2_0 &lt;- Vectorize(function(x) mean(D_SXY_0$X2[idx1_0] &lt;= x))\n    F2_0[, i] &lt;- FdR2_0(vx2_0_mid)\n    Qtl2_0 &lt;- Vectorize(function(x) quantile(D_SXY_0$X2[idx1_0], x))\n    Q2_0[, i] &lt;- Qtl2_0(u)\n\n    idx2_0 &lt;- which(abs(D_SXY_0$X2 - vx2_0_mid[i]) &lt; d)\n    FdR1_0 &lt;- Vectorize(function(x) mean(D_SXY_0$X1[idx2_0] &lt;= x))\n    F1_0[, i] &lt;- FdR1_0(vx1_0_mid)\n    Qtl1_0 &lt;- Vectorize(function(x) quantile(D_SXY_0$X1[idx2_0], x))\n    Q1_0[, i] &lt;- Qtl1_0(u)\n\n    # Subset of White\n    idx1_1 &lt;- which(abs(D_SXY_1$X1 - vx1_1_mid[i]) &lt; d)\n    FdR2_1 &lt;- Vectorize(function(x) mean(D_SXY_1$X2[idx1_1] &lt;= x))\n    F2_1[, i] &lt;- FdR2_1(vx2_1_mid)\n    Qtl2_1 &lt;- Vectorize(function(x) quantile(D_SXY_1$X2[idx1_1], x))\n    Q2_1[, i] &lt;- Qtl2_1(u)\n\n    idx2_1 &lt;- which(abs(D_SXY_1$X2-vx2_1_mid[i])&lt;d)\n    FdR1_1 &lt;- Vectorize(function(x) mean(D_SXY_1$X1[idx2_1] &lt;= x))\n    F1_1[, i] &lt;- FdR1_1(vx1_1_mid)\n    Qtl1_1 &lt;- Vectorize(function(x) quantile(D_SXY_1$X1[idx2_1], x))\n    Q1_1[, i] &lt;- Qtl1_1(u)\n  }\n\n  # Transport for X2\n  T2 &lt;- function(x2) {\n    i &lt;- which.min(abs(vx2_0_mid - x2))\n    p &lt;- f2_0[i]\n    i &lt;- which.min(abs(u - p))\n    x2star &lt;- q2_1[i]\n    x2star\n  }\n\n  # Transport for X1\n  T1 &lt;- function(x1) {\n    i &lt;- which.min(abs(vx1_0_mid - x1))\n    p &lt;- f1_0[i]\n    i &lt;- which.min(abs(u - p))\n    x1star &lt;- q1_1[i]\n    x1star\n  }\n\n  # Transport for X2 conditional on X1\n  T2_cond_x1 &lt;- function(x2, x1) {\n    k0 &lt;- which.min(abs(vx1_0_mid - x1))\n    k1 &lt;- which.min(abs(vx1_1_mid - T1(x1)))\n    i &lt;- which.min(abs(vx2_0_mid - x2))\n    p &lt;- F2_0[i, k0]\n    i &lt;- which.min(abs(u - p))\n    x2star &lt;- Q2_1[i, k1]\n    x2star\n  }\n\n  # Transport for X1 conditional on X2\n  T1_cond_x2 &lt;- function(x1, x2) {\n    k0 &lt;- which.min(abs(vx2_0_mid - x2))\n    k1 &lt;- which.min(abs(vx2_1_mid - x2))\n    i &lt;- which.min(abs(vx1_0_mid - x1))\n    p &lt;- F1_0[i, k0]\n    i &lt;- which.min(abs(u - p))\n    x1star &lt;- Q1_1[i, k1]\n    x1star\n  }\n\n  list(\n    Transport_x1 = T1,\n    Transport_x2 = T2,\n    Transport_x1_cond_x2 = T1_cond_x2,\n    Transport_x2_cond_x1 = T2_cond_x1\n  )\n}\n\n\n\n\n\n\n\nNote\n\n\n\nThe fonction_transport() function returns not only the functions Transport_x1(), Transport_x2(), Transport_x1_cond_x2(), Transport_x2_cond_x1(), but also the useful values of the grid (e.g., vx1_0_mid defined in the environment of the function and used in the functions). Note that defining a global object named vx1_0_mid will not alter the object of the same name defined in the environment of fonction_transport(): R will call the vx1_0_mid from that environment and not the one that may be defined in the global environment.\n\n\nLet us apply this function. Note that we use a grid of length 500 to fasten the computation of sequential transport (the estimation takes about 45 seconds on a standard computer). But first, we create a dataset, df_race_c_light, with the sensitive attribute and the two characteristics only:\n\ndf_race_c_light &lt;- df_race_c |&gt; select(S, X1, X2)\nind_white &lt;- which(df_race_c_light$S == \"White\")\nind_black &lt;- which(df_race_c_light$S == \"Black\")\n\n\nseq_functions &lt;- fonction_transport(\n  data = df_race_c_light, S_0 = \"Black\", n_grid = 500\n)\n\nLet us extract the transport functions to transport \\(X_1\\) and \\(X_2\\):\n\nT_X1 &lt;- seq_functions$Transport_x1\nT_X2 &lt;- seq_functions$Transport_x2\n\nWe also do the same with the transport of \\(X_2\\) conditional on \\(X_1\\):\n\nT_X2_c_X1 &lt;- seq_functions$Transport_x2_cond_x1\n\nNow, we can apply these functions to the subset of Black individuals to sequentially transport \\(X_1\\) (UGPA) and then \\(X_2\\) (LSAT) conditional on the transported value of \\(X_1\\):\nThe values of \\(X_1\\) and \\(X_2\\) for Black individuals:\n\na10 &lt;- df_race_c_light$X1[ind_black]\na20 &lt;- df_race_c_light$X2[ind_black]\n\nThe transported values:\n\nx1_star &lt;- map_dbl(a10, T_X1) # Transport X1 to group S=White\nx2_star &lt;- map2_dbl(a20, a10, T_X2_c_X1) # Transport X2|X1 to group S=White\n\nWe build a dataset with the sensitive attribute of Black individuals changed to white, and their characteristics changed to their transported characteristics:\n\ndf_counterfactuals_seq_black &lt;- \n  df_race_c_light |&gt; mutate(id = row_number()) |&gt; \n  filter(S == \"Black\") |&gt; \n  mutate(\n    S = \"White\",\n    X1 = x1_star,\n    X2 = x2_star\n  )\n\nWe make predictions based on those counterfactuals obrained with sequential transport, on both models (the unaware model, and the aware model):\n\nmodel_unaware &lt;- pred_unaware$model\npred_seq_unaware &lt;- predict(\n  model_unaware, newdata = df_counterfactuals_seq_black,type = \"response\"\n)\n\nmodel_aware &lt;- pred_aware$model\npred_seq_aware &lt;- predict(\n  model_aware, newdata = df_counterfactuals_seq_black,type = \"response\"\n)\n\n\ncounterfactuals_unaware_seq_black &lt;- \n  df_counterfactuals_seq_black |&gt; \n  mutate(pred = pred_seq_unaware, type = \"counterfactual\")\ncounterfactuals_aware_seq_black &lt;- \n  df_counterfactuals_seq_black |&gt; \n  mutate(pred = pred_seq_aware, type = \"counterfactual\")\n\nWe create a tibble with the factuals and the predictions by the aware model, an another with the predictions by the unaware model:\n\nfactuals_aware &lt;- tibble(\n  S = df_race$S,\n  X1 = df_race$X1,\n  X2 = df_race$X2,\n  pred = pred_aware_all,\n  type = \"factual\"\n)\n\nfactuals_unaware &lt;- tibble(\n  S = df_race$S,\n  X1 = df_race$X1,\n  X2 = df_race$X2,\n  pred = pred_unaware_all,\n  type = \"factual\"\n)\n\nLet us put in a single table the predictions made by the classifier (either aware or unaware) on black individuals based on their factual characteristics, and those made based on the counterfactuals:\n\naware_seq_black &lt;- bind_rows(\n  factuals_aware |&gt; mutate(id = row_number()) |&gt; filter(S == \"Black\"), \n  counterfactuals_aware_seq_black\n)\nunaware_seq_black &lt;- bind_rows(\n  factuals_unaware |&gt; mutate(id = row_number()) |&gt; filter(S == \"Black\"), \n  counterfactuals_aware_seq_black)\n\n\nUnawareAware\n\n\n\nggplot(\n  data = unaware_seq_black, \n  mapping = aes(x = pred, fill = type)\n) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), \n    alpha = 0.5, position = \"identity\", binwidth = 0.05\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Unware model, S: Race - Black --&gt; White\",\n       x = \"Predictions for Y\",\n       y = \"Density\") +\n  global_theme()\n\n\n\n\nFigure 7.1: Unaware model, Sensitive: Race, Reference: White individuals\n\n\n\n\n\n\n\n\n\n\n\nggplot(\n  data = aware_seq_black, \n  mapping = aes(x = pred, fill = type)\n) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), \n    alpha = 0.5, position = \"identity\", binwidth = 0.05\n  ) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Aware model, S: Race - Black --&gt; White\",\n       x = \"Predictions for Y\",\n       y = \"Density\") +\n  global_theme()\n\n\n\n\nFigure 7.2: Aware model, Sensitive: Race, Reference: White individuals",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Transport</span>"
    ]
  },
  {
    "objectID": "cf-seq-transport.html#comparison-for-two-individuals",
    "href": "cf-seq-transport.html#comparison-for-two-individuals",
    "title": "7  Sequential Transport",
    "section": "7.3 Comparison for Two Individuals",
    "text": "7.3 Comparison for Two Individuals\nLet us focus on the first three Black individuals of the dataset.\n\nfactuals_unaware |&gt; mutate(id = row_number()) |&gt; \n  filter(S == \"Black\") |&gt; \n  dplyr::slice(1:3)\n\n# A tibble: 3 × 6\n  S        X1    X2  pred type       id\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n1 Black   2.8    29 0.300 factual    24\n2 Black   3.2    19 0.206 factual    40\n3 Black   2.6    23 0.198 factual    51\n\n\nTheir characteristics after sequential transport (and the predicted value with the unaware model):\n\nindiv_counterfactuals_unaware_seq &lt;- counterfactuals_unaware_seq_black[c(1:3), ]\nindiv_counterfactuals_unaware_seq\n\n# A tibble: 3 × 6\n  S        X1    X2    id  pred type          \n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;         \n1 White   3.2  37      24 0.491 counterfactual\n2 White   3.5  28.5    40 0.381 counterfactual\n3 White   3.1  31.5    51 0.379 counterfactual\n\n\n\nindiv_unaware_seq &lt;- bind_rows(\n  factuals_unaware |&gt; \n    mutate(id = row_number()) |&gt; \n    filter(S == \"Black\") |&gt; \n    dplyr::slice(1:3),\n  indiv_counterfactuals_unaware_seq\n)\nindiv_unaware_seq\n\n# A tibble: 6 × 6\n  S        X1    X2  pred type              id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt;\n1 Black   2.8  29   0.300 factual           24\n2 Black   3.2  19   0.206 factual           40\n3 Black   2.6  23   0.198 factual           51\n4 White   3.2  37   0.491 counterfactual    24\n5 White   3.5  28.5 0.381 counterfactual    40\n6 White   3.1  31.5 0.379 counterfactual    51\n\n\nAnd with the aware model:\n\nindiv_counterfactuals_aware_seq &lt;- counterfactuals_aware_seq_black[c(1:3), ]\nindiv_counterfactuals_aware_seq\n\n# A tibble: 3 × 6\n  S        X1    X2    id  pred type          \n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;         \n1 White   3.2  37      24 0.507 counterfactual\n2 White   3.5  28.5    40 0.418 counterfactual\n3 White   3.1  31.5    51 0.413 counterfactual\n\n\n\nindiv_aware_seq &lt;- bind_rows(\n  factuals_aware |&gt; \n    mutate(id = row_number()) |&gt; \n    filter(S == \"Black\") |&gt; \n    dplyr::slice(1:3),\n  indiv_counterfactuals_aware_seq\n)\nindiv_aware_seq\n\n# A tibble: 6 × 6\n  S        X1    X2   pred type              id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt;\n1 Black   2.8  29   0.133  factual           24\n2 Black   3.2  19   0.0933 factual           40\n3 Black   2.6  23   0.0882 factual           51\n4 White   3.2  37   0.507  counterfactual    24\n5 White   3.5  28.5 0.418  counterfactual    40\n6 White   3.1  31.5 0.413  counterfactual    51",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Transport</span>"
    ]
  },
  {
    "objectID": "cf-seq-transport.html#counterfactual-demographic-parity",
    "href": "cf-seq-transport.html#counterfactual-demographic-parity",
    "title": "7  Sequential Transport",
    "section": "7.4 Counterfactual Demographic Parity",
    "text": "7.4 Counterfactual Demographic Parity\nFor the unaware model:\n\nmean(\n  counterfactuals_unaware_seq_black$pred -\n    factuals_unaware |&gt; filter(S == \"Black\") |&gt; pull(\"pred\")\n)\n\n[1] 0.1816625\n\n\nFor the aware model:\n\nmean(\n  counterfactuals_aware_seq_black$pred - \n    factuals_aware |&gt; filter(S == \"Black\") |&gt; pull(\"pred\")\n)\n\n[1] 0.3722886",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Transport</span>"
    ]
  },
  {
    "objectID": "cf-seq-transport.html#sec-cf-seq-t-save",
    "href": "cf-seq-transport.html#sec-cf-seq-t-save",
    "title": "7  Sequential Transport",
    "section": "7.5 Saving Objects",
    "text": "7.5 Saving Objects\n\nsave(counterfactuals_unaware_seq_black  , file = \"data/counterfactuals_unaware_seq_black.rda\")\nsave(counterfactuals_aware_seq_black, file = \"data/counterfactuals_aware_seq_black.rda\")",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Transport</span>"
    ]
  },
  {
    "objectID": "cf-comparison.html",
    "href": "cf-comparison.html",
    "title": "8  Counterfactuals: comparison",
    "section": "",
    "text": "8.1 Load Data and Classifier\nWe load the dataset where the sensitive attribute ((S)) is the race, obtained Chapter 3.3:\nload(\"data/df_race.rda\")\nCounterfactuals constructed with fairadapt and predictions by the classifiers (see Chapter 5):\nload(\"data/counterfactuals_aware_fpt.rda\")\nload(\"data/counterfactuals_unaware_fpt.rda\")\nCounterfactuals constructed with multivariate optimal transport and predictions by the classifiers (see Chapter 6):\nload(\"data/counterfactuals_aware_ot.rda\")\nload(\"data/counterfactuals_unaware_ot.rda\")\nCounterfactuals constructed with sequential transport and predictions by the classifiers (see Chapter 7):\nload(\"data/counterfactuals_aware_seq_black.rda\")\nload(\"data/counterfactuals_unaware_seq_black.rda\")\nWe also need the predictions made by the classifier (see Chapter 4):\n# Predictions on train/test sets\nload(\"data/pred_aware.rda\")\nload(\"data/pred_unaware.rda\")\n# Predictions on the factuals, on the whole dataset\nload(\"data/pred_aware_all.rda\")\nload(\"data/pred_unaware_all.rda\")\nWe create a tibble with the factuals and the predictions by the aware model, an another with the predictions by the unaware model:\nfactuals_aware &lt;- tibble(\n  S = df_race$S,\n  X1 = df_race$X1,\n  X2 = df_race$X2,\n  pred = pred_aware_all,\n  type = \"factual\"\n)\n\nfactuals_unaware &lt;- tibble(\n  S = df_race$S,\n  X1 = df_race$X1,\n  X2 = df_race$X2,\n  pred = pred_unaware_all,\n  type = \"factual\"\n)",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Counterfactuals: comparison</span>"
    ]
  },
  {
    "objectID": "cf-comparison.html#comparison",
    "href": "cf-comparison.html#comparison",
    "title": "8  Counterfactuals: comparison",
    "section": "8.2 Comparison",
    "text": "8.2 Comparison\nLet us focus on the first three Black individuals from the dataset. We will compare the predicted values by the classifier (see Chapter 4) made using the observed characteristics, and the changes in the predictions when using counterfactuals. We use the three types of counterfactuals explored in the previous chapters.\n\nUnaware ModelAware Model\n\n\n\ntb_unaware &lt;- \n  factuals_unaware |&gt; mutate(id = row_number(), counterfactual = \"none\") |&gt; \n  # Fairadapt\n  bind_rows(\n    counterfactuals_unaware_fpt |&gt; \n      mutate(id = row_number(), counterfactual = \"fpt\")\n  ) |&gt; \n  # Multivariate optimal transport\n  bind_rows(\n    counterfactuals_unaware_ot |&gt; mutate(id = row_number(), counterfactual = \"ot\")\n  ) |&gt; \n  # Sequential transport\n  bind_rows(\n    counterfactuals_unaware_seq_black |&gt; mutate(counterfactual = \"seq\")\n  )\n\ntb_indiv_unaware &lt;- \n  tb_unaware |&gt; \n  filter(id %in% counterfactuals_unaware_seq_black$id[1:3])\n\ntb_indiv_unaware\n\n# A tibble: 12 × 7\n   S        X1    X2  pred type              id counterfactual\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;         \n 1 Black  2.8   29   0.300 factual           24 none          \n 2 Black  3.2   19   0.206 factual           40 none          \n 3 Black  2.6   23   0.198 factual           51 none          \n 4 White  3.25  37.6 0.509 counterfactual    24 fpt           \n 5 White  3.6   29.9 0.419 counterfactual    40 fpt           \n 6 White  3.1   32.3 0.394 counterfactual    51 fpt           \n 7 White  3.20  37.6 0.502 counterfactual    24 ot            \n 8 White  3.29  28.0 0.345 counterfactual    40 ot            \n 9 White  2.96  32.1 0.371 counterfactual    51 ot            \n10 White  3.2   37   0.491 counterfactual    24 seq           \n11 White  3.5   28.5 0.381 counterfactual    40 seq           \n12 White  3.1   31.5 0.379 counterfactual    51 seq           \n\n\n\n\n\ntb_aware &lt;- \n  factuals_aware |&gt; mutate(id = row_number(), counterfactual = \"none\") |&gt; \n  # Fairadapt\n  bind_rows(\n    counterfactuals_aware_fpt |&gt; \n      mutate(id = row_number(), counterfactual = \"fpt\")\n  ) |&gt; \n  # Multivariate optimal transport\n  bind_rows(\n    counterfactuals_aware_ot |&gt; mutate(id = row_number(), counterfactual = \"ot\")\n  ) |&gt; \n  # Sequential transport\n  bind_rows(\n    counterfactuals_aware_seq_black |&gt; mutate(counterfactual = \"seq\")\n  ) \n  \ntb_indiv_aware &lt;- \n  tb_aware |&gt; \n  filter(id %in% counterfactuals_aware_seq_black$id[1:3])\n\ntb_indiv_aware\n\n# A tibble: 12 × 7\n   S        X1    X2   pred type              id counterfactual\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;         \n 1 Black  2.8   29   0.133  factual           24 none          \n 2 Black  3.2   19   0.0933 factual           40 none          \n 3 Black  2.6   23   0.0882 factual           51 none          \n 4 White  3.25  37.6 0.522  counterfactual    24 fpt           \n 5 White  3.6   29.9 0.451  counterfactual    40 fpt           \n 6 White  3.1   32.3 0.425  counterfactual    51 fpt           \n 7 White  3.20  37.6 0.515  counterfactual    24 ot            \n 8 White  3.29  28.0 0.386  counterfactual    40 ot            \n 9 White  2.96  32.1 0.405  counterfactual    51 ot            \n10 White  3.2   37   0.507  counterfactual    24 seq           \n11 White  3.5   28.5 0.418  counterfactual    40 seq           \n12 White  3.1   31.5 0.413  counterfactual    51 seq           \n\n\n\n\n\n\nUnaware ModelAware Model\n\n\n\n\nCode\npar(mar = c(2, 2, 0, 0))\n# Initial characteristics with the unaware model\ntb_indiv_unaware_factual &lt;- \n  tb_indiv_unaware |&gt; filter(type == \"factual\")\ncolour_factual &lt;- \"black\"\ncolour_fpt &lt;- \"#D55E00\"\ncolour_ot &lt;- \"#56B4E9\"\ncolour_seq &lt;- \"#CC79A7\"\ncolours_all &lt;- c(\n  \"Factual\" = colour_factual,\n  \"fairadapt\" = colour_fpt,\n  \"Multi. OT\" = colour_ot,\n  \"Seq T.\" = colour_seq\n)\nrange_x1 &lt;- range(tb_indiv_unaware$X1)\nexpansion_amount_x1 &lt;- .1*range_x1\nrange_x2 &lt;- range(tb_indiv_unaware$X2)\nexpansion_amount_x2 &lt;- .05*range_x2\n\nplot(\n  x = tb_indiv_unaware_factual$X1,\n  y = tb_indiv_unaware_factual$X2,\n  col = colour_factual,\n  # xlab = \"X1 (UGPA)\", ylab = \"X2 (LSAT)\",\n  xlab = \"\", ylab = \"\",\n  xlim = c(range_x1[1] - expansion_amount_x1[1], range_x1[2] + expansion_amount_x1[2]),\n  ylim = c(range_x2[1] - expansion_amount_x2[1], range_x2[2] + expansion_amount_x2[2]),\n  pch = 19,\n  axes = FALSE\n)\naxis(1)\nmtext(expression(X[1]~(UGCA)), side = 1, padj = .5)\naxis(2)\nmtext(expression(X[2]~(LSAT)), side = 2, padj = 0)\ntext(\n  x = tb_indiv_unaware_factual$X1, \n  y = tb_indiv_unaware_factual$X2 + 1,\n  paste0(round(100*tb_indiv_unaware_factual$pred, 2), \"%\"),\n  col = colour_factual\n)\n# Transported characteristics with fairadapt\ntb_indiv_unaware_fpt &lt;- \n  tb_indiv_unaware |&gt; filter(counterfactual == \"fpt\")\npoints(\n  x = tb_indiv_unaware_fpt$X1,\n  y = tb_indiv_unaware_fpt$X2,\n  col = colour_fpt,\n  xlab = \"X1\", ylab = \"X2\",\n  pch = 19\n)\n# x1 then x2\nsegments(\n  x0 = tb_indiv_unaware_factual$X1, \n  y0 = tb_indiv_unaware_factual$X2,\n  x1 = tb_indiv_unaware_fpt$X1, \n  y1 = tb_indiv_unaware_factual$X2, \n  col = colour_fpt,\n  lty = 2\n)\nsegments(\n  x0 = tb_indiv_unaware_fpt$X1, \n  y0 = tb_indiv_unaware_factual$X2,\n  x1 = tb_indiv_unaware_fpt$X1, \n  y1 = tb_indiv_unaware_fpt$X2, \n  col = colour_fpt,\n  lty = 2\n)\ntext(\n  x = tb_indiv_unaware_fpt$X1, \n  y = tb_indiv_unaware_fpt$X2 + 1,\n  paste0(round(100*tb_indiv_unaware_fpt$pred, 2), \"%\"),\n  col = colour_fpt\n)\n# Transported characteristics with OT\ntb_indiv_unaware_ot &lt;- \n  tb_indiv_unaware |&gt; filter(counterfactual == \"ot\")\npoints(\n  x = tb_indiv_unaware_ot$X1,\n  y = tb_indiv_unaware_ot$X2,\n  col = colour_ot,\n  xlab = \"X1\", ylab = \"X2\",\n  pch = 19\n)\n# x1 then x2\nsegments(\n  x0 = tb_indiv_unaware_factual$X1, \n  y0 = tb_indiv_unaware_factual$X2,\n  x1 = tb_indiv_unaware_ot$X1, \n  y1 = tb_indiv_unaware_ot$X2, \n  col = colour_ot,\n  lty = 2\n)\ntext(\n  x = tb_indiv_unaware_ot$X1 - .15, \n  y = tb_indiv_unaware_ot$X2,\n  paste0(round(100*tb_indiv_unaware_ot$pred, 2), \"%\"),\n  col = colour_ot\n)\n\n# Transported characteristics with Sequential transport\ntb_indiv_unaware_seq &lt;- \n  tb_indiv_unaware |&gt; filter(counterfactual == \"seq\")\npoints(\n  x = tb_indiv_unaware_seq$X1,\n  y = tb_indiv_unaware_seq$X2,\n  col = colour_seq,\n  xlab = \"X1\", ylab = \"X2\",\n  pch = 19\n)\n# x1 then x2\nsegments(\n  x0 = tb_indiv_unaware_factual$X1, \n  y0 = tb_indiv_unaware_factual$X2,\n  x1 = tb_indiv_unaware_seq$X1, \n  y1 = tb_indiv_unaware_factual$X2, \n  col = colour_seq,\n  lty = 2\n)\nsegments(\n  x0 = tb_indiv_unaware_seq$X1, \n  y0 = tb_indiv_unaware_factual$X2,\n  x1 = tb_indiv_unaware_seq$X1, \n  y1 = tb_indiv_unaware_seq$X2, \n  col = colour_seq,\n  lty = 2\n)\ntext(\n  x = tb_indiv_unaware_seq$X1 - .11, \n  y = tb_indiv_unaware_seq$X2 - 1,\n  paste0(round(100*tb_indiv_unaware_seq$pred, 2), \"%\"),\n  col = colour_seq\n)\nlegend(\n  \"topleft\", \n  pch = 19, col = colours_all, legend = names(colours_all),\n  box.lty=0\n)\n\n\n\n\n\nFigure 8.1: Predictions by the unaware model for three Black individuals.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mar = c(2, 2, 0, 0))\n# Initial characteristics with the aware model\ntb_indiv_aware_factual &lt;- \n  tb_indiv_aware |&gt; filter(type == \"factual\")\n\nrange_x1 &lt;- range(tb_indiv_aware$X1)\nexpansion_amount_x1 &lt;- .1*range_x1\nrange_x2 &lt;- range(tb_indiv_aware$X2)\nexpansion_amount_x2 &lt;- .05*range_x2\n\nplot(\n  x = tb_indiv_aware_factual$X1,\n  y = tb_indiv_aware_factual$X2,\n  col = colour_factual,\n  xlab = \"\", ylab = \"\",\n  # xlab = \"X1 (UGPA)\", ylab = \"X2 (LSAT)\",\n  xlim = c(range_x1[1] - expansion_amount_x1[1], range_x1[2] + expansion_amount_x1[2]),\n  ylim = c(range_x2[1] - expansion_amount_x2[1], range_x2[2] + expansion_amount_x2[2]),\n  pch = 19,\n  axes = FALSE\n)\naxis(1)\nmtext(expression(X[1]~(UGCA)), side = 1, padj = .5)\naxis(2)\nmtext(expression(X[2]~(LSAT)), side = 2, padj = 0)\ntext(\n  x = tb_indiv_aware_factual$X1, \n  y = tb_indiv_aware_factual$X2 + 1,\n  paste0(round(100*tb_indiv_aware_factual$pred, 2), \"%\"),\n  col = colour_factual\n)\n# Transported characteristics with fairadapt\ntb_indiv_aware_fpt &lt;- \n  tb_indiv_aware |&gt; filter(counterfactual == \"fpt\")\npoints(\n  x = tb_indiv_aware_fpt$X1,\n  y = tb_indiv_aware_fpt$X2,\n  col = colour_fpt,\n  xlab = \"X1\", ylab = \"X2\",\n  pch = 19\n)\n# x1 then x2\nsegments(\n  x0 = tb_indiv_aware_factual$X1, \n  y0 = tb_indiv_aware_factual$X2,\n  x1 = tb_indiv_aware_fpt$X1, \n  y1 = tb_indiv_aware_factual$X2, \n  col = colour_fpt,\n  lty = 2\n)\nsegments(\n  x0 = tb_indiv_aware_fpt$X1, \n  y0 = tb_indiv_aware_factual$X2,\n  x1 = tb_indiv_aware_fpt$X1, \n  y1 = tb_indiv_aware_fpt$X2, \n  col = colour_fpt,\n  lty = 2\n)\ntext(\n  x = tb_indiv_aware_fpt$X1, \n  y = tb_indiv_aware_fpt$X2 + 1,\n  paste0(round(100*tb_indiv_aware_fpt$pred, 2), \"%\"),\n  col = colour_fpt\n)\n# Transported characteristics with OT\ntb_indiv_aware_ot &lt;- \n  tb_indiv_aware |&gt; filter(counterfactual == \"ot\")\npoints(\n  x = tb_indiv_aware_ot$X1,\n  y = tb_indiv_aware_ot$X2,\n  col = colour_ot,\n  xlab = \"X1\", ylab = \"X2\",\n  pch = 19\n)\n# x1 then x2\nsegments(\n  x0 = tb_indiv_aware_factual$X1, \n  y0 = tb_indiv_aware_factual$X2,\n  x1 = tb_indiv_aware_ot$X1, \n  y1 = tb_indiv_aware_ot$X2, \n  col = colour_ot,\n  lty = 2\n)\ntext(\n  x = tb_indiv_aware_ot$X1 - .15, \n  y = tb_indiv_aware_ot$X2,\n  paste0(round(100*tb_indiv_aware_ot$pred, 2), \"%\"),\n  col = colour_ot\n)\n\n# Transported characteristics with Sequential transport\ntb_indiv_aware_seq &lt;- \n  tb_indiv_aware |&gt; filter(counterfactual == \"seq\")\npoints(\n  x = tb_indiv_aware_seq$X1,\n  y = tb_indiv_aware_seq$X2,\n  col = colour_seq,\n  xlab = \"X1\", ylab = \"X2\",\n  pch = 19\n)\n# x1 then x2\nsegments(\n  x0 = tb_indiv_aware_factual$X1, \n  y0 = tb_indiv_aware_factual$X2,\n  x1 = tb_indiv_aware_seq$X1, \n  y1 = tb_indiv_aware_factual$X2, \n  col = colour_seq,\n  lty = 2\n)\nsegments(\n  x0 = tb_indiv_aware_seq$X1, \n  y0 = tb_indiv_aware_factual$X2,\n  x1 = tb_indiv_aware_seq$X1, \n  y1 = tb_indiv_aware_seq$X2, \n  col = colour_seq,\n  lty = 2\n)\ntext(\n  x = tb_indiv_aware_seq$X1 - .11, \n  y = tb_indiv_aware_seq$X2 - 1,\n  paste0(round(100*tb_indiv_aware_seq$pred, 2), \"%\"),\n  col = colour_seq\n)\nlegend(\n  \"topleft\", \n  pch = 19, col = colours_all, legend = names(colours_all),\n  box.lty=0\n)\n\n\n\n\n\nFigure 8.2: Predictions by the aware model for three Black individuals.",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Counterfactuals: comparison</span>"
    ]
  },
  {
    "objectID": "cf-comparison.html#densities",
    "href": "cf-comparison.html#densities",
    "title": "8  Counterfactuals: comparison",
    "section": "8.3 Densities",
    "text": "8.3 Densities\nLet us now compare the densities of the predicted values.\n\ncolours &lt;- c(\n  \"0\" = \"#5BBCD6\",\n  \"1\" = \"#FF0000\",\n  \"A\" = \"#00A08A\",\n  \"B\" = \"#F2AD00\",\n  \"with\" = \"#046C9A\",\n  \"without\" = \"#C93312\",\n  \"2\" = \"#0B775E\"\n)\n\n\nUnaware modelAware model\n\n\n\n\nCode\n# Factuals\ncolours_all &lt;- c(\n  \"Factual Black\" = colours[[\"A\"]],\n  \"Factual White\" = colours[[\"B\"]],\n  \"fairadapt\" = colour_fpt,\n  \"OT\" = colour_ot,\n  \"Seq T.\" = colour_seq\n)\n\n# Factuals\ntb_unaware_factuals &lt;- tb_unaware |&gt; \n  filter(counterfactual == \"none\")\n# Predicted values\npred_unaware_factuals_black &lt;- tb_unaware_factuals |&gt; filter(S == \"Black\") |&gt; pull(\"pred\")\npred_unaware_factuals_white &lt;- tb_unaware_factuals |&gt; filter(S == \"White\") |&gt; pull(\"pred\")\n# Estimated densities\nd_unaware_factuals_black &lt;- density(pred_unaware_factuals_black)\nd_unaware_factuals_white &lt;- density(pred_unaware_factuals_white)\n\npar(mfrow = c(3, 1), mar = c(2, 2, 0, 0))\nx_lim &lt;- c(0, .8)\ny_lim &lt;- c(0, 8)\n\n# plot(\n#   d_unaware_factuals_black,\n#   main = \"Factuals\", xlab = \"\", ylab = \"\",\n#   axes = FALSE, col = NA,\n#   xlim = x_lim\n# )\n# axis(1)\n# axis(2)\n# polygon(d_unaware_factuals_black, col = alpha(colours[[\"A\"]], .5), border = NA)\n# polygon(d_unaware_factuals_white, col = alpha(colours[[\"B\"]], .5), border = NA)\n\n# Fairadapt\ntb_unaware_fpt &lt;- tb_unaware |&gt; \n  filter(counterfactual == \"fpt\")\n# Predicted values, focusing on Black --&gt; White\npred_unaware_fpt_black_star &lt;- tb_unaware_fpt |&gt; filter(S == \"White\") |&gt; pull(\"pred\")\n# Estimated densities\nd_unaware_fpt_black_star &lt;- density(pred_unaware_fpt_black_star)\n\nplot(\n  d_unaware_factuals_black,\n  main = \"\", xlab = \"\", ylab = \"\",\n  axes = FALSE, col = NA,\n  xlim = x_lim, ylim = y_lim\n)\naxis(1)\naxis(2)\npolygon(d_unaware_factuals_black, col = alpha(colours[[\"A\"]], .5), border = NA)\nlines(d_unaware_factuals_white, col = colours[[\"B\"]], lty = 2, lwd = 2)\npolygon(d_unaware_fpt_black_star, col = alpha(colour_fpt, .5), border = NA)\ntext(x = .25, y = 6, \"Factuals\")\nind_min &lt;- which.min(abs(d_unaware_factuals_black$x - .2))\narrows(\n  x1 = d_unaware_factuals_black$x[ind_min],\n  y1 = d_unaware_factuals_black$y[ind_min],\n  x0 = .25, \n  y0 = 5,\n  length = 0.05\n)\ntext(x = .53, y = 6, \"fairadapt\")\n\n# legend(\n#   ncol = 5,\n#   \"topleft\", \n#   pch = c(19, NA, rep(19, 3)), \n#   lty = c(NA, 2, rep(NA, 3)),\n#   col = colours_all,\n#   legend = names(colours_all)\n# )\n\n# OT\ntb_unaware_ot &lt;- tb_unaware |&gt; \n  filter(counterfactual == \"ot\")\n# Predicted values, focusing on Black --&gt; White\npred_unaware_ot_black_star &lt;- tb_unaware_ot |&gt; filter(S == \"White\") |&gt; pull(\"pred\")\n# Estimated densities\nd_unaware_ot_black_star &lt;- density(pred_unaware_ot_black_star)\n\nplot(\n  d_unaware_factuals_black,\n  main = \"\", xlab = \"\", ylab = \"\",\n  axes = FALSE, col = NA,\n  xlim = x_lim, ylim = y_lim\n)\naxis(1)\naxis(2)\npolygon(d_unaware_factuals_black, col = alpha(colours[[\"A\"]], .5), border = NA)\nlines(d_unaware_factuals_white, col = colours[[\"B\"]], lty = 2, lwd = 2)\npolygon(d_unaware_ot_black_star, col = alpha(colour_ot, .5), border = NA)\ntext(x = .53, y = 6, \"Multi. OT\")\n\n# Sequential transport\ntb_unaware_seq &lt;- tb_unaware |&gt; \n  filter(counterfactual == \"seq\")\n# Predicted values, focusing on Black --&gt; White\npred_unaware_seq_black_star &lt;- tb_unaware_seq |&gt; filter(S == \"White\") |&gt; pull(\"pred\")\n# Estimated densities\nd_unaware_seq_black_star &lt;- density(pred_unaware_seq_black_star)\n\nplot(\n  d_unaware_factuals_black,\n  main = \"\", xlab = \"\", ylab = \"\",\n  axes = FALSE, col = NA,\n  xlim = x_lim, ylim = y_lim\n)\naxis(1)\naxis(2)\npolygon(d_unaware_factuals_black, col = alpha(colours[[\"A\"]], .5), border = NA)\nlines(d_unaware_factuals_white, col = colours[[\"B\"]], lty = 2, lwd = 2)\npolygon(d_unaware_seq_black_star, col = alpha(colour_seq, .5), border = NA)\ntext(x = .53, y = 6, \"Seq. T.\")\n\n\n\n\n\nFigure 8.3: Densities of predicted scores for Black individuals with factuals and with counterfactuals. The yellow dashed line corresponds to the density of predicted scores for White individuals, using factuals.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Factuals\ncolours_all &lt;- c(\n  \"Factual Black\" = colours[[\"A\"]],\n  \"Factual White\" = colours[[\"B\"]],\n  \"fairadapt\" = colour_fpt,\n  \"OT\" = colour_ot,\n  \"Seq T.\" = colour_seq\n)\n\n# Factuals\ntb_aware_factuals &lt;- tb_aware |&gt; \n  filter(counterfactual == \"none\")\n# Predicted values\npred_aware_factuals_black &lt;- tb_aware_factuals |&gt; filter(S == \"Black\") |&gt; pull(\"pred\")\npred_aware_factuals_white &lt;- tb_aware_factuals |&gt; filter(S == \"White\") |&gt; pull(\"pred\")\n# Estimated densities\nd_aware_factuals_black &lt;- density(pred_aware_factuals_black)\nd_aware_factuals_white &lt;- density(pred_aware_factuals_white)\n\npar(mfrow = c(3, 1), mar = c(2, 2, 0, 0))\nx_lim &lt;- c(0, .8)\n\n# plot(\n#   d_aware_factuals_black,\n#   main = \"Factuals\", xlab = \"\", ylab = \"\",\n#   axes = FALSE, col = NA,\n#   xlim = x_lim\n# )\n# axis(1)\n# axis(2)\n# polygon(d_aware_factuals_black, col = alpha(colours[[\"A\"]], .5), border = NA)\n# polygon(d_aware_factuals_white, col = alpha(colours[[\"B\"]], .5), border = NA)\n\n# Fairadapt\ntb_aware_fpt &lt;- tb_aware |&gt; \n  filter(counterfactual == \"fpt\")\n# Predicted values, focusing on Black --&gt; White\npred_aware_fpt_black_star &lt;- tb_aware_fpt |&gt; filter(S == \"White\") |&gt; pull(\"pred\")\n# Estimated densities\nd_aware_fpt_black_star &lt;- density(pred_aware_fpt_black_star)\n\nplot(\n  d_aware_factuals_black,\n  main = \"\", xlab = \"\", ylab = \"\",\n  axes = FALSE, col = NA,\n  xlim = x_lim\n)\naxis(1)\naxis(2)\npolygon(d_aware_factuals_black, col = alpha(colours[[\"A\"]], .5), border = NA)\nlines(d_aware_factuals_white, col = colours[[\"B\"]], lty = 2, lwd = 2)\npolygon(d_aware_fpt_black_star, col = alpha(colour_fpt, .5), border = NA)\ntext(x = .25, y = 6, \"Factuals\")\nind_min &lt;- which.min(abs(d_aware_factuals_black$x - .2))\narrows(\n  x1 = d_aware_factuals_black$x[ind_min],\n  y1 = d_aware_factuals_black$y[ind_min],\n  x0 = .25, \n  y0 = 5,\n  length = 0.05\n)\ntext(x = .53, y = 6, \"fairadapt\")\n\n# legend(\n#   ncol = 5,\n#   \"topleft\", \n#   pch = c(19, NA, rep(19, 3)), \n#   lty = c(NA, 2, rep(NA, 3)),\n#   col = colours_all,\n#   legend = names(colours_all)\n# )\n\n# OT\ntb_aware_ot &lt;- tb_aware |&gt; \n  filter(counterfactual == \"ot\")\n# Predicted values, focusing on Black --&gt; White\npred_aware_ot_black_star &lt;- tb_aware_ot |&gt; filter(S == \"White\") |&gt; pull(\"pred\")\n# Estimated densities\nd_aware_ot_black_star &lt;- density(pred_aware_ot_black_star)\n\nplot(\n  d_aware_factuals_black,\n  main = \"\", xlab = \"\", ylab = \"\",\n  axes = FALSE, col = NA,\n  xlim = x_lim\n)\naxis(1)\naxis(2)\npolygon(d_aware_factuals_black, col = alpha(colours[[\"A\"]], .5), border = NA)\nlines(d_aware_factuals_white, col = colours[[\"B\"]], lty = 2, lwd = 2)\npolygon(d_aware_ot_black_star, col = alpha(colour_ot, .5), border = NA)\ntext(x = .53, y = 6, \"Multi. OT\")\n\n# Sequential transport\ntb_aware_seq &lt;- tb_aware |&gt; \n  filter(counterfactual == \"seq\")\n# Predicted values, focusing on Black --&gt; White\npred_aware_seq_black_star &lt;- tb_aware_seq |&gt; filter(S == \"White\") |&gt; pull(\"pred\")\n# Estimated densities\nd_aware_seq_black_star &lt;- density(pred_aware_seq_black_star)\n\nplot(\n  d_aware_factuals_black,\n  main = \"\", xlab = \"\", ylab = \"\",\n  axes = FALSE, col = NA,\n  xlim = x_lim\n)\naxis(1)\naxis(2)\npolygon(d_aware_factuals_black, col = alpha(colours[[\"A\"]], .5), border = NA)\nlines(d_aware_factuals_white, col = colours[[\"B\"]], lty = 2, lwd = 2)\npolygon(d_aware_seq_black_star, col = alpha(colour_seq, .5), border = NA)\ntext(x = .53, y = 6, \"Seq. T.\")\n\n\n\n\n\nFigure 8.4: Densities of predicted scores for Black individuals with factuals and with counterfactuals. The yellow dashed line corresponds to the density of predicted scores for White individuals, using factuals.",
    "crumbs": [
      "Counterfactuals with Law Dataset",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Counterfactuals: comparison</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Black, Emily, Samuel Yeom, and Matt Fredrikson. 2020. “Fliptest:\nFairness Testing via Optimal Transport.” In Proceedings of\nthe 2020 Conference on Fairness, Accountability, and Transparency,\n111–21.\n\n\nDe Lara, Lucas, Alberto González-Sanz, Nicholas Asher, Laurent Risser,\nand Jean-Michel Loubes. 2024. “Transport-Based Counterfactual\nModels.” Journal of Machine Learning Research 25 (136):\n1–59.\n\n\nHigham, Nicholas J. 2008. Functions of Matrices: Theory and\nComputation. SIAM.\n\n\nKusner, Matt J, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017.\n“Counterfactual Fairness.” In Advances in Neural\nInformation Processing Systems 30, edited by I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R.\nGarnett, 4066–76. NIPS.\n\n\nLara, Lucas de, Alberto González-Sanz, Nicholas Asher, and Jean-Michel\nLoubes. 2021. “Transport-Based Counterfactual Models.”\narXiv 2108.13025.\n\n\nPlečko, Drago, Nicolas Bennett, and Nicolai Meinshausen. 2021.\n“Fairadapt: Causal Reasoning for Fair Data Pre-Processing.”\narXiv Preprint arXiv:2110.10200.\n\n\nPlečko, Drago, and Nicolai Meinshausen. 2020. “Fair Data\nAdaptation with Quantile Preservation.” Journal of Machine\nLearning Research 21 (242): 1–44.\n\n\nSander, Richard H. 2004. “A Systemic Analysis of Affirmative\nAction in American Law Schools.” Stan. L. Rev. 57: 367.\n\n\nTakatsu, Asuka. 2011. “Wasserstein Geometry of Gaussian\nMeasures.” Osaka Journal of Mathematics 48 (4): 1005–26.\n\n\nWightman, Linda F. 1998. “LSAC National Longitudinal Bar Passage\nStudy. LSAC Research Report Series.” In. https://api.semanticscholar.org/CorpusID:151073942.",
    "crumbs": [
      "References"
    ]
  }
]